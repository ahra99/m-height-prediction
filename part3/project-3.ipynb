{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e5c8bac",
   "metadata": {},
   "source": [
    "# **DEEP LEARNING PROJECT - PART 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ab2f4",
   "metadata": {},
   "source": [
    "# In this notebook, the first part is for LP estimations, the second part is for Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d415bc0",
   "metadata": {},
   "source": [
    "# Code for creating m-height \n",
    "### NOTE: I used the provided dataset for training, however the following code uses the LP to find m-height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8dbaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_m_height(G, m):\n",
    "    k, n = G.shape\n",
    "    print('n ',n)\n",
    "    assert m <= n - k, \"m must be at most n - k\"\n",
    "    \n",
    "    # Ensure G is systematic\n",
    "    I_k = np.eye(k)\n",
    "    assert np.allclose(G[:, :k], I_k), \"G must be systematic (first k columns should form an identity matrix)\"\n",
    "    \n",
    "    indices = list(range(n)) #### [n> --> for any positive integer n\n",
    "    Psi = list(itertools.product([-1, 1], repeat=m))  # Set of all binary vectors of length m\n",
    "    \n",
    "    best_height = -np.inf\n",
    "    s = 0\n",
    "    for a, b in itertools.permutations(indices, 2):#permutations(indices, 2):\n",
    "        X_candidates = [set(comb) for comb in itertools.combinations(set(indices) - {a, b}, m - 1)] # combinations\n",
    "        # print('a ',a)\n",
    "        # print('b ', b)\n",
    "        for X in X_candidates:\n",
    "            Y = set(indices) - X - {a, b}\n",
    "            # print('Y ', Y)\n",
    "            # print('X ', X)\n",
    "            for psi in Psi:\n",
    "                tau = [a] + sorted(X) + [b] + sorted(Y)\n",
    "                # print('tau ', tau)\n",
    "                tau_inv = {xj: j for j, xj in enumerate(tau)}  # Calculate tau^{-1}\n",
    "                # for i in range(k):\n",
    "                #     print(psi[i])\n",
    "                c = np.array([psi[0] * G[i, a] for i in range(k)])  # Update objective function\n",
    "\n",
    "                A = []\n",
    "                b_ineq = []\n",
    "\n",
    "                for j in X:\n",
    "                    # row = [(psi[list(X).index(j)] * G[i, tau_inv[j]] - psi[0] * G[i, a]) for i in range(k)]\n",
    "                    row = [(psi[tau_inv[j]] * G[i, j] - psi[0] * G[i, a]) for i in range(k)]\n",
    "                    # row = [( - psi[tau_inv[j]] * G[i, j] + psi[0] * G[i, a]) for i in range(k)]\n",
    "                    A.append(row)\n",
    "                    b_ineq.append(0)\n",
    "\n",
    "                    # row_neg = [-psi[list(X).index(j)] * G[i, tau_inv[j]] for i in range(k)]\n",
    "                    row_neg = [-psi[tau_inv[j]] * G[i, j] for i in range(k)]\n",
    "                    # row_neg = [psi[tau_inv[j]] * G[i, j] for i in range(k)]\n",
    "                    A.append(row_neg)\n",
    "                    b_ineq.append(-1)\n",
    "                    # b_ineq.append(1)\n",
    "\n",
    "                for j in Y:\n",
    "                    # A.append([G[i, tau_inv[j]] for i in range(k)])  # Correctly index using tau_inv\n",
    "                    A.append([G[i, j] for i in range(k)])  # Correctly index using tau_inv\n",
    "                    b_ineq.append(1)\n",
    "                    # b_ineq.append(-1)\n",
    "\n",
    "                    A.append([-G[i, j] for i in range(k)])\n",
    "                    b_ineq.append(1)\n",
    "                    # b_ineq.append(-1)\n",
    "\n",
    "                A_eq = [[G[i, b] for i in range(k)]]\n",
    "                b_eq = [1]\n",
    "                # b_eq = [-1]\n",
    "\n",
    "                res = linprog(-c, A_ub=A, b_ub=b_ineq, A_eq=A_eq, b_eq=b_eq, method='highs')\n",
    "\n",
    "                s += 1\n",
    "\n",
    "                if res.success:\n",
    "                    best_height = max(best_height, -res.fun)\n",
    "    # print(s)\n",
    "    return best_height if best_height != -np.inf else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7645f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n  5\n",
      "m-height: 1.9242383477894285\n"
     ]
    }
   ],
   "source": [
    "# # Example Usage:\n",
    "G = np.array([[1, 0, 0.4759809, 0.993823, 0.819425], \n",
    "              [0, 1, -0.8960798 , -0.7442706, 0.3345122]])  # Systematic generator matrix\n",
    "\n",
    "# G = np.array([[1, 0, 0.534, 0.583, -0.709, -0.053, 0.543, 0.927], \n",
    "#               [0, 1, 0.611 , -1.211, 0.233, -0.784, -0.659, -0.947]])  # Systematic generator matrix\n",
    "m = 2\n",
    "print(\"m-height:\", compute_m_height(G, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5101abd",
   "metadata": {},
   "source": [
    "# ***Code for DNNs***\n",
    "\n",
    "## **NOTE:** the main difference between this code and the code for project 2, is that in this code, the model is more complex (2 more convolution layers + the number of channels have been increased); **another difference** is that in this model, I increased the number of modified input features from 20 to 30. **Finally**, in this notebook similar to project 2, for each setting (n,k,m), one separate model is being trained.\n",
    "\n",
    "## **NOTE:** after these modifications, the test performance was marginally improved with respect to project 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280cc8a",
   "metadata": {},
   "source": [
    "# Required packages for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007a318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459548a",
   "metadata": {},
   "source": [
    "# Class for creating the dataset for dataloader + function for preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddea7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "def pre_process_data(gen_mats, nfeat = 1): # matrix of shape [n_samples, n_dims]\n",
    "    struct_mat = []\n",
    "    m = nn.AdaptiveAvgPool1d(nfeat)\n",
    "    for item in gen_mats:\n",
    "        if type(item) == np.ndarray:\n",
    "            item = torch.tensor(item.astype(float))\n",
    "        # if type(item) == torch.Tensor:\n",
    "        #     continue\n",
    "        item = item.unsqueeze(0)\n",
    "        struct_mat.append(m(item))\n",
    "        \n",
    "    return torch.cat((struct_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7cbff1",
   "metadata": {},
   "source": [
    "An explanation for the model:\n",
    "Generally, the model consists of 3 components: \n",
    "- An encoder \n",
    "- - encoder itself contains 2 calls to the simple encoder which maps modified matrices (27 features in this code) and the \\[n,m,k\\], each to a 10 dimensional feature vector (hence, after concatanation, the input to the first conv1d layer is 20 dimensional)\n",
    "- - encoder also utilizes  5 conv1d layers with avg pooling\n",
    "- A Normalizing flow module (where I used planar flows for the trained model with 10 flows)\n",
    "- A decoder: similar to the encoder, without the use of simple encoders \n",
    "\n",
    "### NOTE: the complete model is provided in VAEWithFlow class \n",
    "\n",
    "### NOTE: the model predicts log2($\\cdot$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43c85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_encoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, 2)\n",
    "        self.fc2 = nn.Linear(2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        o = self.fc1(z)\n",
    "        o = self.relu(o)\n",
    "        o = self.fc2(o)\n",
    "        return o\n",
    "    \n",
    "\n",
    "\n",
    "# Planar Flow\n",
    "class PlanarFlow(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(1, dim))\n",
    "        self.u = nn.Parameter(torch.randn(1, dim))\n",
    "        self.b = nn.Parameter(torch.randn(1))\n",
    "    \n",
    "    def forward(self, z):\n",
    "        linear = torch.matmul(z, self.w.T) + self.b\n",
    "        activation = torch.tanh(linear)\n",
    "        psi = (1 - activation ** 2) * self.w\n",
    "        det_jacobian = torch.abs(1 + torch.matmul(psi, self.u.T))\n",
    "        log_det = torch.log(det_jacobian + 1e-6)\n",
    "        z_next = z + self.u * activation\n",
    "        return z_next, log_det\n",
    "\n",
    "# Radial Flow\n",
    "class RadialFlow(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.z0 = nn.Parameter(torch.randn(1, dim))\n",
    "        self.alpha = nn.Parameter(torch.randn(1))\n",
    "        self.beta = nn.Parameter(torch.randn(1))\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, z):\n",
    "        r = torch.norm(z - self.z0, dim=1, keepdim=True)\n",
    "        h = 1 / (1 + r)\n",
    "        beta_h = self.beta * h\n",
    "        z_next = z + beta_h * (z - self.z0)\n",
    "        log_det = (self.dim - 1) * torch.log(1 + beta_h) + torch.log(1 + beta_h + self.beta * h**2)\n",
    "        return z_next, log_det\n",
    "\n",
    "# Normalizing Flow Model\n",
    "class NormalizingFlow(nn.Module):\n",
    "    def __init__(self, dim, num_flows=2, flow_type='planar'):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList([PlanarFlow(dim) if flow_type == 'planar' else RadialFlow(dim) for _ in range(num_flows)])\n",
    "    \n",
    "    def forward(self, z0):\n",
    "        log_det_sum = 0\n",
    "        z = z0\n",
    "        for flow in self.flows:\n",
    "            z, log_det = flow(z)\n",
    "            log_det_sum += log_det\n",
    "        return z, log_det_sum\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emnk = simple_encoder(input_dim = 3, output_dim = 10)\n",
    "        self.eg = simple_encoder(input_dim = input_dim - 3, output_dim = 10)\n",
    "    \n",
    "#         self.conv1 = nn.Conv1d(in_channels = 1, out_channels = 16, kernel_size = 2, padding = 1)\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size = 2)\n",
    "#         self.conv2 = nn.Conv1d(in_channels = 16, out_channels = 64, kernel_size = 2, padding = 1) # was 32 previously - was 3 previously\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size = 2)\n",
    "#         self.conv3 = nn.Conv1d(in_channels = 64, out_channels = 256, kernel_size = 2, padding = 1) # was 128 previously\n",
    "#         self.pool3 = nn.MaxPool1d(kernel_size = 2)\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels = 1, out_channels = 32, kernel_size = 2, padding = 1)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size = 2) #nn.MaxPool1d(kernel_size = 2)\n",
    "        self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 128, kernel_size = 2, padding = 1) # was 32 previously - was 3 previously\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size = 2) #nn.MaxPool1d(kernel_size = 2)\n",
    "        self.conv22 = nn.Conv1d(in_channels = 128, out_channels = 128, kernel_size = 2, padding = 1)\n",
    "        self.conv3 = nn.Conv1d(in_channels = 128, out_channels = 256, kernel_size = 2, padding = 1) # was 128 previously\n",
    "        self.conv33 = nn.Conv1d(in_channels = 256, out_channels = 512, kernel_size = 2, padding = 1) \n",
    "#         self.conv33 = nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size = 2, padding = 1) \n",
    "        self.pool3 = nn.AvgPool1d(kernel_size = 2) #nn.MaxPool1d(kernel_size = 2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.fc1 = nn.Linear(input_dim, 64)\n",
    "#         self.fc1 = nn.Linear(32*5, 64)\n",
    "#         self.fc1 = nn.Linear(256*3, 64) # 128 * 2 previously\n",
    "#         self.fc1 = nn.Linear(256*4, 64) # 128 * 2 previously\n",
    "#         self.fc1 = nn.Linear(512*4, 64) # 128 * 2 previously\n",
    "#         self.fc1 = nn.Linear(256*4, 32) # 128 * 2 previously\n",
    "        self.fc1 = nn.Linear(512*4, 32) # 128 * 2 previously\n",
    "        self.fc11 = nn.Linear(32, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.mu = nn.Linear(64, latent_dim)\n",
    "        self.log_var = nn.Linear(64, latent_dim)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        f1 = self.emnk(x[:,-3:])\n",
    "        f2 = self.eg(x[:,:-3])\n",
    "        f = torch.cat([f1,f2], axis = 1)\n",
    "        \n",
    "        f = f.unsqueeze(1)\n",
    "        \n",
    "        h = self.conv1(f)\n",
    "        h = self.relu(h)\n",
    "        h = self.pool1(h)\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.conv22(h) ##### added \n",
    "        h = self.relu(h) ##### added\n",
    "        h = self.pool2(h)\n",
    "\n",
    "        h = self.conv3(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.conv33(h) #### added\n",
    "        h = self.relu(h) #### added\n",
    "        h = self.pool3(h)\n",
    "        h = self.flatten(h)\n",
    "#         print(h.shape)\n",
    "        \n",
    "        h = self.fc1(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc11(h) #### added\n",
    "        h = self.relu(h) #### added\n",
    "        h = self.dropout(h) #### added\n",
    "        h = self.fc2(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "\n",
    "        mu = self.mu(h)\n",
    "        log_var = self.log_var(h)\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z0 = mu + eps * std\n",
    "        return z0, mu, log_var\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.out = nn.Linear(64, output_dim)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        h = self.fc1(z)\n",
    "        h = self.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.dropout(h) ### the following layer was not previously there....\n",
    "        h = self.fc2(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        o = self.out(h)\n",
    "        return self.relu(o) # o if output is normalized, so it can be positive and # relu(o) output must be positive \n",
    "\n",
    "# Full VAE + Flow Model\n",
    "class VAEWithFlow(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, output_dim, num_flows=2, flow_type='planar'):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, latent_dim)\n",
    "        self.flow = NormalizingFlow(latent_dim, num_flows, flow_type)\n",
    "        self.decoder = Decoder(latent_dim, output_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z0, mu, log_var = self.encoder(x)\n",
    "        z_k, log_det = self.flow(z0)\n",
    "        x_recon = self.decoder(z_k)\n",
    "        return x_recon, z_k, mu, log_var, log_det\n",
    "    \n",
    "    def sample(self, num_samples=1):\n",
    "        z0 = torch.randn(num_samples, self.latent_dim)\n",
    "        z_k, _ = self.flow(z0)\n",
    "        return self.decoder(z_k)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b62c23",
   "metadata": {},
   "source": [
    "# function for setting the random seeds (for reproduibility purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ad2959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)  # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU\n",
    "    torch.cuda.manual_seed_all(seed)  # Multi-GPU\n",
    "    np.random.seed(seed)  # NumPy\n",
    "    # random.seed(seed)  # Python's random module\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False  # Slower, but more reproducible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140e266",
   "metadata": {},
   "source": [
    "### gathering all different settings in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea1d3c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 4, 2],\n",
       " [9, 4, 3],\n",
       " [9, 4, 4],\n",
       " [9, 4, 5],\n",
       " [9, 5, 2],\n",
       " [9, 5, 3],\n",
       " [9, 5, 4],\n",
       " [9, 6, 2],\n",
       " [9, 6, 3],\n",
       " [10, 4, 2],\n",
       " [10, 4, 3],\n",
       " [10, 4, 4],\n",
       " [10, 4, 5],\n",
       " [10, 4, 6],\n",
       " [10, 5, 2],\n",
       " [10, 5, 3],\n",
       " [10, 5, 4],\n",
       " [10, 5, 5],\n",
       " [10, 6, 2],\n",
       " [10, 6, 3],\n",
       " [10, 6, 4]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nkm = []\n",
    "for n in [9,10]:\n",
    "    for k in [4,5,6]:\n",
    "        for m in [2,3,4,5,6]:\n",
    "            if n-k >= m:\n",
    "                nkm.append([n,k,m])\n",
    "\n",
    "nkm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a054387",
   "metadata": {},
   "source": [
    "# loading unmodified dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6564711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\TAMU\\\\courses\\\\spring 2025\\\\deep learning\\\\project\\\\New folder\\\\\"\n",
    "s_inds = np.load(path+'inds_small_dset.npy')\n",
    "df = joblib.load(path + \"results_dataframe.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37069f0",
   "metadata": {},
   "source": [
    "# creating the modified datset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b6c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Getting targets, integers (n,k,m), and the modified matrices (all matrices will have the same number of features)')\n",
    "target_vals = torch.tensor(df['result'].values)\n",
    "nkm_vals = torch.tensor(df[['n','k','m']].values)\n",
    "gmat_vals = pre_process_data(df['P'], nfeat=27)\n",
    "\n",
    "print('Creating the modified features....')\n",
    "modif_data = torch.cat([gmat_vals, nkm_vals], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859633a",
   "metadata": {},
   "source": [
    "# saving the modified dataset (to avoid loading the big dataset again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edcd606",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modif_data, 'new_modif_data_all.pt')\n",
    "torch.save(target_vals, 'target_vals_all.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da433265",
   "metadata": {},
   "source": [
    "# Loading modified datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vals = torch.load('target_vals_all.pt')\n",
    "modif_data = torch.load('new_modif_data_all.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d85486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10980480, 30])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modif_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9327e7",
   "metadata": {},
   "source": [
    "## In the following block, after fixing the random seeds, for each setting (n,k,m), I first create a train/validation/test dataset, and then train a specific model for that specific dataset. Train/Validation losses and the Best model for each specific setting are saved where the name of each file ends with nkm ....\n",
    "## Also for encouraging a wider prediction rather than just the mean/mode, I devided the loss with the variance of training m-heights in each batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "107f6a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/job.13464513/ipykernel_981070/3883693184.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = CustomDataset(torch.tensor(train_data), (torch.log2(torch.tensor(train_targets))))\n",
      "/tmp/job.13464513/ipykernel_981070/3883693184.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  valid_dataset = CustomDataset(torch.tensor(val_data), torch.tensor(val_targets))\n",
      "/tmp/job.13464513/ipykernel_981070/3883693184.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_dataset = CustomDataset(torch.tensor(test_data), torch.tensor(test_targets))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):9, 4, 2 ---  epoch 1, train Loss: 903.9570055033194, valid loss: 5.587164760922466, lr: [0.00099], time: 23.13011908531189\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 2, train Loss: 0.7698372174346444, valid loss: 0.2119177551591072, lr: [0.0009801], time: 7.474376916885376\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 3, train Loss: 0.46671991186211914, valid loss: 0.20735464028784062, lr: [0.000970299], time: 7.399242877960205\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 4, train Loss: 0.4572597888031827, valid loss: 0.20415100874309108, lr: [0.0009605960099999999], time: 7.437420129776001\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 5, train Loss: 0.45306429222575606, valid loss: 0.20290074286891588, lr: [0.0009509900498999999], time: 7.379920482635498\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 6, train Loss: 0.45159738248721026, valid loss: 0.20334610450636942, lr: [0.0009414801494009999], time: 7.426389694213867\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 7, train Loss: 0.45074413156738036, valid loss: 0.20195098837539427, lr: [0.0009320653479069899], time: 7.386130094528198\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 8, train Loss: 0.45045003343815593, valid loss: 0.2032622407748985, lr: [0.00092274469442792], time: 7.434743642807007\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 9, train Loss: 0.44963297169285116, valid loss: 0.20199582846903172, lr: [0.0009135172474836408], time: 7.436879396438599\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 10, train Loss: 0.4503094752104158, valid loss: 0.20322558518484754, lr: [0.0009043820750088043], time: 7.656059741973877\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 11, train Loss: 0.4495022990230998, valid loss: 0.20160125392463787, lr: [0.0008953382542587163], time: 7.4799089431762695\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 12, train Loss: 0.44966282096433347, valid loss: 0.2025570366343912, lr: [0.0008863848717161291], time: 7.37696647644043\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 13, train Loss: 0.449833216713613, valid loss: 0.20367139246567886, lr: [0.0008775210229989678], time: 7.44411826133728\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 14, train Loss: 0.4496479670035205, valid loss: 0.2016060423282281, lr: [0.0008687458127689781], time: 7.376743316650391\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 15, train Loss: 0.4493143269113634, valid loss: 0.20156496730291984, lr: [0.0008600583546412883], time: 7.477914333343506\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 16, train Loss: 0.4502687930371725, valid loss: 0.20145504254992652, lr: [0.0008514577710948754], time: 7.4561004638671875\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 17, train Loss: 0.4503336767970675, valid loss: 0.20223112083777628, lr: [0.0008429431933839266], time: 7.378340005874634\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 18, train Loss: 0.44942881862722406, valid loss: 0.20342267614871368, lr: [0.0008345137614500873], time: 7.388211965560913\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 19, train Loss: 0.44968239093858714, valid loss: 0.20247211587331196, lr: [0.0008261686238355864], time: 7.392719268798828\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 20, train Loss: 0.4503418818403901, valid loss: 0.20167174758294631, lr: [0.0008179069375972306], time: 7.3746397495269775\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 21, train Loss: 0.45089135529913527, valid loss: 0.20125429655454594, lr: [0.0008097278682212583], time: 7.39052152633667\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 22, train Loss: 0.4511230136756362, valid loss: 0.20253733985055788, lr: [0.0008016305895390457], time: 7.395711183547974\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 23, train Loss: 0.4496797035583348, valid loss: 0.20270965964102808, lr: [0.0007936142836436553], time: 7.4080281257629395\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 24, train Loss: 0.4521808993899541, valid loss: 0.20416727542628563, lr: [0.0007856781408072188], time: 7.446964740753174\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 25, train Loss: 0.4508968210123561, valid loss: 0.20127962534809424, lr: [0.0007778213593991466], time: 7.4417548179626465\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 26, train Loss: 0.4505530232341589, valid loss: 0.20198470957209103, lr: [0.000770043145805155], time: 7.4045422077178955\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 27, train Loss: 0.4491548434233968, valid loss: 0.20379884827276523, lr: [0.0007623427143471034], time: 7.4507787227630615\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 28, train Loss: 0.45057124572646257, valid loss: 0.20173752833555184, lr: [0.0007547192872036325], time: 7.37323522567749\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 29, train Loss: 0.44959621933535865, valid loss: 0.2036157270471114, lr: [0.0007471720943315961], time: 7.387446403503418\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 30, train Loss: 0.44965492368157367, valid loss: 0.20347184607759447, lr: [0.0007397003733882801], time: 7.400030851364136\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 31, train Loss: 0.4507358948904296, valid loss: 0.20238436193958415, lr: [0.0007323033696543973], time: 7.454258441925049\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 32, train Loss: 0.4501398637013603, valid loss: 0.20113186272187136, lr: [0.0007249803359578533], time: 7.464618921279907\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 33, train Loss: 0.4509950706547843, valid loss: 0.20128320133421182, lr: [0.0007177305325982747], time: 7.4073545932769775\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 34, train Loss: 0.4508631410671317, valid loss: 0.2052714293380932, lr: [0.000710553227272292], time: 7.406643390655518\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 35, train Loss: 0.4504219762112107, valid loss: 0.20373084048217757, lr: [0.000703447694999569], time: 7.564625024795532\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 36, train Loss: 0.45038613645175807, valid loss: 0.2039375805881102, lr: [0.0006964132180495733], time: 7.447765350341797\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 37, train Loss: 0.45002313538074795, valid loss: 0.2038649489413099, lr: [0.0006894490858690775], time: 7.456597328186035\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 38, train Loss: 0.44914990911041114, valid loss: 0.20136226632798998, lr: [0.0006825545950103868], time: 7.404000759124756\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 39, train Loss: 0.4493825748989919, valid loss: 0.20114493195670474, lr: [0.000675729049060283], time: 7.330694198608398\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 40, train Loss: 0.45022570388366256, valid loss: 0.20092710041629025, lr: [0.0006689717585696801], time: 7.459696054458618\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 41, train Loss: 0.4494801911227341, valid loss: 0.20098842779626586, lr: [0.0006622820409839833], time: 7.4115424156188965\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 42, train Loss: 0.45029835016117864, valid loss: 0.20104685887500917, lr: [0.0006556592205741434], time: 7.440240383148193\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 43, train Loss: 0.4502493923821476, valid loss: 0.20371986519443777, lr: [0.0006491026283684019], time: 7.459481716156006\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 44, train Loss: 0.4533447962573973, valid loss: 0.20130826919055317, lr: [0.0006426116020847179], time: 7.3800883293151855\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 45, train Loss: 0.4505515238719096, valid loss: 0.20151739260992504, lr: [0.0006361854860638707], time: 7.330402374267578\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 46, train Loss: 0.44937845623922923, valid loss: 0.20102109473641555, lr: [0.000629823631203232], time: 7.439002752304077\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 47, train Loss: 0.4492660393311267, valid loss: 0.20234820155097133, lr: [0.0006235253948911997], time: 7.396467924118042\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 48, train Loss: 0.4490726191305182, valid loss: 0.20103581158942832, lr: [0.0006172901409422877], time: 7.388110876083374\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 49, train Loss: 0.4502112967999131, valid loss: 0.20051471695256723, lr: [0.0006111172395328649], time: 7.478199005126953\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 50, train Loss: 0.4503464628616264, valid loss: 0.20354848413581972, lr: [0.0006050060671375363], time: 7.447437286376953\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 51, train Loss: 0.44908152845438587, valid loss: 0.20099099738061948, lr: [0.0005989560064661609], time: 7.383028745651245\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 52, train Loss: 0.44930080862395927, valid loss: 0.20137339907722768, lr: [0.0005929664464014993], time: 7.411034107208252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):9, 4, 2 ---  epoch 53, train Loss: 0.4491279734117613, valid loss: 0.20197872172202283, lr: [0.0005870367819374844], time: 7.391506195068359\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 54, train Loss: 0.4501386531150124, valid loss: 0.2012060062394708, lr: [0.0005811664141181095], time: 7.458419561386108\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 55, train Loss: 0.44893839343992087, valid loss: 0.20141039554577828, lr: [0.0005753547499769285], time: 7.388744115829468\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 56, train Loss: 0.4490615182614723, valid loss: 0.20123580719839868, lr: [0.0005696012024771592], time: 7.478545665740967\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 57, train Loss: 0.45092144986285887, valid loss: 0.20473225472079024, lr: [0.0005639051904523875], time: 7.461352586746216\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 58, train Loss: 0.44953817991661255, valid loss: 0.20320995433475786, lr: [0.0005582661385478637], time: 7.384713411331177\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 59, train Loss: 0.4497986505949499, valid loss: 0.20149583836762555, lr: [0.000552683477162385], time: 7.457254886627197\n",
      "Setting (n,k,m):9, 4, 2 ---  epoch 60, train Loss: 0.44842590308874447, valid loss: 0.20105848881912342, lr: [0.0005471566423907612], time: 7.423538684844971\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 1, train Loss: 22.640667947350828, valid loss: 3.6844785725486955, lr: [0.00099], time: 7.800853490829468\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 2, train Loss: 1.4994674376914505, valid loss: 0.5150277538326039, lr: [0.0009801], time: 7.456254005432129\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 3, train Loss: 0.8363254058868463, valid loss: 0.34848004235728364, lr: [0.000970299], time: 7.456631898880005\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 4, train Loss: 0.6159436470793378, valid loss: 0.285070299919428, lr: [0.0009605960099999999], time: 7.439345121383667\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 5, train Loss: 0.5429132272395752, valid loss: 0.26595999606134485, lr: [0.0009509900498999999], time: 7.449610233306885\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 6, train Loss: 0.5190686013568737, valid loss: 0.26198797962521053, lr: [0.0009414801494009999], time: 7.622927904129028\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 7, train Loss: 0.5122382538045974, valid loss: 0.25656807061055564, lr: [0.0009320653479069899], time: 7.453550100326538\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 8, train Loss: 0.5083061619074419, valid loss: 0.2716348860097331, lr: [0.00092274469442792], time: 8.02307915687561\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 9, train Loss: 0.5066096932926397, valid loss: 0.2545152165931825, lr: [0.0009135172474836408], time: 7.457392930984497\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 10, train Loss: 0.5055881983359886, valid loss: 0.26975266838226863, lr: [0.0009043820750088043], time: 7.513890981674194\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 11, train Loss: 0.5050093528207592, valid loss: 0.25404746938346334, lr: [0.0008953382542587163], time: 7.544922590255737\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 12, train Loss: 0.5064731996010831, valid loss: 0.2825156776452381, lr: [0.0008863848717161291], time: 7.433007001876831\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 13, train Loss: 0.5055082169610888, valid loss: 0.2566768174163566, lr: [0.0008775210229989678], time: 7.368813991546631\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 14, train Loss: 0.5051925922965309, valid loss: 0.25425118305204647, lr: [0.0008687458127689781], time: 7.492290019989014\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 15, train Loss: 0.5079907141149067, valid loss: 0.25397973458230094, lr: [0.0008600583546412883], time: 7.442476511001587\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 16, train Loss: 0.5068869683603674, valid loss: 0.2543698882968402, lr: [0.0008514577710948754], time: 7.42648458480835\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 17, train Loss: 0.5030593226623356, valid loss: 0.25285757718559665, lr: [0.0008429431933839266], time: 7.4476869106292725\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 18, train Loss: 0.5052954786848105, valid loss: 0.256068704096975, lr: [0.0008345137614500873], time: 7.493635177612305\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 19, train Loss: 0.5026086654002885, valid loss: 0.25472426881572563, lr: [0.0008261686238355864], time: 7.430363893508911\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 20, train Loss: 0.5030907395215221, valid loss: 0.2601018952147025, lr: [0.0008179069375972306], time: 7.495888948440552\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 21, train Loss: 0.5021766598892666, valid loss: 0.25176197227287417, lr: [0.0008097278682212583], time: 7.521125555038452\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 22, train Loss: 0.5023695290447778, valid loss: 0.25548515386230525, lr: [0.0008016305895390457], time: 7.444119691848755\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 23, train Loss: 0.5018037116960341, valid loss: 0.2551697770611861, lr: [0.0007936142836436553], time: 7.501021146774292\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 24, train Loss: 0.5008280915196648, valid loss: 0.25772514921181644, lr: [0.0007856781408072188], time: 7.496870756149292\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 25, train Loss: 0.5026702224543256, valid loss: 0.2519933606983903, lr: [0.0007778213593991466], time: 7.374279499053955\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 26, train Loss: 0.5025275950493822, valid loss: 0.25355568321869887, lr: [0.000770043145805155], time: 7.500226974487305\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 27, train Loss: 0.5011605956275338, valid loss: 0.25534489627713436, lr: [0.0007623427143471034], time: 7.440354347229004\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 28, train Loss: 0.5024518012851269, valid loss: 0.25190539902753195, lr: [0.0007547192872036325], time: 7.5036211013793945\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 29, train Loss: 0.5014580598816875, valid loss: 0.2618198771612397, lr: [0.0007471720943315961], time: 7.459495782852173\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 30, train Loss: 0.5014481083917709, valid loss: 0.25208379351274873, lr: [0.0007397003733882801], time: 7.510707855224609\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 31, train Loss: 0.5002439005554329, valid loss: 0.2510576548031623, lr: [0.0007323033696543973], time: 7.455221652984619\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 32, train Loss: 0.5007654084520042, valid loss: 0.2518422427581111, lr: [0.0007249803359578533], time: 7.444312572479248\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 33, train Loss: 0.5006209041110623, valid loss: 0.29555457374393324, lr: [0.0007177305325982747], time: 7.4460768699646\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 34, train Loss: 0.502707121636047, valid loss: 0.25169894583293306, lr: [0.000710553227272292], time: 7.501276731491089\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 35, train Loss: 0.500560092507884, valid loss: 0.25292801582015234, lr: [0.000703447694999569], time: 7.436461687088013\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 36, train Loss: 0.5024928137731167, valid loss: 0.25517054896176844, lr: [0.0006964132180495733], time: 7.5124781131744385\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 37, train Loss: 0.49962316103092363, valid loss: 0.25142297581571266, lr: [0.0006894490858690775], time: 7.445263862609863\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 38, train Loss: 0.5006892597582042, valid loss: 0.25162090534615744, lr: [0.0006825545950103868], time: 7.500492572784424\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 39, train Loss: 0.4997372032799845, valid loss: 0.25110599819341767, lr: [0.000675729049060283], time: 7.428836345672607\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 40, train Loss: 0.5001909606676762, valid loss: 0.2509835872555061, lr: [0.0006689717585696801], time: 7.524035930633545\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 41, train Loss: 0.5000521918504207, valid loss: 0.2560382491313576, lr: [0.0006622820409839833], time: 7.439943075180054\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 42, train Loss: 0.5002402300613628, valid loss: 0.25258965669221534, lr: [0.0006556592205741434], time: 7.5114195346832275\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 43, train Loss: 0.4985720656422279, valid loss: 0.2529972094431466, lr: [0.0006491026283684019], time: 7.434638500213623\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 44, train Loss: 0.49929802905082454, valid loss: 0.2516867308849275, lr: [0.0006426116020847179], time: 7.511348724365234\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 45, train Loss: 0.4989566640582115, valid loss: 0.25414905196680865, lr: [0.0006361854860638707], time: 7.534376621246338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):9, 4, 3 ---  epoch 46, train Loss: 0.5000997664674971, valid loss: 0.25099855258860165, lr: [0.000629823631203232], time: 7.483178377151489\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 47, train Loss: 0.49915139724303714, valid loss: 0.25793987655437045, lr: [0.0006235253948911997], time: 7.487507343292236\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 48, train Loss: 0.4991155198686973, valid loss: 0.2514041743341334, lr: [0.0006172901409422877], time: 7.351088762283325\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 49, train Loss: 0.4991075951529239, valid loss: 0.2547091542842542, lr: [0.0006111172395328649], time: 7.505786180496216\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 50, train Loss: 0.4991732734446721, valid loss: 0.25218798992694125, lr: [0.0006050060671375363], time: 7.416121959686279\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 51, train Loss: 0.49894494251256405, valid loss: 0.26796855453439067, lr: [0.0005989560064661609], time: 7.481668710708618\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 52, train Loss: 0.49944438405039393, valid loss: 0.2510131800591821, lr: [0.0005929664464014993], time: 7.418822526931763\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 53, train Loss: 0.49756618134745034, valid loss: 0.25090203016256113, lr: [0.0005870367819374844], time: 7.517409801483154\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 54, train Loss: 0.4984422372470779, valid loss: 0.25087189055217307, lr: [0.0005811664141181095], time: 7.435374021530151\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 55, train Loss: 0.5002247749635944, valid loss: 0.25088521312653445, lr: [0.0005753547499769285], time: 7.482513666152954\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 56, train Loss: 0.4984968913968893, valid loss: 0.2515644163198947, lr: [0.0005696012024771592], time: 7.419519901275635\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 57, train Loss: 0.49920238105806025, valid loss: 0.25962898440091825, lr: [0.0005639051904523875], time: 7.4536826610565186\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 58, train Loss: 0.4998681411415335, valid loss: 0.25091252005618764, lr: [0.0005582661385478637], time: 7.495877742767334\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 59, train Loss: 0.4996745218013113, valid loss: 0.2517434032886193, lr: [0.000552683477162385], time: 7.418952226638794\n",
      "Setting (n,k,m):9, 4, 3 ---  epoch 60, train Loss: 0.4985459444716487, valid loss: 0.251178954550451, lr: [0.0005471566423907612], time: 7.507408857345581\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 1, train Loss: 19.816319024405388, valid loss: 5.533524845655605, lr: [0.00099], time: 7.573391675949097\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 2, train Loss: 1.2062230343878984, valid loss: 0.9198136368989676, lr: [0.0009801], time: 7.392952919006348\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 3, train Loss: 0.9666042778005396, valid loss: 0.9128059772403208, lr: [0.000970299], time: 7.384277105331421\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 4, train Loss: 0.9582438767224862, valid loss: 0.9092112001925822, lr: [0.0009605960099999999], time: 7.382986068725586\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 5, train Loss: 0.9524991891933391, valid loss: 0.8992418913403223, lr: [0.0009509900498999999], time: 7.450913429260254\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 6, train Loss: 0.9508780274970936, valid loss: 0.9016962779829418, lr: [0.0009414801494009999], time: 7.386953830718994\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 7, train Loss: 0.950421941687886, valid loss: 0.9006929251704312, lr: [0.0009320653479069899], time: 7.397361993789673\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 8, train Loss: 0.9527365572170516, valid loss: 0.9053665886959308, lr: [0.00092274469442792], time: 7.458109617233276\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 9, train Loss: 0.9507397015124025, valid loss: 0.9046889306731815, lr: [0.0009135172474836408], time: 7.385380268096924\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 10, train Loss: 0.952708045687914, valid loss: 0.9001194506472864, lr: [0.0009043820750088043], time: 7.383071422576904\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 11, train Loss: 0.9503483211399455, valid loss: 0.9000884302950137, lr: [0.0008953382542587163], time: 7.45060920715332\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 12, train Loss: 0.9535849185616396, valid loss: 0.9044896763341604, lr: [0.0008863848717161291], time: 7.349245548248291\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 13, train Loss: 0.9513381993066208, valid loss: 0.8991592861345441, lr: [0.0008775210229989678], time: 7.470757007598877\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 14, train Loss: 0.9507647723892387, valid loss: 0.8988897006985926, lr: [0.0008687458127689781], time: 7.405665397644043\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 15, train Loss: 0.9516980421218585, valid loss: 0.9120740804852979, lr: [0.0008600583546412883], time: 7.457391977310181\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 16, train Loss: 0.9517859836326311, valid loss: 0.9033781522028209, lr: [0.0008514577710948754], time: 7.335784435272217\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 17, train Loss: 0.9509374522685992, valid loss: 0.8996063304931223, lr: [0.0008429431933839266], time: 7.453827857971191\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 18, train Loss: 0.9537096918872704, valid loss: 0.9008048768132385, lr: [0.0008345137614500873], time: 7.38693380355835\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 19, train Loss: 0.9526576378563024, valid loss: 0.9676124380297483, lr: [0.0008261686238355864], time: 7.39169454574585\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 20, train Loss: 0.9527263580575895, valid loss: 0.8981675251678034, lr: [0.0008179069375972306], time: 7.489486455917358\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 21, train Loss: 0.9518036273803328, valid loss: 0.9014620023156317, lr: [0.0008097278682212583], time: 7.394795179367065\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 22, train Loss: 0.9512009375546284, valid loss: 0.9122866035036686, lr: [0.0008016305895390457], time: 7.3835508823394775\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 23, train Loss: 0.9507735253158786, valid loss: 0.8983654390172433, lr: [0.0007936142836436553], time: 7.45273756980896\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 24, train Loss: 0.950393522891975, valid loss: 0.8984075413279033, lr: [0.0007856781408072188], time: 7.4083638191223145\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 25, train Loss: 0.9527009212302228, valid loss: 0.9100675329273075, lr: [0.0007778213593991466], time: 7.3884055614471436\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 26, train Loss: 0.9525672178554545, valid loss: 0.8985438393155165, lr: [0.000770043145805155], time: 7.448973894119263\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 27, train Loss: 0.9512071878346018, valid loss: 0.90003330464098, lr: [0.0007623427143471034], time: 7.386070728302002\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 28, train Loss: 0.9520634721487662, valid loss: 0.9010365065008065, lr: [0.0007547192872036325], time: 7.337110996246338\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 29, train Loss: 0.9531514160816442, valid loss: 0.8972494368826508, lr: [0.0007471720943315961], time: 7.466113805770874\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 30, train Loss: 0.9511293946382333, valid loss: 0.9493410147941603, lr: [0.0007397003733882801], time: 7.381619453430176\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 31, train Loss: 0.9531246455543078, valid loss: 0.9031925473064, lr: [0.0007323033696543973], time: 7.615076303482056\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 32, train Loss: 0.9518711649004713, valid loss: 0.9016911193063019, lr: [0.0007249803359578533], time: 7.625130891799927\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 33, train Loss: 0.9574058155436757, valid loss: 0.9034757297879032, lr: [0.0007177305325982747], time: 7.398353576660156\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 34, train Loss: 0.9900697691222098, valid loss: 0.9117864454312469, lr: [0.000710553227272292], time: 7.406696796417236\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 35, train Loss: 0.9582574505808108, valid loss: 0.9049160416702394, lr: [0.000703447694999569], time: 7.467876195907593\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 36, train Loss: 0.9521155202579329, valid loss: 0.8989919186791281, lr: [0.0006964132180495733], time: 7.41659140586853\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 37, train Loss: 0.9531392109181623, valid loss: 0.9080674783736782, lr: [0.0006894490858690775], time: 7.394376039505005\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 38, train Loss: 0.9511756604446029, valid loss: 0.8979341246740953, lr: [0.0006825545950103868], time: 7.552383184432983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):9, 4, 4 ---  epoch 39, train Loss: 0.950583120076663, valid loss: 0.9210358458296489, lr: [0.000675729049060283], time: 7.339039325714111\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 40, train Loss: 0.9520976902439839, valid loss: 0.9075924069038165, lr: [0.0006689717585696801], time: 7.508583307266235\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 41, train Loss: 0.9498666796959785, valid loss: 0.9000371207391494, lr: [0.0006622820409839833], time: 7.415187358856201\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 42, train Loss: 0.9518677684980579, valid loss: 0.8991142177482826, lr: [0.0006556592205741434], time: 7.408042669296265\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 43, train Loss: 0.9487831850107195, valid loss: 0.9046317007852683, lr: [0.0006491026283684019], time: 7.403388738632202\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 44, train Loss: 0.9504656609272, valid loss: 0.907328014396577, lr: [0.0006426116020847179], time: 7.438408136367798\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 45, train Loss: 0.9505785352163868, valid loss: 0.8983096422627371, lr: [0.0006361854860638707], time: 8.58786940574646\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 46, train Loss: 0.9509277618820441, valid loss: 0.8987352043715136, lr: [0.000629823631203232], time: 7.345218896865845\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 47, train Loss: 0.9493106221802674, valid loss: 0.8973452659828891, lr: [0.0006235253948911997], time: 7.476709604263306\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 48, train Loss: 0.9497134702874375, valid loss: 0.8974155748489316, lr: [0.0006172901409422877], time: 7.433987855911255\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 49, train Loss: 0.958974808925166, valid loss: 0.9334892048616971, lr: [0.0006111172395328649], time: 7.422694444656372\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 50, train Loss: 0.9541328200085053, valid loss: 0.8997246324473689, lr: [0.0006050060671375363], time: 7.475996017456055\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 51, train Loss: 0.949421950394773, valid loss: 0.8977561760020237, lr: [0.0005989560064661609], time: 7.4082932472229\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 52, train Loss: 0.9486234172368105, valid loss: 0.9342444561665983, lr: [0.0005929664464014993], time: 7.351155757904053\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 53, train Loss: 0.9499326190968553, valid loss: 0.9038624109213717, lr: [0.0005870367819374844], time: 7.492626667022705\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 54, train Loss: 0.9594239223449997, valid loss: 0.8973618328138483, lr: [0.0005811664141181095], time: 7.406769275665283\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 55, train Loss: 0.9484199663329342, valid loss: 0.8970948730448877, lr: [0.0005753547499769285], time: 7.486840009689331\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 56, train Loss: 0.9492999824266944, valid loss: 0.8997281029344368, lr: [0.0005696012024771592], time: 7.428497791290283\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 57, train Loss: 0.9490692202023147, valid loss: 0.8971593577878513, lr: [0.0005639051904523875], time: 7.49214243888855\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 58, train Loss: 0.9497438034291956, valid loss: 0.9008568017548452, lr: [0.0005582661385478637], time: 7.405144453048706\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 59, train Loss: 0.948715059545873, valid loss: 0.8997162589940231, lr: [0.000552683477162385], time: 7.337218761444092\n",
      "Setting (n,k,m):9, 4, 4 ---  epoch 60, train Loss: 0.948939977251526, valid loss: 0.898826370840419, lr: [0.0005471566423907612], time: 7.4901533126831055\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 1, train Loss: 26.289152300482026, valid loss: 10.944827665621679, lr: [0.00099], time: 7.516568183898926\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 2, train Loss: 2.3376150412705794, valid loss: 3.460326912816789, lr: [0.0009801], time: 7.444631099700928\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 3, train Loss: 1.8694115928268684, valid loss: 3.3733441001409514, lr: [0.000970299], time: 7.445690870285034\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 4, train Loss: 1.8595533792542027, valid loss: 3.3652654465306426, lr: [0.0009605960099999999], time: 7.517911434173584\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 5, train Loss: 1.857298024738605, valid loss: 3.3629149138590515, lr: [0.0009509900498999999], time: 7.441959619522095\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 6, train Loss: 1.8553176997555252, valid loss: 3.3638537350103834, lr: [0.0009414801494009999], time: 7.494345188140869\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 7, train Loss: 1.8577481992048794, valid loss: 3.435878099929978, lr: [0.0009320653479069899], time: 7.428847312927246\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 8, train Loss: 1.8614974229812629, valid loss: 3.368026899160281, lr: [0.00092274469442792], time: 7.494667291641235\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 9, train Loss: 1.8575132017613665, valid loss: 3.406983535479734, lr: [0.0009135172474836408], time: 7.502645969390869\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 10, train Loss: 1.8587743330344721, valid loss: 3.3635895806013734, lr: [0.0009043820750088043], time: 7.361010313034058\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 11, train Loss: 1.8702460203094493, valid loss: 3.388118840278753, lr: [0.0008953382542587163], time: 7.4915549755096436\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 12, train Loss: 1.860141256945522, valid loss: 3.3795760359692717, lr: [0.0008863848717161291], time: 7.437697410583496\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 13, train Loss: 1.8572615370101289, valid loss: 3.3780742525640224, lr: [0.0008775210229989678], time: 7.507491588592529\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 14, train Loss: 1.8572034122744656, valid loss: 3.3724786439292602, lr: [0.0008687458127689781], time: 7.4350621700286865\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 15, train Loss: 1.8588551215860543, valid loss: 3.3629487742254853, lr: [0.0008600583546412883], time: 7.489186525344849\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 16, train Loss: 1.8579658429459336, valid loss: 3.365169443381277, lr: [0.0008514577710948754], time: 7.422188997268677\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 17, train Loss: 1.855974645437441, valid loss: 3.3744274758004438, lr: [0.0008429431933839266], time: 7.4359471797943115\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 18, train Loss: 1.86032826432503, valid loss: 3.372208459768842, lr: [0.0008345137614500873], time: 7.496933460235596\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 19, train Loss: 1.85808333404371, valid loss: 3.374631079230901, lr: [0.0008261686238355864], time: 7.411799669265747\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 20, train Loss: 1.8579459277049863, valid loss: 3.38479326889399, lr: [0.0008179069375972306], time: 7.491982460021973\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 21, train Loss: 1.8569746291997278, valid loss: 3.3745844258550983, lr: [0.0008097278682212583], time: 7.442070722579956\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 22, train Loss: 1.8568347114569748, valid loss: 3.362231946057293, lr: [0.0008016305895390457], time: 7.513313293457031\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 23, train Loss: 1.8583592762812569, valid loss: 3.3690305257286925, lr: [0.0007936142836436553], time: 7.416965007781982\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 24, train Loss: 1.8587690189787935, valid loss: 3.360540619601611, lr: [0.0007856781408072188], time: 7.505518436431885\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 25, train Loss: 1.8582723940577048, valid loss: 3.367928103383284, lr: [0.0007778213593991466], time: 7.440832853317261\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 26, train Loss: 1.861758147857731, valid loss: 3.3632167804521975, lr: [0.000770043145805155], time: 7.492218017578125\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 27, train Loss: 1.8592614548287267, valid loss: 3.3615470643918224, lr: [0.0007623427143471034], time: 7.428196907043457\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 28, train Loss: 1.857092310831235, valid loss: 3.371557506895692, lr: [0.0007547192872036325], time: 7.499138116836548\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 29, train Loss: 1.8593726653312905, valid loss: 3.372071711469278, lr: [0.0007471720943315961], time: 7.435761451721191\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 30, train Loss: 1.8577470566284155, valid loss: 3.3882728806950326, lr: [0.0007397003733882801], time: 7.437426328659058\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 31, train Loss: 1.8566356264812325, valid loss: 3.38226174817899, lr: [0.0007323033696543973], time: 7.480433225631714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):9, 4, 5 ---  epoch 32, train Loss: 1.8557080514428628, valid loss: 3.3614265230710676, lr: [0.0007249803359578533], time: 7.4262120723724365\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 33, train Loss: 1.8574752212893786, valid loss: 3.368228412879841, lr: [0.0007177305325982747], time: 7.498406887054443\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 34, train Loss: 1.8573519245372656, valid loss: 3.3623189655457777, lr: [0.000710553227272292], time: 7.427379608154297\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 35, train Loss: 1.8595555460736544, valid loss: 3.364734836355708, lr: [0.000703447694999569], time: 7.500561237335205\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 36, train Loss: 1.858041793139151, valid loss: 3.360635440839058, lr: [0.0006964132180495733], time: 7.9715986251831055\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 37, train Loss: 1.8570046449126187, valid loss: 3.3682711367496183, lr: [0.0006894490858690775], time: 7.3799262046813965\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 38, train Loss: 1.8569655862987284, valid loss: 3.36759972088932, lr: [0.0006825545950103868], time: 7.501892805099487\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 39, train Loss: 1.8573264192024732, valid loss: 3.379542815169996, lr: [0.000675729049060283], time: 7.424319505691528\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 40, train Loss: 1.8571796898184536, valid loss: 3.3680589516619586, lr: [0.0006689717585696801], time: 7.5031819343566895\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 41, train Loss: 1.8586703259706678, valid loss: 3.4525232133308923, lr: [0.0006622820409839833], time: 7.443318843841553\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 42, train Loss: 1.8610614716938512, valid loss: 3.3607262982902566, lr: [0.0006556592205741434], time: 7.5042829513549805\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 43, train Loss: 1.8566147561795054, valid loss: 3.4047265911099673, lr: [0.0006491026283684019], time: 7.422836780548096\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 44, train Loss: 1.8595496548894699, valid loss: 3.363406852772884, lr: [0.0006426116020847179], time: 7.444965600967407\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 45, train Loss: 1.8925981881132654, valid loss: 3.3624719501055016, lr: [0.0006361854860638707], time: 7.5219526290893555\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 46, train Loss: 1.8562776850735134, valid loss: 3.370307962669394, lr: [0.000629823631203232], time: 7.4312403202056885\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 47, train Loss: 1.8607755971759934, valid loss: 3.4083286088759066, lr: [0.0006235253948911997], time: 7.498600721359253\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 48, train Loss: 1.887393008780338, valid loss: 3.374853267875208, lr: [0.0006172901409422877], time: 7.439417123794556\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 49, train Loss: 1.8574148316213792, valid loss: 3.367147197369702, lr: [0.0006111172395328649], time: 7.50693416595459\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 50, train Loss: 1.8550807985714386, valid loss: 3.359027323820366, lr: [0.0006050060671375363], time: 7.522268295288086\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 51, train Loss: 1.8559206554437941, valid loss: 3.379811736041033, lr: [0.0005989560064661609], time: 8.697736978530884\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 52, train Loss: 1.8574856007433775, valid loss: 3.3840485364952553, lr: [0.0005929664464014993], time: 7.367290735244751\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 53, train Loss: 1.8561709288642456, valid loss: 3.3652921656021406, lr: [0.0005870367819374844], time: 7.501465559005737\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 54, train Loss: 1.8565479863239926, valid loss: 3.395749697953312, lr: [0.0005811664141181095], time: 7.450103759765625\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 55, train Loss: 1.8560068746928056, valid loss: 3.36252191511399, lr: [0.0005753547499769285], time: 7.504155397415161\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 56, train Loss: 1.8552429638002994, valid loss: 3.3707379691504404, lr: [0.0005696012024771592], time: 7.501474142074585\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 57, train Loss: 1.855491536728889, valid loss: 3.3793896708125555, lr: [0.0005639051904523875], time: 7.3635573387146\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 58, train Loss: 1.8541973311075977, valid loss: 3.3679496774472937, lr: [0.0005582661385478637], time: 7.516576528549194\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 59, train Loss: 1.8566148772443074, valid loss: 3.3755843485373522, lr: [0.000552683477162385], time: 7.4417338371276855\n",
      "Setting (n,k,m):9, 4, 5 ---  epoch 60, train Loss: 1.854798783421177, valid loss: 3.364148217600416, lr: [0.0005471566423907612], time: 7.527423620223999\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 1, train Loss: 24.0710265354673, valid loss: 3.514929973565366, lr: [0.00099], time: 10.088317394256592\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 2, train Loss: 0.6545721244946061, valid loss: 0.24129226400307027, lr: [0.0009801], time: 9.949287176132202\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 3, train Loss: 0.4909293838477058, valid loss: 0.2386767762406344, lr: [0.000970299], time: 9.951059818267822\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 4, train Loss: 0.4858030755123205, valid loss: 0.23583053035027968, lr: [0.0009605960099999999], time: 9.95055890083313\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 5, train Loss: 0.48317369510199515, valid loss: 0.23583801168874755, lr: [0.0009509900498999999], time: 9.956398248672485\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 6, train Loss: 0.48343151735192824, valid loss: 0.23677795217409453, lr: [0.0009414801494009999], time: 9.94190263748169\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 7, train Loss: 0.4825290952877705, valid loss: 0.23612061257671876, lr: [0.0009320653479069899], time: 9.865097045898438\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 8, train Loss: 0.4836627152038811, valid loss: 0.23566087168631297, lr: [0.00092274469442792], time: 9.88375473022461\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 9, train Loss: 0.4828365869126013, valid loss: 0.2354196925583651, lr: [0.0009135172474836408], time: 9.970548152923584\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 10, train Loss: 0.4833692954361735, valid loss: 0.23552333592757443, lr: [0.0009043820750088043], time: 9.933533191680908\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 11, train Loss: 0.4832804216584459, valid loss: 0.24136629920673516, lr: [0.0008953382542587163], time: 9.93812346458435\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 12, train Loss: 0.4832801677796608, valid loss: 0.23707939772542239, lr: [0.0008863848717161291], time: 9.965970754623413\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 13, train Loss: 0.4836486192406919, valid loss: 0.235633930917964, lr: [0.0008775210229989678], time: 9.86758804321289\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 14, train Loss: 0.48402297550227846, valid loss: 0.23545569739438743, lr: [0.0008687458127689781], time: 9.873719692230225\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 15, train Loss: 0.48524107200881383, valid loss: 0.24215935791910012, lr: [0.0008600583546412883], time: 9.916795253753662\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 16, train Loss: 0.4833770701261025, valid loss: 0.23510999573495905, lr: [0.0008514577710948754], time: 9.95926022529602\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 17, train Loss: 0.48349457400775514, valid loss: 0.2383343941830339, lr: [0.0008429431933839266], time: 9.949286222457886\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 18, train Loss: 0.4846243487431381, valid loss: 0.23502285878240317, lr: [0.0008345137614500873], time: 9.995753049850464\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 19, train Loss: 0.48435981703359343, valid loss: 0.24104544135606965, lr: [0.0008261686238355864], time: 9.9580717086792\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 20, train Loss: 0.4831467640821786, valid loss: 0.23538671817866497, lr: [0.0008179069375972306], time: 9.886758804321289\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 21, train Loss: 0.48363173505004015, valid loss: 0.23654995887589164, lr: [0.0008097278682212583], time: 9.982734203338623\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 22, train Loss: 0.4827111309240942, valid loss: 0.2361341706730383, lr: [0.0008016305895390457], time: 9.966954231262207\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 23, train Loss: 0.48456080141663604, valid loss: 0.2352689228952629, lr: [0.0007936142836436553], time: 9.932798147201538\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 24, train Loss: 0.4819168544615144, valid loss: 0.234817415743015, lr: [0.0007856781408072188], time: 9.915356397628784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):9, 5, 2 ---  epoch 25, train Loss: 0.4855948783129127, valid loss: 0.237455509326606, lr: [0.0007778213593991466], time: 9.953442811965942\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 26, train Loss: 0.4830420206686004, valid loss: 0.2347481136985846, lr: [0.000770043145805155], time: 9.894999027252197\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 27, train Loss: 0.48360128144150943, valid loss: 0.2352431360068583, lr: [0.0007623427143471034], time: 9.980042457580566\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 28, train Loss: 0.48198436130924, valid loss: 0.23468409412051627, lr: [0.0007547192872036325], time: 9.90033221244812\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 29, train Loss: 0.48179232935704996, valid loss: 0.23516019386416073, lr: [0.0007471720943315961], time: 9.961617231369019\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 30, train Loss: 0.4830489737312039, valid loss: 0.23535529852843665, lr: [0.0007397003733882801], time: 9.97313928604126\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 31, train Loss: 0.4814334440495816, valid loss: 0.2359451229746486, lr: [0.0007323033696543973], time: 9.977083206176758\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 32, train Loss: 0.48399574827479735, valid loss: 0.23455406374198234, lr: [0.0007249803359578533], time: 9.90044641494751\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 33, train Loss: 0.4822081782660673, valid loss: 0.23499941497586305, lr: [0.0007177305325982747], time: 9.960803747177124\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 34, train Loss: 0.4829484610396987, valid loss: 0.23505713432021352, lr: [0.000710553227272292], time: 9.905371904373169\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 35, train Loss: 0.4820297528856238, valid loss: 0.24268920678076503, lr: [0.000703447694999569], time: 9.961188793182373\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 36, train Loss: 0.4830907678747223, valid loss: 0.2434005245024621, lr: [0.0006964132180495733], time: 10.206969261169434\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 37, train Loss: 0.4827390383560573, valid loss: 0.234979531012982, lr: [0.0006894490858690775], time: 9.978480577468872\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 38, train Loss: 0.48163968061754026, valid loss: 0.23455765017222013, lr: [0.0006825545950103868], time: 9.963385820388794\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 39, train Loss: 0.48203851571825385, valid loss: 0.23580604515129944, lr: [0.000675729049060283], time: 9.903259754180908\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 40, train Loss: 0.48173660935709944, valid loss: 0.2349574542356808, lr: [0.0006689717585696801], time: 9.98168396949768\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 41, train Loss: 0.4822966061045064, valid loss: 0.23451518009091415, lr: [0.0006622820409839833], time: 10.527876615524292\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 42, train Loss: 0.4810686969119572, valid loss: 0.23456422105374036, lr: [0.0006556592205741434], time: 10.043290853500366\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 43, train Loss: 0.48234112759149916, valid loss: 0.2444354042855613, lr: [0.0006491026283684019], time: 9.915123224258423\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 44, train Loss: 0.4824950736723049, valid loss: 0.2345572656398572, lr: [0.0006426116020847179], time: 9.96433162689209\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 45, train Loss: 0.482055349722462, valid loss: 0.23465942341187646, lr: [0.0006361854860638707], time: 9.908331632614136\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 46, train Loss: 0.4817669872283477, valid loss: 0.23648350787833702, lr: [0.000629823631203232], time: 9.972413301467896\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 47, train Loss: 0.4811830495218406, valid loss: 0.2376821217895569, lr: [0.0006235253948911997], time: 9.901560306549072\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 48, train Loss: 0.4818201046993556, valid loss: 0.23483301847676527, lr: [0.0006172901409422877], time: 9.969248294830322\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 49, train Loss: 0.481046643648714, valid loss: 0.23458908410437498, lr: [0.0006111172395328649], time: 11.93808650970459\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 50, train Loss: 0.4813592281756559, valid loss: 0.23823223378632347, lr: [0.0006050060671375363], time: 10.09028959274292\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 51, train Loss: 0.4815294416754859, valid loss: 0.2350439350964685, lr: [0.0005989560064661609], time: 9.963690757751465\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 52, train Loss: 0.4816848059893605, valid loss: 0.2344202205040921, lr: [0.0005929664464014993], time: 9.925484657287598\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 53, train Loss: 0.4818822812181233, valid loss: 0.23604368779515336, lr: [0.0005870367819374844], time: 9.979314088821411\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 54, train Loss: 0.4817794187328067, valid loss: 0.23517754217475823, lr: [0.0005811664141181095], time: 9.982948541641235\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 55, train Loss: 0.48261848935832513, valid loss: 0.2404521177938187, lr: [0.0005753547499769285], time: 9.971773624420166\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 56, train Loss: 0.4831655312886857, valid loss: 0.23442136809194647, lr: [0.0005696012024771592], time: 9.922906637191772\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 57, train Loss: 0.4809152058062296, valid loss: 0.23446517241710463, lr: [0.0005639051904523875], time: 9.966169357299805\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 58, train Loss: 0.4814536015860187, valid loss: 0.23720303454899336, lr: [0.0005582661385478637], time: 9.905505895614624\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 59, train Loss: 0.48149751253178175, valid loss: 0.23484419919743502, lr: [0.000552683477162385], time: 9.98644232749939\n",
      "Setting (n,k,m):9, 5, 2 ---  epoch 60, train Loss: 0.4816408015894787, valid loss: 0.23724690872184587, lr: [0.0005471566423907612], time: 9.915148973464966\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 1, train Loss: 19.26804019563835, valid loss: 9.007539192380879, lr: [0.00099], time: 10.061238288879395\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 2, train Loss: 1.1659694088638606, valid loss: 0.8691479659628364, lr: [0.0009801], time: 9.930088996887207\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 3, train Loss: 0.9445650511726028, valid loss: 0.8677812712282681, lr: [0.000970299], time: 9.883918523788452\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 4, train Loss: 0.9433701205734562, valid loss: 0.8657630387682088, lr: [0.0009605960099999999], time: 9.871971845626831\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 5, train Loss: 0.9426488537321138, valid loss: 0.8668624756909339, lr: [0.0009509900498999999], time: 9.951830863952637\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 6, train Loss: 0.9422498175800276, valid loss: 0.8616679181330705, lr: [0.0009414801494009999], time: 9.919260501861572\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 7, train Loss: 0.9403401145409812, valid loss: 0.8640232353377596, lr: [0.0009320653479069899], time: 9.959473133087158\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 8, train Loss: 0.9415632128568697, valid loss: 0.8626334126938281, lr: [0.00092274469442792], time: 9.885029554367065\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 9, train Loss: 0.9407754296703131, valid loss: 0.8617655697118144, lr: [0.0009135172474836408], time: 9.89999008178711\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 10, train Loss: 0.9413813522691691, valid loss: 0.8719217802522577, lr: [0.0009043820750088043], time: 9.8714017868042\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 11, train Loss: 0.9403888894831779, valid loss: 0.8720021555789648, lr: [0.0008953382542587163], time: 9.92707896232605\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 12, train Loss: 0.9399527543132907, valid loss: 0.8608625049400795, lr: [0.0008863848717161291], time: 9.975048303604126\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 13, train Loss: 0.9398936208560594, valid loss: 0.8595158784959512, lr: [0.0008775210229989678], time: 9.880810022354126\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 14, train Loss: 0.9403929186400599, valid loss: 0.8599992399057234, lr: [0.0008687458127689781], time: 9.934428691864014\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 15, train Loss: 0.940084752892854, valid loss: 0.8612893679474605, lr: [0.0008600583546412883], time: 9.894268274307251\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 16, train Loss: 0.9400950821507261, valid loss: 0.8796605545320769, lr: [0.0008514577710948754], time: 9.87950849533081\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 17, train Loss: 0.9405161366445027, valid loss: 0.8639756362279912, lr: [0.0008429431933839266], time: 9.931718826293945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):9, 5, 3 ---  epoch 18, train Loss: 0.9417572133714562, valid loss: 0.8644210769616205, lr: [0.0008345137614500873], time: 9.902558326721191\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 19, train Loss: 0.9386299419247849, valid loss: 0.8593110166389837, lr: [0.0008261686238355864], time: 9.95151972770691\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 20, train Loss: 0.9386678157353087, valid loss: 0.8588170755001285, lr: [0.0008179069375972306], time: 9.882934808731079\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 21, train Loss: 0.9393087206040935, valid loss: 0.8586957770987643, lr: [0.0008097278682212583], time: 9.997518539428711\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 22, train Loss: 0.9409811595594942, valid loss: 0.8572713735635464, lr: [0.0008016305895390457], time: 9.914591789245605\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 23, train Loss: 0.9382317569635877, valid loss: 0.8691289654085168, lr: [0.0007936142836436553], time: 9.90394639968872\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 24, train Loss: 0.9385794290361483, valid loss: 0.8586741034373031, lr: [0.0007856781408072188], time: 9.99676251411438\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 25, train Loss: 0.9374097248835569, valid loss: 0.8644942681381245, lr: [0.0007778213593991466], time: 9.965532064437866\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 26, train Loss: 0.9389377558874091, valid loss: 0.8612791573940732, lr: [0.000770043145805155], time: 10.288530349731445\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 27, train Loss: 0.9378278997790851, valid loss: 0.8594491288567545, lr: [0.0007623427143471034], time: 9.987760782241821\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 28, train Loss: 0.938458322614573, valid loss: 0.8575984443983298, lr: [0.0007547192872036325], time: 9.96227741241455\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 29, train Loss: 0.9376013216547565, valid loss: 0.8585267543811087, lr: [0.0007471720943315961], time: 9.905030488967896\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 30, train Loss: 0.9384421848709655, valid loss: 0.8586301299427074, lr: [0.0007397003733882801], time: 10.006457090377808\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 31, train Loss: 0.9386722560362836, valid loss: 0.8576103604127856, lr: [0.0007323033696543973], time: 9.90737795829773\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 32, train Loss: 0.9372416700343272, valid loss: 0.8685587190426678, lr: [0.0007249803359578533], time: 9.962386131286621\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 33, train Loss: 0.9375347300829725, valid loss: 0.8587277572321272, lr: [0.0007177305325982747], time: 9.922832727432251\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 34, train Loss: 0.9366979069516211, valid loss: 0.8612080041896043, lr: [0.000710553227272292], time: 9.969788312911987\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 35, train Loss: 0.93814551303346, valid loss: 0.8572932896736994, lr: [0.000703447694999569], time: 9.962841272354126\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 36, train Loss: 0.9367455648079557, valid loss: 0.8570141737062649, lr: [0.0006964132180495733], time: 9.94518494606018\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 37, train Loss: 0.9374751221698824, valid loss: 0.865426788833642, lr: [0.0006894490858690775], time: 9.900396823883057\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 38, train Loss: 0.9384449678570047, valid loss: 0.8592884903228537, lr: [0.0006825545950103868], time: 10.66421914100647\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 39, train Loss: 0.9377452684872626, valid loss: 0.8561826645530364, lr: [0.000675729049060283], time: 9.998740911483765\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 40, train Loss: 0.9365112644780186, valid loss: 0.8587668052830864, lr: [0.0006689717585696801], time: 9.906800031661987\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 41, train Loss: 0.9365528519408615, valid loss: 0.8711813762021998, lr: [0.0006622820409839833], time: 9.964543342590332\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 42, train Loss: 0.9443347108260133, valid loss: 0.8568951700735262, lr: [0.0006556592205741434], time: 9.908121109008789\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 43, train Loss: 0.9367129186606918, valid loss: 0.8712925988078019, lr: [0.0006491026283684019], time: 9.921256065368652\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 44, train Loss: 0.9366044367803134, valid loss: 0.8595937452420455, lr: [0.0006426116020847179], time: 9.946859359741211\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 45, train Loss: 0.9355078190220874, valid loss: 0.8564362080932111, lr: [0.0006361854860638707], time: 9.903512954711914\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 46, train Loss: 0.9353856399097862, valid loss: 0.8592061000912723, lr: [0.000629823631203232], time: 9.970322608947754\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 47, train Loss: 0.937255159721101, valid loss: 0.85659469182063, lr: [0.0006235253948911997], time: 10.986177206039429\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 48, train Loss: 0.9359816839446518, valid loss: 0.8727193593193052, lr: [0.0006172901409422877], time: 9.973127841949463\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 49, train Loss: 0.9368357441567715, valid loss: 0.8713854088535192, lr: [0.0006111172395328649], time: 9.967360019683838\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 50, train Loss: 0.9366848253702384, valid loss: 0.8566802389142171, lr: [0.0006050060671375363], time: 9.882736921310425\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 51, train Loss: 0.9351806296664117, valid loss: 0.8571012727194469, lr: [0.0005989560064661609], time: 9.965296983718872\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 52, train Loss: 0.9367648820041218, valid loss: 0.8564369100233645, lr: [0.0005929664464014993], time: 9.90302062034607\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 53, train Loss: 0.9360148433480237, valid loss: 0.8602606959069103, lr: [0.0005870367819374844], time: 9.965404510498047\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 54, train Loss: 0.935481777982685, valid loss: 0.8563258991230458, lr: [0.0005811664141181095], time: 9.893153667449951\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 55, train Loss: 0.93677543572741, valid loss: 0.86042469309349, lr: [0.0005753547499769285], time: 9.954174518585205\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 56, train Loss: 0.9365869840706257, valid loss: 0.8567433431006131, lr: [0.0005696012024771592], time: 9.93665885925293\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 57, train Loss: 0.936607902981033, valid loss: 0.85599778216191, lr: [0.0005639051904523875], time: 9.950664281845093\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 58, train Loss: 0.9361252042998941, valid loss: 0.8653125145475155, lr: [0.0005582661385478637], time: 9.900036334991455\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 59, train Loss: 0.9361586590215318, valid loss: 0.8617980546705856, lr: [0.000552683477162385], time: 12.062884092330933\n",
      "Setting (n,k,m):9, 5, 3 ---  epoch 60, train Loss: 0.9354036755628615, valid loss: 0.8596835861767365, lr: [0.0005471566423907612], time: 10.109562635421753\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 1, train Loss: 5556.917138835418, valid loss: 9.569581269147424, lr: [0.00099], time: 10.086930274963379\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 2, train Loss: 1.9536471247388492, valid loss: 3.470816286284396, lr: [0.0009801], time: 9.948641061782837\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 3, train Loss: 1.8643038686917923, valid loss: 3.4472443651538742, lr: [0.000970299], time: 9.90484094619751\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 4, train Loss: 1.860228270947225, valid loss: 3.5111467716707536, lr: [0.0009605960099999999], time: 9.888972282409668\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 5, train Loss: 1.8588083888925653, valid loss: 3.4432196360751806, lr: [0.0009509900498999999], time: 9.999379873275757\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 6, train Loss: 1.8572552278190981, valid loss: 3.5135315292382745, lr: [0.0009414801494009999], time: 10.160285234451294\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 7, train Loss: 1.8574822371481106, valid loss: 3.4391488443823497, lr: [0.0009320653479069899], time: 10.040262699127197\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 8, train Loss: 1.8574878763000116, valid loss: 3.4451616687106443, lr: [0.00092274469442792], time: 9.897551774978638\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 9, train Loss: 1.8594500617711118, valid loss: 3.4419010711815585, lr: [0.0009135172474836408], time: 10.102129459381104\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 10, train Loss: 1.856680255466384, valid loss: 3.44330031014713, lr: [0.0009043820750088043], time: 10.15291714668274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):9, 5, 4 ---  epoch 11, train Loss: 1.8610012467009607, valid loss: 3.476809500170787, lr: [0.0008953382542587163], time: 10.26911187171936\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 12, train Loss: 1.858769661456674, valid loss: 3.4448531975426158, lr: [0.0008863848717161291], time: 11.084144353866577\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 13, train Loss: 1.8596485378064063, valid loss: 3.487138561924509, lr: [0.0008775210229989678], time: 9.997069835662842\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 14, train Loss: 1.8577281427183374, valid loss: 3.4573541595333324, lr: [0.0008687458127689781], time: 19.918105840682983\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 15, train Loss: 1.859299473839258, valid loss: 3.4684889047532557, lr: [0.0008600583546412883], time: 9.93906307220459\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 16, train Loss: 1.8585982628093398, valid loss: 3.5601364736981567, lr: [0.0008514577710948754], time: 9.944884300231934\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 17, train Loss: 1.8612942214244335, valid loss: 3.4744208998396924, lr: [0.0008429431933839266], time: 9.957362174987793\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 18, train Loss: 1.859881491142268, valid loss: 3.4522791976523277, lr: [0.0008345137614500873], time: 9.940029859542847\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 19, train Loss: 1.859289072420308, valid loss: 3.4438170211066317, lr: [0.0008261686238355864], time: 9.940191984176636\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 20, train Loss: 1.8624096064129325, valid loss: 3.4408839934562585, lr: [0.0008179069375972306], time: 9.966297388076782\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 21, train Loss: 1.8587574518023542, valid loss: 3.4842626814288504, lr: [0.0008097278682212583], time: 9.936132192611694\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 22, train Loss: 1.8596200725703778, valid loss: 3.4463111667456188, lr: [0.0008016305895390457], time: 9.94378137588501\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 23, train Loss: 1.8576251674305906, valid loss: 3.443117519777996, lr: [0.0007936142836436553], time: 9.9585440158844\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 24, train Loss: 1.862390949515567, valid loss: 3.4429915564877622, lr: [0.0007856781408072188], time: 9.934155225753784\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 25, train Loss: 1.8565567090929083, valid loss: 3.445301266141721, lr: [0.0007778213593991466], time: 9.941101789474487\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 26, train Loss: 1.8589047434025903, valid loss: 3.442600931604861, lr: [0.000770043145805155], time: 9.959182024002075\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 27, train Loss: 1.8583731698497985, valid loss: 3.4395469989480976, lr: [0.0007623427143471034], time: 9.941126346588135\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 28, train Loss: 1.8590242843830833, valid loss: 3.4642962273329254, lr: [0.0007547192872036325], time: 9.948639869689941\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 29, train Loss: 1.8556820499021798, valid loss: 3.4391473973127904, lr: [0.0007471720943315961], time: 12.271256446838379\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 30, train Loss: 1.8565387885300393, valid loss: 3.448131764000814, lr: [0.0007397003733882801], time: 9.9788339138031\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 31, train Loss: 1.8577679843549835, valid loss: 3.439704508596715, lr: [0.0007323033696543973], time: 11.005775928497314\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 32, train Loss: 1.856454350143493, valid loss: 3.451345488485069, lr: [0.0007249803359578533], time: 10.316708326339722\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 33, train Loss: 1.8573747233266578, valid loss: 3.4585684127962866, lr: [0.0007177305325982747], time: 10.065579652786255\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 34, train Loss: 1.8554131232559976, valid loss: 3.441581776536619, lr: [0.000710553227272292], time: 11.147315263748169\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 35, train Loss: 1.854808541127973, valid loss: 3.442774486222305, lr: [0.000703447694999569], time: 9.955970764160156\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 36, train Loss: 1.8549262483774593, valid loss: 3.499598079131229, lr: [0.0006964132180495733], time: 9.935976028442383\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 37, train Loss: 1.856082105951583, valid loss: 3.4374937222673854, lr: [0.0006894490858690775], time: 10.065855741500854\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 38, train Loss: 1.8564232537531442, valid loss: 3.4390225392874587, lr: [0.0006825545950103868], time: 9.914676666259766\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 39, train Loss: 1.854569448752148, valid loss: 3.442861359210647, lr: [0.000675729049060283], time: 9.969236373901367\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 40, train Loss: 1.8563561349638464, valid loss: 3.435420822270763, lr: [0.0006689717585696801], time: 9.958246231079102\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 41, train Loss: 1.8549076235652324, valid loss: 3.4406684383044537, lr: [0.0006622820409839833], time: 9.912309646606445\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 42, train Loss: 1.8550555811607945, valid loss: 3.4372917510423817, lr: [0.0006556592205741434], time: 9.962289333343506\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 43, train Loss: 1.856206714792775, valid loss: 3.439580039111469, lr: [0.0006491026283684019], time: 9.884012460708618\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 44, train Loss: 1.855511565577477, valid loss: 3.4344831940592973, lr: [0.0006426116020847179], time: 9.95899248123169\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 45, train Loss: 1.8569329614649694, valid loss: 3.435599039939652, lr: [0.0006361854860638707], time: 9.96666693687439\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 46, train Loss: 1.8550919149954856, valid loss: 3.466075903437718, lr: [0.000629823631203232], time: 9.94668197631836\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 47, train Loss: 1.856842646319445, valid loss: 3.4525733284948865, lr: [0.0006235253948911997], time: 9.952808618545532\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 48, train Loss: 1.8540086831129146, valid loss: 3.440572203119727, lr: [0.0006172901409422877], time: 9.968881368637085\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 49, train Loss: 1.8544127764740292, valid loss: 3.4405010723867195, lr: [0.0006111172395328649], time: 9.958747148513794\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 50, train Loss: 1.8545284581418007, valid loss: 3.4456103912471154, lr: [0.0006050060671375363], time: 9.952204465866089\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 51, train Loss: 1.8554610214214993, valid loss: 3.4359815955151896, lr: [0.0005989560064661609], time: 9.97533917427063\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 52, train Loss: 1.8545301725787686, valid loss: 3.4359792170782906, lr: [0.0005929664464014993], time: 9.953883171081543\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 53, train Loss: 1.8551653697101111, valid loss: 3.435953074703179, lr: [0.0005870367819374844], time: 10.014423131942749\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 54, train Loss: 1.854630449385244, valid loss: 3.4353396357137336, lr: [0.0005811664141181095], time: 10.01229476928711\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 55, train Loss: 1.853332921090058, valid loss: 3.43810955016541, lr: [0.0005753547499769285], time: 9.968909502029419\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 56, train Loss: 1.8548692495606822, valid loss: 3.439110335599682, lr: [0.0005696012024771592], time: 9.946408033370972\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 57, train Loss: 1.8551243456622948, valid loss: 3.4632636564626678, lr: [0.0005639051904523875], time: 9.966355323791504\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 58, train Loss: 1.8551099517604919, valid loss: 3.4495294879411924, lr: [0.0005582661385478637], time: 9.963074922561646\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 59, train Loss: 1.8550771457449342, valid loss: 3.4807614233425896, lr: [0.000552683477162385], time: 9.947478294372559\n",
      "Setting (n,k,m):9, 5, 4 ---  epoch 60, train Loss: 1.854736017001345, valid loss: 3.4458371691920453, lr: [0.0005471566423907612], time: 9.953817129135132\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 1, train Loss: 16.048554095011177, valid loss: 5.506919975698489, lr: [0.00099], time: 15.588306188583374\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 2, train Loss: 0.9149101471967778, valid loss: 0.6689988861130434, lr: [0.0009801], time: 14.911582946777344\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 3, train Loss: 0.8182740282383338, valid loss: 0.664124491882393, lr: [0.000970299], time: 14.84331750869751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):9, 6, 2 ---  epoch 4, train Loss: 0.8157326671724617, valid loss: 0.662234732574527, lr: [0.0009605960099999999], time: 14.94535517692566\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 5, train Loss: 0.8159427076647635, valid loss: 0.6624876540663879, lr: [0.0009509900498999999], time: 14.834798812866211\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 6, train Loss: 0.814005707591598, valid loss: 0.6621297693644078, lr: [0.0009414801494009999], time: 14.990220308303833\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 7, train Loss: 0.8151245478501509, valid loss: 0.6613022630196327, lr: [0.0009320653479069899], time: 14.841039657592773\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 8, train Loss: 0.8148286757764559, valid loss: 0.6608839240369633, lr: [0.00092274469442792], time: 14.916725397109985\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 9, train Loss: 0.8150115977959139, valid loss: 0.671493176623384, lr: [0.0009135172474836408], time: 14.919549703598022\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 10, train Loss: 0.8152886341340986, valid loss: 0.6725511777809982, lr: [0.0009043820750088043], time: 15.86717939376831\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 11, train Loss: 0.8142350484625471, valid loss: 0.6623963079997295, lr: [0.0008953382542587163], time: 14.999058485031128\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 12, train Loss: 0.8151297706659931, valid loss: 0.661036753244068, lr: [0.0008863848717161291], time: 14.925775051116943\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 13, train Loss: 0.8138220916255834, valid loss: 0.6606374337931308, lr: [0.0008775210229989678], time: 14.938389778137207\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 14, train Loss: 0.8142035049355686, valid loss: 0.6742652615378143, lr: [0.0008687458127689781], time: 14.918198585510254\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 15, train Loss: 0.8145515505526049, valid loss: 0.6636144127244944, lr: [0.0008600583546412883], time: 15.005054473876953\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 16, train Loss: 0.8139491201864762, valid loss: 0.6609218915168736, lr: [0.0008514577710948754], time: 14.85488247871399\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 17, train Loss: 0.8146081603170608, valid loss: 0.6693158097717962, lr: [0.0008429431933839266], time: 14.90984845161438\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 18, train Loss: 0.8141432285855587, valid loss: 0.6589253517005011, lr: [0.0008345137614500873], time: 19.37659502029419\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 19, train Loss: 0.8126049198849404, valid loss: 0.6626555282308307, lr: [0.0008261686238355864], time: 16.036589860916138\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 20, train Loss: 0.8150775749125906, valid loss: 0.6603327740920494, lr: [0.0008179069375972306], time: 15.031974792480469\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 21, train Loss: 0.8133246436333339, valid loss: 0.6704596157001856, lr: [0.0008097278682212583], time: 14.892013311386108\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 22, train Loss: 0.8121446541617524, valid loss: 0.6612596596339326, lr: [0.0008016305895390457], time: 14.966223001480103\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 23, train Loss: 0.8132183176333266, valid loss: 0.6614516530204074, lr: [0.0007936142836436553], time: 14.975431203842163\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 24, train Loss: 0.8122958923907219, valid loss: 0.6643373335310978, lr: [0.0007856781408072188], time: 15.020601987838745\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 25, train Loss: 0.8131097141929823, valid loss: 0.659701530458489, lr: [0.0007778213593991466], time: 14.96427607536316\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 26, train Loss: 0.8129578114526056, valid loss: 0.6587306254554969, lr: [0.000770043145805155], time: 14.985041379928589\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 27, train Loss: 0.8129000960292975, valid loss: 0.6629016010381538, lr: [0.0007623427143471034], time: 14.964142560958862\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 28, train Loss: 0.8129909919959027, valid loss: 0.6677199349745467, lr: [0.0007547192872036325], time: 15.019347429275513\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 29, train Loss: 0.8126448627739046, valid loss: 0.6587128917711048, lr: [0.0007471720943315961], time: 14.914322853088379\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 30, train Loss: 0.8119958784054865, valid loss: 0.6602209780415069, lr: [0.0007397003733882801], time: 14.930535316467285\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 31, train Loss: 0.812598744451234, valid loss: 0.6598712298675452, lr: [0.0007323033696543973], time: 14.9592125415802\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 32, train Loss: 0.8133237762965234, valid loss: 0.6628821252165689, lr: [0.0007249803359578533], time: 14.936767816543579\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 33, train Loss: 0.8126100053955176, valid loss: 0.6595137377430015, lr: [0.0007177305325982747], time: 15.016146898269653\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 34, train Loss: 0.8117355101648539, valid loss: 0.6586196439715334, lr: [0.000710553227272292], time: 14.977837324142456\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 35, train Loss: 0.8132736148732822, valid loss: 0.6592762581586171, lr: [0.000703447694999569], time: 14.952103853225708\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 36, train Loss: 0.8124464962660315, valid loss: 0.6648892007374368, lr: [0.0006964132180495733], time: 14.93614411354065\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 37, train Loss: 0.8113036018063893, valid loss: 0.6615888851777915, lr: [0.0006894490858690775], time: 14.942625761032104\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 38, train Loss: 0.8113816290238155, valid loss: 0.6586582761636748, lr: [0.0006825545950103868], time: 15.28644847869873\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 39, train Loss: 0.8113336944381194, valid loss: 0.6710259701215608, lr: [0.000675729049060283], time: 14.861307859420776\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 40, train Loss: 0.8126120337135183, valid loss: 0.6692147505042849, lr: [0.0006689717585696801], time: 14.98316216468811\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 41, train Loss: 0.8122646139015249, valid loss: 0.6583805942930003, lr: [0.0006622820409839833], time: 14.870100498199463\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 42, train Loss: 0.8122137037532817, valid loss: 0.6605169961676534, lr: [0.0006556592205741434], time: 15.634837627410889\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 43, train Loss: 0.811372035271494, valid loss: 0.659355996921586, lr: [0.0006491026283684019], time: 17.799434185028076\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 44, train Loss: 0.811518770028442, valid loss: 0.6621403651763936, lr: [0.0006426116020847179], time: 14.849958658218384\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 45, train Loss: 0.8123868317275611, valid loss: 0.6583164725137887, lr: [0.0006361854860638707], time: 15.107732772827148\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 46, train Loss: 0.8107057234220804, valid loss: 0.6599384348983395, lr: [0.000629823631203232], time: 14.894251108169556\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 47, train Loss: 0.8112706324720637, valid loss: 0.6669010377087187, lr: [0.0006235253948911997], time: 14.899045705795288\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 48, train Loss: 0.8119879546185482, valid loss: 0.6590079816946345, lr: [0.0006172901409422877], time: 14.881059885025024\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 49, train Loss: 0.8122470989971917, valid loss: 0.6599054367766697, lr: [0.0006111172395328649], time: 14.907992124557495\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 50, train Loss: 0.8110871475679206, valid loss: 0.6585723401548484, lr: [0.0006050060671375363], time: 14.966427326202393\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 51, train Loss: 0.8108504376972624, valid loss: 0.6604305710402872, lr: [0.0005989560064661609], time: 14.884264469146729\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 52, train Loss: 0.8124135991145277, valid loss: 0.6585787553095931, lr: [0.0005929664464014993], time: 14.896519660949707\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 53, train Loss: 0.8107967380830066, valid loss: 0.6669985980825595, lr: [0.0005870367819374844], time: 14.937962055206299\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 54, train Loss: 0.8118470522238453, valid loss: 0.6595943632233785, lr: [0.0005811664141181095], time: 14.825902462005615\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 55, train Loss: 0.8121919523114902, valid loss: 0.6586311812061726, lr: [0.0005753547499769285], time: 14.892691850662231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):9, 6, 2 ---  epoch 56, train Loss: 0.8109947972854614, valid loss: 0.6595897002088807, lr: [0.0005696012024771592], time: 14.954359292984009\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 57, train Loss: 0.810775694717926, valid loss: 0.658549333234012, lr: [0.0005639051904523875], time: 14.880738258361816\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 58, train Loss: 0.8107204057183648, valid loss: 0.6631846504634547, lr: [0.0005582661385478637], time: 14.911797761917114\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 59, train Loss: 0.8107323525883688, valid loss: 0.6583291075846441, lr: [0.000552683477162385], time: 14.974687099456787\n",
      "Setting (n,k,m):9, 6, 2 ---  epoch 60, train Loss: 0.8111819131255682, valid loss: 0.6596245748426606, lr: [0.0005471566423907612], time: 14.826801538467407\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 1, train Loss: 23.035296384621148, valid loss: 7.721629520458619, lr: [0.00099], time: 15.11990237236023\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 2, train Loss: 1.891376720190435, valid loss: 3.4487672757653383, lr: [0.0009801], time: 14.84708309173584\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 3, train Loss: 1.8444480201214508, valid loss: 3.4163232974714117, lr: [0.000970299], time: 14.857791900634766\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 4, train Loss: 1.8398151150430944, valid loss: 3.4105753983726546, lr: [0.0009605960099999999], time: 14.785396099090576\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 5, train Loss: 1.8382331634012008, valid loss: 3.403735693466341, lr: [0.0009509900498999999], time: 14.933894634246826\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 6, train Loss: 1.8362561426816002, valid loss: 3.4002829085627746, lr: [0.0009414801494009999], time: 14.878858804702759\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 7, train Loss: 1.8376573282380013, valid loss: 3.4182474844618436, lr: [0.0009320653479069899], time: 14.778775215148926\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 8, train Loss: 1.8366782317452897, valid loss: 3.407336301182373, lr: [0.00092274469442792], time: 14.861682415008545\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 9, train Loss: 1.83805574967911, valid loss: 3.397567705228358, lr: [0.0009135172474836408], time: 14.875044107437134\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 10, train Loss: 1.8367422126457698, valid loss: 3.4021480759009486, lr: [0.0009043820750088043], time: 14.85942816734314\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 11, train Loss: 1.8371409027417689, valid loss: 3.4255970317032034, lr: [0.0008953382542587163], time: 14.944019079208374\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 12, train Loss: 1.8376344751857308, valid loss: 3.4003538525356665, lr: [0.0008863848717161291], time: 14.8993399143219\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 13, train Loss: 1.8362888216565119, valid loss: 3.4048400202706843, lr: [0.0008775210229989678], time: 14.917890071868896\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 14, train Loss: 1.8377204692145634, valid loss: 3.398778817250367, lr: [0.0008687458127689781], time: 14.885412693023682\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 15, train Loss: 1.8369125186305695, valid loss: 3.4071297608746964, lr: [0.0008600583546412883], time: 14.877351760864258\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 16, train Loss: 1.8373970923731233, valid loss: 3.4128584840750595, lr: [0.0008514577710948754], time: 14.857325792312622\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 17, train Loss: 1.8372793844253188, valid loss: 3.3993354556197932, lr: [0.0008429431933839266], time: 14.935818195343018\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 18, train Loss: 1.838330460843192, valid loss: 3.4163906771483887, lr: [0.0008345137614500873], time: 14.77094841003418\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 19, train Loss: 1.8363680154889273, valid loss: 3.3953780782192977, lr: [0.0008261686238355864], time: 14.899847507476807\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 20, train Loss: 1.837244520574269, valid loss: 3.402883358260818, lr: [0.0008179069375972306], time: 15.078021049499512\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 21, train Loss: 1.8384673329020385, valid loss: 3.401096596961468, lr: [0.0008097278682212583], time: 14.908478498458862\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 22, train Loss: 1.8363584776058939, valid loss: 3.396100674174535, lr: [0.0008016305895390457], time: 15.425833702087402\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 23, train Loss: 1.835468908842151, valid loss: 3.413315755244231, lr: [0.0007936142836436553], time: 14.917925596237183\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 24, train Loss: 1.839333557484983, valid loss: 3.4264321943850615, lr: [0.0007856781408072188], time: 14.835784673690796\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 25, train Loss: 1.837860355948872, valid loss: 3.3985015466134914, lr: [0.0007778213593991466], time: 14.787664651870728\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 26, train Loss: 1.8349858877859515, valid loss: 3.3973547682864984, lr: [0.000770043145805155], time: 14.860746383666992\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 27, train Loss: 1.8364142887849093, valid loss: 3.4232453074717784, lr: [0.0007623427143471034], time: 14.915131568908691\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 28, train Loss: 1.8380829904344103, valid loss: 3.3961086225083905, lr: [0.0007547192872036325], time: 14.890717267990112\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 29, train Loss: 1.8363280040804963, valid loss: 3.416053072710214, lr: [0.0007471720943315961], time: 14.916064500808716\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 30, train Loss: 1.8369903886308518, valid loss: 3.395021128486299, lr: [0.0007397003733882801], time: 14.915763854980469\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 31, train Loss: 1.8360473932018506, valid loss: 3.395857796117865, lr: [0.0007323033696543973], time: 14.767322778701782\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 32, train Loss: 1.8377626716164748, valid loss: 3.3947682988317514, lr: [0.0007249803359578533], time: 14.851999044418335\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 33, train Loss: 1.8347048721242736, valid loss: 3.4570084116653765, lr: [0.0007177305325982747], time: 14.796755313873291\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 34, train Loss: 1.8364260538243815, valid loss: 3.39522004784481, lr: [0.000710553227272292], time: 14.845940351486206\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 35, train Loss: 1.8354668859349956, valid loss: 3.3953441047477493, lr: [0.000703447694999569], time: 14.856266498565674\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 36, train Loss: 1.8362088431324324, valid loss: 3.395662203901923, lr: [0.0006964132180495733], time: 14.927586793899536\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 37, train Loss: 1.8382606670058408, valid loss: 3.409483084763855, lr: [0.0006894490858690775], time: 14.76752758026123\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 38, train Loss: 1.8355372770785219, valid loss: 3.3948621162037456, lr: [0.0006825545950103868], time: 14.86185598373413\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 39, train Loss: 1.8359807035178612, valid loss: 3.395001301950355, lr: [0.000675729049060283], time: 14.880902290344238\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 40, train Loss: 1.836018713127888, valid loss: 3.394692397527093, lr: [0.0006689717585696801], time: 14.928346157073975\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 41, train Loss: 1.8359607159604858, valid loss: 3.401056441497142, lr: [0.0006622820409839833], time: 14.811016082763672\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 42, train Loss: 1.8364355603330538, valid loss: 3.395468426209501, lr: [0.0006556592205741434], time: 14.881827592849731\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 43, train Loss: 1.836537740600154, valid loss: 3.447161009166316, lr: [0.0006491026283684019], time: 14.814720630645752\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 44, train Loss: 1.8361513233546176, valid loss: 3.396738335302747, lr: [0.0006426116020847179], time: 14.938382625579834\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 45, train Loss: 1.8362594029980712, valid loss: 3.3996120046725693, lr: [0.0006361854860638707], time: 14.865352630615234\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 46, train Loss: 1.835916180084078, valid loss: 3.4042564967869984, lr: [0.000629823631203232], time: 14.806093215942383\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 47, train Loss: 1.8345100661396974, valid loss: 3.4033831446570115, lr: [0.0006235253948911997], time: 14.887840270996094\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 48, train Loss: 1.8374858244067336, valid loss: 3.395142607973849, lr: [0.0006172901409422877], time: 14.921580791473389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):9, 6, 3 ---  epoch 49, train Loss: 1.8358601349051615, valid loss: 3.4012127264370133, lr: [0.0006111172395328649], time: 14.911018371582031\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 50, train Loss: 1.834473927723797, valid loss: 3.412944897287543, lr: [0.0006050060671375363], time: 14.906076431274414\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 51, train Loss: 1.8343838777720105, valid loss: 3.404058404773768, lr: [0.0005989560064661609], time: 14.902284145355225\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 52, train Loss: 1.8349197579491856, valid loss: 3.395938077052343, lr: [0.0005929664464014993], time: 14.853895425796509\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 53, train Loss: 1.8364177179521128, valid loss: 3.406503594520133, lr: [0.0005870367819374844], time: 14.876619577407837\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 54, train Loss: 1.8363913697294678, valid loss: 3.394777767921939, lr: [0.0005811664141181095], time: 14.915814876556396\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 55, train Loss: 1.83580603410456, valid loss: 3.394766402046308, lr: [0.0005753547499769285], time: 14.880102157592773\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 56, train Loss: 1.835535406674967, valid loss: 3.4387843803722995, lr: [0.0005696012024771592], time: 14.890491724014282\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 57, train Loss: 1.8354609497027725, valid loss: 3.4160367192929635, lr: [0.0005639051904523875], time: 14.87502646446228\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 58, train Loss: 1.8359091843150896, valid loss: 3.414359916272534, lr: [0.0005582661385478637], time: 14.823303699493408\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 59, train Loss: 1.8355291848097042, valid loss: 3.396352303051852, lr: [0.000552683477162385], time: 14.889282703399658\n",
      "Setting (n,k,m):9, 6, 3 ---  epoch 60, train Loss: 1.835422605766135, valid loss: 3.395635832778055, lr: [0.0005471566423907612], time: 14.894428253173828\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 1, train Loss: 7.378167728837279, valid loss: 4.373352868806304, lr: [0.00099], time: 6.0058207511901855\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 2, train Loss: 1.072238822041549, valid loss: 0.860459817461113, lr: [0.0009801], time: 5.863142728805542\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 3, train Loss: 0.9323160867249721, valid loss: 0.8590507937705568, lr: [0.000970299], time: 5.989313125610352\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 4, train Loss: 0.9295996850345036, valid loss: 0.85645839685607, lr: [0.0009605960099999999], time: 5.868896961212158\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 5, train Loss: 0.9282659247413199, valid loss: 0.8560330951602124, lr: [0.0009509900498999999], time: 5.9412314891815186\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 6, train Loss: 0.9283199282764754, valid loss: 0.8558035323156925, lr: [0.0009414801494009999], time: 5.9331653118133545\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 7, train Loss: 0.9280208630352912, valid loss: 0.8556538395780983, lr: [0.0009320653479069899], time: 5.868247985839844\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 8, train Loss: 0.9289230237395228, valid loss: 0.8616360190549, lr: [0.00092274469442792], time: 5.929184675216675\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 9, train Loss: 0.928771010118704, valid loss: 0.8608886234777564, lr: [0.0009135172474836408], time: 5.930704832077026\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 10, train Loss: 0.9290860743649503, valid loss: 0.8562399421804999, lr: [0.0009043820750088043], time: 7.237357139587402\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 11, train Loss: 0.9279605111629166, valid loss: 0.8578721279238793, lr: [0.0008953382542587163], time: 8.315945386886597\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 12, train Loss: 0.9285184804222434, valid loss: 0.8582205244560935, lr: [0.0008863848717161291], time: 7.081726312637329\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 13, train Loss: 0.9277749849478425, valid loss: 0.8557010777402203, lr: [0.0008775210229989678], time: 7.336847305297852\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 14, train Loss: 0.9285209671275345, valid loss: 0.8563827218032944, lr: [0.0008687458127689781], time: 7.78319525718689\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 15, train Loss: 0.9279473187512234, valid loss: 0.8566282581986515, lr: [0.0008600583546412883], time: 6.012348651885986\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 16, train Loss: 0.9283037826070752, valid loss: 0.8561564964450845, lr: [0.0008514577710948754], time: 7.662278175354004\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 17, train Loss: 0.9297264169947126, valid loss: 0.8561373771946524, lr: [0.0008429431933839266], time: 6.930601596832275\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 18, train Loss: 0.9280218382673758, valid loss: 0.8577056689919625, lr: [0.0008345137614500873], time: 6.578278064727783\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 19, train Loss: 0.9288672707207564, valid loss: 0.8705608794353887, lr: [0.0008261686238355864], time: 6.805927991867065\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 20, train Loss: 0.9298039972557749, valid loss: 0.8557867609137891, lr: [0.0008179069375972306], time: 7.344193696975708\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 21, train Loss: 0.9294966410397669, valid loss: 0.857342611905326, lr: [0.0008097278682212583], time: 6.463124990463257\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 22, train Loss: 0.9290283337601971, valid loss: 0.855701842876582, lr: [0.0008016305895390457], time: 6.50541877746582\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 23, train Loss: 0.9269167205788433, valid loss: 0.855813431499007, lr: [0.0007936142836436553], time: 7.112222909927368\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 24, train Loss: 0.9290015077473427, valid loss: 0.8553111011815889, lr: [0.0007856781408072188], time: 6.546742677688599\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 25, train Loss: 0.9274129291985206, valid loss: 0.8549821896598185, lr: [0.0007778213593991466], time: 7.000577211380005\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 26, train Loss: 0.9271612363175958, valid loss: 0.8557684049454771, lr: [0.000770043145805155], time: 5.930981636047363\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 27, train Loss: 0.9286579196245525, valid loss: 0.8552196190859948, lr: [0.0007623427143471034], time: 5.86023736000061\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 28, train Loss: 0.9297898250984953, valid loss: 0.8549257125205882, lr: [0.0007547192872036325], time: 5.945770740509033\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 29, train Loss: 0.9283037021004736, valid loss: 0.8549972507352235, lr: [0.0007471720943315961], time: 6.997334718704224\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 30, train Loss: 0.9280863324670018, valid loss: 0.8556516148736828, lr: [0.0007397003733882801], time: 6.000096559524536\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 31, train Loss: 0.9279459894327717, valid loss: 0.8576519138528351, lr: [0.0007323033696543973], time: 5.872929573059082\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 32, train Loss: 0.9280577326720352, valid loss: 0.8563500589477772, lr: [0.0007249803359578533], time: 6.599009275436401\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 33, train Loss: 0.9269697480650503, valid loss: 0.8566842979596179, lr: [0.0007177305325982747], time: 9.107257843017578\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 34, train Loss: 0.9289181328258567, valid loss: 0.8559094367782253, lr: [0.000710553227272292], time: 6.841514348983765\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 35, train Loss: 0.9276545126914691, valid loss: 0.8567774881414733, lr: [0.000703447694999569], time: 7.914758920669556\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 36, train Loss: 0.9281358652172443, valid loss: 0.8548553568593488, lr: [0.0006964132180495733], time: 6.037196636199951\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 37, train Loss: 0.9286339977668452, valid loss: 0.8640336134237978, lr: [0.0006894490858690775], time: 5.867823362350464\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 38, train Loss: 0.928144950412041, valid loss: 0.8549374021278479, lr: [0.0006825545950103868], time: 8.216864824295044\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 39, train Loss: 0.9279410767148795, valid loss: 0.8590512136692369, lr: [0.000675729049060283], time: 6.679964065551758\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 40, train Loss: 0.9268796359277468, valid loss: 0.8553281012608743, lr: [0.0006689717585696801], time: 8.394505500793457\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 41, train Loss: 0.9277714190079994, valid loss: 0.8562776048197422, lr: [0.0006622820409839833], time: 8.291112899780273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 4, 2 ---  epoch 42, train Loss: 0.927438587983538, valid loss: 0.857225525743923, lr: [0.0006556592205741434], time: 6.770611047744751\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 43, train Loss: 0.9278530852592017, valid loss: 0.8566445009502766, lr: [0.0006491026283684019], time: 6.251923561096191\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 44, train Loss: 0.9270033651521082, valid loss: 0.8556566781119583, lr: [0.0006426116020847179], time: 7.37072491645813\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 45, train Loss: 0.9284686123470721, valid loss: 0.855557612425049, lr: [0.0006361854860638707], time: 6.152777910232544\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 46, train Loss: 0.9271846593907183, valid loss: 0.8550213532541872, lr: [0.000629823631203232], time: 5.950651407241821\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 47, train Loss: 0.9278035636626274, valid loss: 0.8585883744971379, lr: [0.0006235253948911997], time: 5.870304346084595\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 48, train Loss: 0.9271387161737834, valid loss: 0.8560246812613597, lr: [0.0006172901409422877], time: 5.924686670303345\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 49, train Loss: 0.9270781499173497, valid loss: 0.8620021990039067, lr: [0.0006111172395328649], time: 5.886552095413208\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 50, train Loss: 0.9288719408387266, valid loss: 0.8554744633672491, lr: [0.0006050060671375363], time: 6.372505187988281\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 51, train Loss: 0.927500947700811, valid loss: 0.855785824658373, lr: [0.0005989560064661609], time: 6.359235048294067\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 52, train Loss: 0.9269450800901531, valid loss: 0.8558884093078072, lr: [0.0005929664464014993], time: 5.936276912689209\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 53, train Loss: 0.9271864323185413, valid loss: 0.8556319452753773, lr: [0.0005870367819374844], time: 5.866441011428833\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 54, train Loss: 0.9274427485052444, valid loss: 0.8547859246359214, lr: [0.0005811664141181095], time: 5.956931114196777\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 55, train Loss: 0.9271154192448665, valid loss: 0.8551231880749338, lr: [0.0005753547499769285], time: 5.886383533477783\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 56, train Loss: 0.9274738951749577, valid loss: 0.8549017070361205, lr: [0.0005696012024771592], time: 5.9399824142456055\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 57, train Loss: 0.928167572488923, valid loss: 0.8570071720333498, lr: [0.0005639051904523875], time: 5.927342891693115\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 58, train Loss: 0.9272413024625948, valid loss: 0.8558690523528323, lr: [0.0005582661385478637], time: 5.861201524734497\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 59, train Loss: 0.9284430981139536, valid loss: 0.8552557490761967, lr: [0.000552683477162385], time: 5.864091396331787\n",
      "Setting (n,k,m):10, 4, 2 ---  epoch 60, train Loss: 0.9271644777739327, valid loss: 0.8553452351647554, lr: [0.0005471566423907612], time: 6.014594554901123\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 1, train Loss: 30.887415667580473, valid loss: 3.3935443108832235, lr: [0.00099], time: 7.093717098236084\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 2, train Loss: 0.7175635918487681, valid loss: 0.11556296260127451, lr: [0.0009801], time: 6.374219179153442\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 3, train Loss: 0.3444898370512033, valid loss: 0.11163974820752014, lr: [0.000970299], time: 5.977307319641113\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 4, train Loss: 0.33905740140593216, valid loss: 0.11050475755189966, lr: [0.0009605960099999999], time: 6.044912576675415\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 5, train Loss: 0.3365498737994183, valid loss: 0.1097359666955296, lr: [0.0009509900498999999], time: 5.914574861526489\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 6, train Loss: 0.33504323915120765, valid loss: 0.11072437059919946, lr: [0.0009414801494009999], time: 5.97089147567749\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 7, train Loss: 0.3353782044784835, valid loss: 0.11114014364744801, lr: [0.0009320653479069899], time: 5.966756343841553\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 8, train Loss: 0.33458096085987854, valid loss: 0.1097250073605282, lr: [0.00092274469442792], time: 5.909703731536865\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 9, train Loss: 0.33435007449190324, valid loss: 0.10921661760582294, lr: [0.0009135172474836408], time: 5.975629568099976\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 10, train Loss: 0.33415897971302516, valid loss: 0.11239384643028491, lr: [0.0009043820750088043], time: 5.966488838195801\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 11, train Loss: 0.33531670185935014, valid loss: 0.11572786998353808, lr: [0.0008953382542587163], time: 6.132621765136719\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 12, train Loss: 0.3341932423209495, valid loss: 0.10871518771066806, lr: [0.0008863848717161291], time: 7.493364572525024\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 13, train Loss: 0.3331842066153534, valid loss: 0.11238553744348738, lr: [0.0008775210229989678], time: 5.963924169540405\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 14, train Loss: 0.3334669316850289, valid loss: 0.11746029492879176, lr: [0.0008687458127689781], time: 5.970883369445801\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 15, train Loss: 0.33396314486082895, valid loss: 0.10840547211591241, lr: [0.0008600583546412883], time: 5.93309760093689\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 16, train Loss: 0.3325079350592979, valid loss: 0.10871483325673846, lr: [0.0008514577710948754], time: 5.966871738433838\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 17, train Loss: 0.33282432122728733, valid loss: 0.13754583562211678, lr: [0.0008429431933839266], time: 5.898056983947754\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 18, train Loss: 0.33535438107742965, valid loss: 0.11318294792793518, lr: [0.0008345137614500873], time: 5.963447570800781\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 19, train Loss: 0.3337674197327236, valid loss: 0.10914260321072572, lr: [0.0008261686238355864], time: 5.916795015335083\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 20, train Loss: 0.3321755899865505, valid loss: 0.11027058543441787, lr: [0.0008179069375972306], time: 6.005902290344238\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 21, train Loss: 0.33301470945684014, valid loss: 0.11194738760549178, lr: [0.0008097278682212583], time: 5.91463303565979\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 22, train Loss: 0.3327838623161876, valid loss: 0.10866895118513649, lr: [0.0008016305895390457], time: 5.98406982421875\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 23, train Loss: 0.3324981281577885, valid loss: 0.11638829945963772, lr: [0.0007936142836436553], time: 5.985041856765747\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 24, train Loss: 0.33493650627009147, valid loss: 0.12004298007771556, lr: [0.0007856781408072188], time: 5.927441835403442\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 25, train Loss: 0.333284592469885, valid loss: 0.129024025854264, lr: [0.0007778213593991466], time: 5.999332904815674\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 26, train Loss: 0.3335688538669359, valid loss: 0.1082489391035639, lr: [0.000770043145805155], time: 6.003998041152954\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 27, train Loss: 0.3329697669129257, valid loss: 0.10873074421872231, lr: [0.0007623427143471034], time: 5.9277424812316895\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 28, train Loss: 0.33311694485986376, valid loss: 0.10893824367906856, lr: [0.0007547192872036325], time: 5.98233962059021\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 29, train Loss: 0.33364716135575667, valid loss: 0.1108695146428971, lr: [0.0007471720943315961], time: 5.914637804031372\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 30, train Loss: 0.3330406408808655, valid loss: 0.1341085591517094, lr: [0.0007397003733882801], time: 6.004750490188599\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 31, train Loss: 0.3358751563602963, valid loss: 0.11111026693290715, lr: [0.0007323033696543973], time: 5.992241859436035\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 32, train Loss: 0.332933342050018, valid loss: 0.1209321899492403, lr: [0.0007249803359578533], time: 5.919444561004639\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 33, train Loss: 0.33363997589490946, valid loss: 0.1832255549378029, lr: [0.0007177305325982747], time: 5.974673748016357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 4, 3 ---  epoch 34, train Loss: 0.3399625643107303, valid loss: 0.1130944105431353, lr: [0.000710553227272292], time: 5.900914907455444\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 35, train Loss: 0.3326146462236148, valid loss: 0.10833574706877752, lr: [0.000703447694999569], time: 5.9831438064575195\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 36, train Loss: 0.3325837047334801, valid loss: 0.12246673621754706, lr: [0.0006964132180495733], time: 5.981637716293335\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 37, train Loss: 0.3332400683753829, valid loss: 0.11856423844911242, lr: [0.0006894490858690775], time: 5.905420303344727\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 38, train Loss: 0.33283366321626245, valid loss: 0.10829918264356073, lr: [0.0006825545950103868], time: 5.968281984329224\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 39, train Loss: 0.3327744665088475, valid loss: 0.10981880264550123, lr: [0.000675729049060283], time: 5.962315082550049\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 40, train Loss: 0.33215979078182056, valid loss: 0.1091090459035029, lr: [0.0006689717585696801], time: 5.927671909332275\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 41, train Loss: 0.33355818936184745, valid loss: 0.11858573347983141, lr: [0.0006622820409839833], time: 5.975614309310913\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 42, train Loss: 0.33381772585082375, valid loss: 0.11597233001272828, lr: [0.0006556592205741434], time: 5.9023003578186035\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 43, train Loss: 0.3329845376929818, valid loss: 0.10844199652498336, lr: [0.0006491026283684019], time: 5.966426849365234\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 44, train Loss: 0.33246546088947637, valid loss: 0.1350718043935307, lr: [0.0006426116020847179], time: 5.975667953491211\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 45, train Loss: 0.33463603983705914, valid loss: 0.12832626100431416, lr: [0.0006361854860638707], time: 5.922018766403198\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 46, train Loss: 0.3346046862126413, valid loss: 0.1147183025434672, lr: [0.000629823631203232], time: 5.973074197769165\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 47, train Loss: 0.3322612694521323, valid loss: 0.109453128706631, lr: [0.0006235253948911997], time: 5.90271520614624\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 48, train Loss: 0.3315099005315324, valid loss: 0.10840670938369261, lr: [0.0006172901409422877], time: 5.967627286911011\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 49, train Loss: 0.33147272907298886, valid loss: 0.17088431704343282, lr: [0.0006111172395328649], time: 5.974946975708008\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 50, train Loss: 0.33654404323500303, valid loss: 0.11599130383341232, lr: [0.0006050060671375363], time: 5.926387071609497\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 51, train Loss: 0.33151133736754035, valid loss: 0.11237648277668799, lr: [0.0005989560064661609], time: 6.030801057815552\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 52, train Loss: 0.333701520514966, valid loss: 0.10942059357263116, lr: [0.0005929664464014993], time: 5.969627618789673\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 53, train Loss: 0.3307603335423233, valid loss: 0.12454323371731574, lr: [0.0005870367819374844], time: 5.9103453159332275\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 54, train Loss: 0.3329492987828479, valid loss: 0.10881561751747565, lr: [0.0005811664141181095], time: 5.989517450332642\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 55, train Loss: 0.3309633834780243, valid loss: 0.10857098079369691, lr: [0.0005753547499769285], time: 5.899259328842163\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 56, train Loss: 0.3309216929737841, valid loss: 0.11831981659901686, lr: [0.0005696012024771592], time: 5.97786808013916\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 57, train Loss: 0.3322740916222131, valid loss: 0.11090884870644188, lr: [0.0005639051904523875], time: 5.972943305969238\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 58, train Loss: 0.3320404947170926, valid loss: 0.11610554135778692, lr: [0.0005582661385478637], time: 5.903779983520508\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 59, train Loss: 0.33257582434720195, valid loss: 0.12726807602065904, lr: [0.000552683477162385], time: 5.990057468414307\n",
      "Setting (n,k,m):10, 4, 3 ---  epoch 60, train Loss: 0.3334195789956475, valid loss: 0.14873161833991155, lr: [0.0005471566423907612], time: 5.904202222824097\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 1, train Loss: 28.403952457237697, valid loss: 1.9464599666813767, lr: [0.00099], time: 6.056121349334717\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 2, train Loss: 0.6871813057096658, valid loss: 0.3037561026980453, lr: [0.0009801], time: 5.963426351547241\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 3, train Loss: 0.5536206116413863, valid loss: 0.29821836833949045, lr: [0.000970299], time: 5.900001525878906\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 4, train Loss: 0.5477801112250504, valid loss: 0.2961341761298909, lr: [0.0009605960099999999], time: 5.970181465148926\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 5, train Loss: 0.5443112191940886, valid loss: 0.2951747946681841, lr: [0.0009509900498999999], time: 5.84425950050354\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 6, train Loss: 0.5430516810301255, valid loss: 0.29389173573058786, lr: [0.0009414801494009999], time: 5.92005729675293\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 7, train Loss: 0.5424719715297788, valid loss: 0.2950140699524823, lr: [0.0009320653479069899], time: 5.972072601318359\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 8, train Loss: 0.5423986365090431, valid loss: 0.2940253286427348, lr: [0.00092274469442792], time: 5.900262355804443\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 9, train Loss: 0.5420261015031198, valid loss: 0.29351197487482394, lr: [0.0009135172474836408], time: 5.97775411605835\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 10, train Loss: 0.5414283120463861, valid loss: 0.29363622995097916, lr: [0.0009043820750088043], time: 5.902879476547241\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 11, train Loss: 0.5416687219844669, valid loss: 0.2944942592866009, lr: [0.0008953382542587163], time: 5.978863000869751\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 12, train Loss: 0.541398220031036, valid loss: 0.29382347332235714, lr: [0.0008863848717161291], time: 5.984867572784424\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 13, train Loss: 0.5427720976635335, valid loss: 0.29340444789757725, lr: [0.0008775210229989678], time: 5.912136077880859\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 14, train Loss: 0.5415607077996776, valid loss: 0.2933259217902251, lr: [0.0008687458127689781], time: 5.982128381729126\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 15, train Loss: 0.5416058785711954, valid loss: 0.2940685496675642, lr: [0.0008600583546412883], time: 5.90461540222168\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 16, train Loss: 0.5411439131439467, valid loss: 0.293600177183457, lr: [0.0008514577710948754], time: 5.991627931594849\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 17, train Loss: 0.5415964777345702, valid loss: 0.2933139687752797, lr: [0.0008429431933839266], time: 5.912502288818359\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 18, train Loss: 0.5407475955999943, valid loss: 0.29635553576694185, lr: [0.0008345137614500873], time: 5.972379922866821\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 19, train Loss: 0.5415471230321608, valid loss: 0.2945788146956189, lr: [0.0008261686238355864], time: 5.8962461948394775\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 20, train Loss: 0.5417795460796518, valid loss: 0.2930012940168614, lr: [0.0008179069375972306], time: 5.979069232940674\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 21, train Loss: 0.5416051893490784, valid loss: 0.2949066836758003, lr: [0.0008097278682212583], time: 5.93080472946167\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 22, train Loss: 0.5413325315236864, valid loss: 0.2927813466512745, lr: [0.0008016305895390457], time: 5.923539400100708\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 23, train Loss: 0.5406915117107137, valid loss: 0.2948238421338336, lr: [0.0007936142836436553], time: 5.971576690673828\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 24, train Loss: 0.5423690067020897, valid loss: 0.2978207635982795, lr: [0.0007856781408072188], time: 5.885362863540649\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 25, train Loss: 0.5425357351414114, valid loss: 0.2925594202781715, lr: [0.0007778213593991466], time: 5.893992185592651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 4, 4 ---  epoch 26, train Loss: 0.5409501702392554, valid loss: 0.29327760372906186, lr: [0.000770043145805155], time: 5.977016448974609\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 27, train Loss: 0.541659759426833, valid loss: 0.29360371747792935, lr: [0.0007623427143471034], time: 5.892918109893799\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 28, train Loss: 0.5407145994342213, valid loss: 0.29239389007387095, lr: [0.0007547192872036325], time: 5.968763589859009\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 29, train Loss: 0.5414413752297735, valid loss: 0.29562655208790406, lr: [0.0007471720943315961], time: 5.893118143081665\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 30, train Loss: 0.5403002958669001, valid loss: 0.2957824217989389, lr: [0.0007397003733882801], time: 5.892781972885132\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 31, train Loss: 0.5410847566349889, valid loss: 0.29268859044955353, lr: [0.0007323033696543973], time: 5.981759786605835\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 32, train Loss: 0.5396023023934463, valid loss: 0.2925411863552162, lr: [0.0007249803359578533], time: 5.897152900695801\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 33, train Loss: 0.5408007402675263, valid loss: 0.29300112645979426, lr: [0.0007177305325982747], time: 5.954807281494141\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 34, train Loss: 0.5407287026533731, valid loss: 0.29379668344425397, lr: [0.000710553227272292], time: 5.90013861656189\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 35, train Loss: 0.5402933336412431, valid loss: 0.2923634039781816, lr: [0.000703447694999569], time: 5.980584621429443\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 36, train Loss: 0.5398780044341418, valid loss: 0.29319040932893176, lr: [0.0006964132180495733], time: 5.930763244628906\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 37, train Loss: 0.5404657677450772, valid loss: 0.2925335459721659, lr: [0.0006894490858690775], time: 5.979549407958984\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 38, train Loss: 0.5416509155141274, valid loss: 0.2929906510828102, lr: [0.0006825545950103868], time: 6.318029880523682\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 39, train Loss: 0.5422739191088457, valid loss: 0.2920491361198391, lr: [0.000675729049060283], time: 5.921961069107056\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 40, train Loss: 0.5411112923135608, valid loss: 0.292104178118188, lr: [0.0006689717585696801], time: 5.976879358291626\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 41, train Loss: 0.539633553692129, valid loss: 0.2941912382372693, lr: [0.0006622820409839833], time: 5.913130760192871\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 42, train Loss: 0.5410251356995313, valid loss: 0.293591100819939, lr: [0.0006556592205741434], time: 5.987887382507324\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 43, train Loss: 0.5392099485564755, valid loss: 0.29462667308676393, lr: [0.0006491026283684019], time: 5.841663122177124\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 44, train Loss: 0.5397629751405257, valid loss: 0.29230717698644093, lr: [0.0006426116020847179], time: 5.976226091384888\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 45, train Loss: 0.5396855384812288, valid loss: 0.2920797886908911, lr: [0.0006361854860638707], time: 5.905776023864746\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 46, train Loss: 0.540032264124057, valid loss: 0.30150732411609127, lr: [0.000629823631203232], time: 5.992051839828491\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 47, train Loss: 0.5400951649976944, valid loss: 0.2920336696167348, lr: [0.0006235253948911997], time: 5.932642698287964\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 48, train Loss: 0.5402782448555609, valid loss: 0.2947682151208856, lr: [0.0006172901409422877], time: 5.9756247997283936\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 49, train Loss: 0.5397331830265295, valid loss: 0.29256670230557774, lr: [0.0006111172395328649], time: 5.976524591445923\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 50, train Loss: 0.5394705759409257, valid loss: 0.29343770600633323, lr: [0.0006050060671375363], time: 5.906454563140869\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 51, train Loss: 0.5398707767427613, valid loss: 0.2921732659562608, lr: [0.0005989560064661609], time: 5.994183301925659\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 52, train Loss: 0.5393899601747623, valid loss: 0.29219058042256707, lr: [0.0005929664464014993], time: 5.996201276779175\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 53, train Loss: 0.5405022940863303, valid loss: 0.29243729839273874, lr: [0.0005870367819374844], time: 6.318251132965088\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 54, train Loss: 0.5387140322275381, valid loss: 0.29230044150646955, lr: [0.0005811664141181095], time: 5.911365270614624\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 55, train Loss: 0.5387197868758445, valid loss: 0.29310057409796286, lr: [0.0005753547499769285], time: 5.975064516067505\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 56, train Loss: 0.5391921673626543, valid loss: 0.2924976393239828, lr: [0.0005696012024771592], time: 5.986318588256836\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 57, train Loss: 0.5390363576252486, valid loss: 0.2950748609399264, lr: [0.0005639051904523875], time: 5.923208713531494\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 58, train Loss: 0.5395136217974463, valid loss: 0.29297923398877734, lr: [0.0005582661385478637], time: 5.972494840621948\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 59, train Loss: 0.5400832961427808, valid loss: 0.2922614000478745, lr: [0.000552683477162385], time: 5.84041428565979\n",
      "Setting (n,k,m):10, 4, 4 ---  epoch 60, train Loss: 0.5394634400521134, valid loss: 0.29324115724969363, lr: [0.0005471566423907612], time: 5.980252265930176\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 1, train Loss: 21.15139890123202, valid loss: 8.483685438722226, lr: [0.00099], time: 5.939886569976807\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 2, train Loss: 1.263697777594024, valid loss: 0.9308190038716031, lr: [0.0009801], time: 5.975182294845581\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 3, train Loss: 0.9721990141343291, valid loss: 0.9283867267548972, lr: [0.000970299], time: 5.979318380355835\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 4, train Loss: 0.9713696111264465, valid loss: 0.9260564005414315, lr: [0.0009605960099999999], time: 5.906203985214233\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 5, train Loss: 0.9699093591016548, valid loss: 0.9253136793212428, lr: [0.0009509900498999999], time: 5.966574192047119\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 6, train Loss: 0.9704747042221997, valid loss: 0.9266655633484137, lr: [0.0009414801494009999], time: 5.97249960899353\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 7, train Loss: 0.9693403292090187, valid loss: 0.9253802420896388, lr: [0.0009320653479069899], time: 5.885674715042114\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 8, train Loss: 0.9727990544995957, valid loss: 0.9252103898648854, lr: [0.00092274469442792], time: 5.962286710739136\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 9, train Loss: 0.9698588450397848, valid loss: 0.9269918027733054, lr: [0.0009135172474836408], time: 5.889571189880371\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 10, train Loss: 0.9692685979433648, valid loss: 0.9267353169632037, lr: [0.0009043820750088043], time: 5.948030471801758\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 11, train Loss: 0.9707936137521654, valid loss: 0.9263126494742444, lr: [0.0008953382542587163], time: 5.960522651672363\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 12, train Loss: 0.9700663314519294, valid loss: 0.9252231826825397, lr: [0.0008863848717161291], time: 5.89221978187561\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 13, train Loss: 0.9699421249451353, valid loss: 0.9257325253282553, lr: [0.0008775210229989678], time: 5.974975109100342\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 14, train Loss: 0.9718477546942638, valid loss: 0.9263228209754568, lr: [0.0008687458127689781], time: 5.8824450969696045\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 15, train Loss: 0.9699934024868209, valid loss: 0.9244880023371549, lr: [0.0008600583546412883], time: 5.959927797317505\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 16, train Loss: 0.9710988001206076, valid loss: 0.9267871502774416, lr: [0.0008514577710948754], time: 5.9397289752960205\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 17, train Loss: 0.9716480277026965, valid loss: 0.924596159238803, lr: [0.0008429431933839266], time: 5.8727498054504395\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 18, train Loss: 0.9721710751204078, valid loss: 0.9291823134476266, lr: [0.0008345137614500873], time: 5.955101251602173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 4, 5 ---  epoch 19, train Loss: 0.9711523160495396, valid loss: 0.9253994784134005, lr: [0.0008261686238355864], time: 5.872092008590698\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 20, train Loss: 0.9699166812676308, valid loss: 0.9284330503179478, lr: [0.0008179069375972306], time: 5.937734842300415\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 21, train Loss: 0.9710876423213458, valid loss: 0.9254477925285595, lr: [0.0008097278682212583], time: 5.935869455337524\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 22, train Loss: 0.9690787810475915, valid loss: 0.9276165006770901, lr: [0.0008016305895390457], time: 5.873995780944824\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 23, train Loss: 0.9732011586719025, valid loss: 0.9293964937782732, lr: [0.0007936142836436553], time: 5.96037220954895\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 24, train Loss: 0.9698344373164862, valid loss: 0.9331456495566582, lr: [0.0007856781408072188], time: 5.940135955810547\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 25, train Loss: 0.9691133737717194, valid loss: 0.9239727879420371, lr: [0.0007778213593991466], time: 5.892623662948608\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 26, train Loss: 0.9721252758586324, valid loss: 0.9237311880617882, lr: [0.000770043145805155], time: 5.95268440246582\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 27, train Loss: 0.9704271598411395, valid loss: 0.9261298067543028, lr: [0.0007623427143471034], time: 5.947783708572388\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 28, train Loss: 0.9700961908035703, valid loss: 0.9240324942763309, lr: [0.0007547192872036325], time: 5.95988655090332\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 29, train Loss: 0.9700341160389382, valid loss: 0.9570084840008961, lr: [0.0007471720943315961], time: 5.8785107135772705\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 30, train Loss: 0.9704014019226177, valid loss: 0.9244162168364721, lr: [0.0007397003733882801], time: 5.945133686065674\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 31, train Loss: 0.9726575325934905, valid loss: 0.9252168557374725, lr: [0.0007323033696543973], time: 5.949319839477539\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 32, train Loss: 0.9687941844399439, valid loss: 0.9246395965422687, lr: [0.0007249803359578533], time: 5.879895925521851\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 33, train Loss: 0.9698151336397162, valid loss: 0.9244264644583402, lr: [0.0007177305325982747], time: 5.959198236465454\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 34, train Loss: 0.9698341696889913, valid loss: 0.9295716970075644, lr: [0.000710553227272292], time: 5.932622909545898\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 35, train Loss: 0.9691729677938884, valid loss: 0.927086623970206, lr: [0.000703447694999569], time: 5.86405611038208\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 36, train Loss: 0.9703998198785267, valid loss: 0.9307663211372029, lr: [0.0006964132180495733], time: 5.9264655113220215\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 37, train Loss: 0.969912544057663, valid loss: 0.9344080981734155, lr: [0.0006894490858690775], time: 5.929173231124878\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 38, train Loss: 0.9706523723089707, valid loss: 0.9287525516421299, lr: [0.0006825545950103868], time: 5.882567644119263\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 39, train Loss: 0.968884465561696, valid loss: 0.9259077520850661, lr: [0.000675729049060283], time: 5.927030324935913\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 40, train Loss: 0.9711565879446301, valid loss: 0.927543923290544, lr: [0.0006689717585696801], time: 5.864659786224365\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 41, train Loss: 0.9704713261948171, valid loss: 0.9235106331523978, lr: [0.0006622820409839833], time: 5.941373109817505\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 42, train Loss: 0.9717783886389628, valid loss: 0.9240259029395358, lr: [0.0006556592205741434], time: 5.927145957946777\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 43, train Loss: 0.9686827490716785, valid loss: 0.9239165121952388, lr: [0.0006491026283684019], time: 5.879740476608276\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 44, train Loss: 0.9711656623201225, valid loss: 0.9239629620751558, lr: [0.0006426116020847179], time: 5.928068399429321\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 45, train Loss: 0.9708998370556399, valid loss: 0.9236375048260711, lr: [0.0006361854860638707], time: 5.864539623260498\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 46, train Loss: 0.9686183642727196, valid loss: 0.9242117760797441, lr: [0.000629823631203232], time: 5.9267237186431885\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 47, train Loss: 0.9698411852906137, valid loss: 0.9239618511327836, lr: [0.0006235253948911997], time: 5.932027101516724\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 48, train Loss: 0.9697395148013027, valid loss: 0.9288757817095947, lr: [0.0006172901409422877], time: 5.877800703048706\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 49, train Loss: 0.9711837777514478, valid loss: 0.9262917150617667, lr: [0.0006111172395328649], time: 5.941896438598633\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 50, train Loss: 0.968820991591807, valid loss: 0.9285930832219547, lr: [0.0006050060671375363], time: 5.951167345046997\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 51, train Loss: 0.9691876140056433, valid loss: 0.9231858364306914, lr: [0.0005989560064661609], time: 5.890020370483398\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 52, train Loss: 0.972199491744665, valid loss: 0.9235608228108093, lr: [0.0005929664464014993], time: 5.94945502281189\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 53, train Loss: 0.9709371101289304, valid loss: 0.9235115804590577, lr: [0.0005870367819374844], time: 5.960890054702759\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 54, train Loss: 0.9696565305758691, valid loss: 0.9299125736863234, lr: [0.0005811664141181095], time: 5.943579196929932\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 55, train Loss: 0.9702772303982184, valid loss: 0.9233813991819293, lr: [0.0005753547499769285], time: 5.883938312530518\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 56, train Loss: 0.9681035765655653, valid loss: 0.9242043753948886, lr: [0.0005696012024771592], time: 5.941876411437988\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 57, train Loss: 0.9716449394065331, valid loss: 0.9237530320569611, lr: [0.0005639051904523875], time: 5.947024822235107\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 58, train Loss: 0.9696481784959782, valid loss: 0.9254013970806105, lr: [0.0005582661385478637], time: 5.8821702003479\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 59, train Loss: 0.9685014818273339, valid loss: 0.9233475788847803, lr: [0.000552683477162385], time: 5.965331315994263\n",
      "Setting (n,k,m):10, 4, 5 ---  epoch 60, train Loss: 0.9706235195582815, valid loss: 0.9301251706544846, lr: [0.0005471566423907612], time: 5.946972608566284\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 1, train Loss: 32.66474580697344, valid loss: 9.091368881716285, lr: [0.00099], time: 5.929088830947876\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 2, train Loss: 1.9962574575511143, valid loss: 3.438669718891685, lr: [0.0009801], time: 5.946451663970947\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 3, train Loss: 1.8584364145286572, valid loss: 3.4243860990232537, lr: [0.000970299], time: 5.944486618041992\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 4, train Loss: 1.8523084705691986, valid loss: 3.4133851262908563, lr: [0.0009605960099999999], time: 6.291761636734009\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 5, train Loss: 1.851170931734953, valid loss: 3.4128214362298555, lr: [0.0009509900498999999], time: 6.594208478927612\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 6, train Loss: 1.8523587476980534, valid loss: 3.4229559493745585, lr: [0.0009414801494009999], time: 5.919294595718384\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 7, train Loss: 1.852357449271294, valid loss: 3.419689342441438, lr: [0.0009320653479069899], time: 5.854185581207275\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 8, train Loss: 1.8508553424972531, valid loss: 3.4125503402440462, lr: [0.00092274469442792], time: 5.929300785064697\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 9, train Loss: 1.8502917571713544, valid loss: 3.4227770580719525, lr: [0.0009135172474836408], time: 5.922835826873779\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 10, train Loss: 1.8516828424590894, valid loss: 3.4119159499713394, lr: [0.0009043820750088043], time: 5.876660585403442\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 11, train Loss: 1.8514305758957235, valid loss: 3.4084150783888245, lr: [0.0008953382542587163], time: 6.382534742355347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 4, 6 ---  epoch 12, train Loss: 1.8524071306812635, valid loss: 3.4200102011903732, lr: [0.0008863848717161291], time: 5.837650775909424\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 13, train Loss: 1.8500683982400643, valid loss: 3.4117902569892955, lr: [0.0008775210229989678], time: 5.901554346084595\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 14, train Loss: 1.851827254277452, valid loss: 3.4253301991305083, lr: [0.0008687458127689781], time: 5.843740463256836\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 15, train Loss: 1.8513689577038317, valid loss: 3.425269490198855, lr: [0.0008600583546412883], time: 5.897634744644165\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 16, train Loss: 1.8575870188564807, valid loss: 3.4138286945717247, lr: [0.0008514577710948754], time: 5.9011499881744385\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 17, train Loss: 1.8525189813746408, valid loss: 3.486056912714806, lr: [0.0008429431933839266], time: 5.8392016887664795\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 18, train Loss: 1.8521586969103327, valid loss: 3.418967708582626, lr: [0.0008345137614500873], time: 5.901865243911743\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 19, train Loss: 1.8526786516056855, valid loss: 3.4192225710194983, lr: [0.0008261686238355864], time: 7.804484128952026\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 20, train Loss: 1.8531278776924216, valid loss: 3.424510104123173, lr: [0.0008179069375972306], time: 5.9055867195129395\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 21, train Loss: 1.8529852005523921, valid loss: 3.4095918451449476, lr: [0.0008097278682212583], time: 5.904892921447754\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 22, train Loss: 1.851746755449874, valid loss: 3.412360482648607, lr: [0.0008016305895390457], time: 5.838857412338257\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 23, train Loss: 1.8522365723971395, valid loss: 3.412461488965432, lr: [0.0007936142836436553], time: 10.560311079025269\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 24, train Loss: 1.850716091560331, valid loss: 3.415184721456136, lr: [0.0007856781408072188], time: 5.840801477432251\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 25, train Loss: 1.8522765975943996, valid loss: 3.4141436830254355, lr: [0.0007778213593991466], time: 5.917047500610352\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 26, train Loss: 1.8558668079834642, valid loss: 3.41311289211571, lr: [0.000770043145805155], time: 6.390347957611084\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 27, train Loss: 1.854670008651813, valid loss: 3.412704742051167, lr: [0.0007623427143471034], time: 5.8447020053863525\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 28, train Loss: 1.8522556939594297, valid loss: 3.4136705558122324, lr: [0.0007547192872036325], time: 5.905094623565674\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 29, train Loss: 1.8532450438676173, valid loss: 3.412766924565022, lr: [0.0007471720943315961], time: 5.836620807647705\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 30, train Loss: 1.8518791000137858, valid loss: 3.409509287887892, lr: [0.0007397003733882801], time: 6.0613884925842285\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 31, train Loss: 1.851646323017356, valid loss: 3.420340114648776, lr: [0.0007323033696543973], time: 5.909233093261719\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 32, train Loss: 1.8514900235894765, valid loss: 3.442163103735262, lr: [0.0007249803359578533], time: 5.841888427734375\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 33, train Loss: 1.8531465123105657, valid loss: 3.422248356982902, lr: [0.0007177305325982747], time: 5.903317213058472\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 34, train Loss: 1.8520444133660745, valid loss: 3.4176112192162, lr: [0.000710553227272292], time: 5.842424392700195\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 35, train Loss: 1.8548918340525806, valid loss: 3.411084883516507, lr: [0.000703447694999569], time: 5.920831203460693\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 36, train Loss: 1.8537965513455956, valid loss: 3.453747695687464, lr: [0.0006964132180495733], time: 6.0970680713653564\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 37, train Loss: 1.8517474330630956, valid loss: 3.4097960223481985, lr: [0.0006894490858690775], time: 5.8412580490112305\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 38, train Loss: 1.8524919937076352, valid loss: 3.4112344643744987, lr: [0.0006825545950103868], time: 5.906259059906006\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 39, train Loss: 1.8507135379338702, valid loss: 3.410157961537679, lr: [0.000675729049060283], time: 5.840371370315552\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 40, train Loss: 1.8520248558792027, valid loss: 3.42727715012405, lr: [0.0006689717585696801], time: 5.918034791946411\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 41, train Loss: 1.8525322640350412, valid loss: 3.4197777636964486, lr: [0.0006622820409839833], time: 5.907415866851807\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 42, train Loss: 1.8522866395025432, valid loss: 3.410463838121659, lr: [0.0006556592205741434], time: 5.845284938812256\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 43, train Loss: 1.8514882488767732, valid loss: 3.410602025562031, lr: [0.0006491026283684019], time: 5.90267539024353\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 44, train Loss: 1.852591491244333, valid loss: 3.4167525030761943, lr: [0.0006426116020847179], time: 5.839240550994873\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 45, train Loss: 1.8530538054317527, valid loss: 3.4110621687437046, lr: [0.0006361854860638707], time: 5.915088415145874\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 46, train Loss: 1.8553965581225016, valid loss: 3.4116433997206665, lr: [0.000629823631203232], time: 5.912875652313232\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 47, train Loss: 1.85112154077155, valid loss: 3.4135544834005795, lr: [0.0006235253948911997], time: 5.846804141998291\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 48, train Loss: 1.8518193257835855, valid loss: 3.4149097931972654, lr: [0.0006172901409422877], time: 5.906909465789795\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 49, train Loss: 1.8537840406315709, valid loss: 3.4430029198123564, lr: [0.0006111172395328649], time: 5.8425209522247314\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 50, train Loss: 1.8515258945385122, valid loss: 3.419523959307611, lr: [0.0006050060671375363], time: 5.918361663818359\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 51, train Loss: 1.8518094727697167, valid loss: 3.410785541673473, lr: [0.0005989560064661609], time: 5.913431167602539\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 52, train Loss: 1.8511212564062485, valid loss: 3.4184031144456903, lr: [0.0005929664464014993], time: 5.843765020370483\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 53, train Loss: 1.8531473698533876, valid loss: 3.435103374022087, lr: [0.0005870367819374844], time: 5.905620336532593\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 54, train Loss: 1.8526388312498903, valid loss: 3.410723115361252, lr: [0.0005811664141181095], time: 9.290123701095581\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 55, train Loss: 1.8520709034303429, valid loss: 3.409392710439493, lr: [0.0005753547499769285], time: 5.92561411857605\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 56, train Loss: 1.8512396205628447, valid loss: 3.424783827348502, lr: [0.0005696012024771592], time: 5.913320779800415\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 57, train Loss: 1.8514998790062285, valid loss: 3.4143434065064096, lr: [0.0005639051904523875], time: 8.353637456893921\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 58, train Loss: 1.851824872937481, valid loss: 3.41760602040929, lr: [0.0005582661385478637], time: 5.912893772125244\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 59, train Loss: 1.854108010124309, valid loss: 3.4140167005847744, lr: [0.000552683477162385], time: 5.847123861312866\n",
      "Setting (n,k,m):10, 4, 6 ---  epoch 60, train Loss: 1.851700877442787, valid loss: 3.4148525971179704, lr: [0.0005471566423907612], time: 5.926668405532837\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 1, train Loss: 32.10550240078507, valid loss: 2.756019777177024, lr: [0.00099], time: 9.154450416564941\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 2, train Loss: 0.633405180355733, valid loss: 0.13334050296257144, lr: [0.0009801], time: 7.383637428283691\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 3, train Loss: 0.36944830531866435, valid loss: 0.13104604613023776, lr: [0.000970299], time: 7.380602121353149\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 4, train Loss: 0.3649789418539342, valid loss: 0.12937057894198387, lr: [0.0009605960099999999], time: 7.53016996383667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 5, 2 ---  epoch 5, train Loss: 0.36077137215707294, valid loss: 0.12830347674420256, lr: [0.0009509900498999999], time: 7.451306343078613\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 6, train Loss: 0.35975421837538923, valid loss: 0.1282590873016939, lr: [0.0009414801494009999], time: 7.385872840881348\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 7, train Loss: 0.3592331417965251, valid loss: 0.12842487451928483, lr: [0.0009320653479069899], time: 7.439133167266846\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 8, train Loss: 0.3592153755504774, valid loss: 0.1290353693795763, lr: [0.00092274469442792], time: 7.444663763046265\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 9, train Loss: 0.35865047728724786, valid loss: 0.12815682375935222, lr: [0.0009135172474836408], time: 7.563595771789551\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 10, train Loss: 0.3588275652869866, valid loss: 0.12835276501380122, lr: [0.0009043820750088043], time: 7.446855545043945\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 11, train Loss: 0.3590708115339796, valid loss: 0.12803601099895887, lr: [0.0008953382542587163], time: 7.440995931625366\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 12, train Loss: 0.35885412293425645, valid loss: 0.1280772708695604, lr: [0.0008863848717161291], time: 7.382202386856079\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 13, train Loss: 0.3588250557812297, valid loss: 0.13204009292603677, lr: [0.0008775210229989678], time: 7.445544958114624\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 14, train Loss: 0.3591612530028086, valid loss: 0.12768652597288482, lr: [0.0008687458127689781], time: 7.44822883605957\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 15, train Loss: 0.359600191146038, valid loss: 0.13468578625065014, lr: [0.0008600583546412883], time: 7.378220319747925\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 16, train Loss: 0.3604376920926121, valid loss: 0.13164665721101548, lr: [0.0008514577710948754], time: 7.443568229675293\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 17, train Loss: 0.35939719273657983, valid loss: 0.12797747570072804, lr: [0.0008429431933839266], time: 7.382879734039307\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 18, train Loss: 0.35992604284209523, valid loss: 0.12819681976878994, lr: [0.0008345137614500873], time: 9.093232870101929\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 19, train Loss: 0.3598644364179035, valid loss: 0.13086704051520384, lr: [0.0008261686238355864], time: 7.4283223152160645\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 20, train Loss: 0.3592506404676143, valid loss: 0.12815674610102762, lr: [0.0008179069375972306], time: 7.453276872634888\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 21, train Loss: 0.36078377683527585, valid loss: 0.12840199018335724, lr: [0.0008097278682212583], time: 7.385883092880249\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 22, train Loss: 0.36030384959814726, valid loss: 0.1278343909045768, lr: [0.0008016305895390457], time: 7.44651460647583\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 23, train Loss: 0.359075137833804, valid loss: 0.12791464820264856, lr: [0.0007936142836436553], time: 7.760782480239868\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 24, train Loss: 0.35855878505649535, valid loss: 0.14664699972046558, lr: [0.0007856781408072188], time: 7.446814775466919\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 25, train Loss: 0.3608524104538837, valid loss: 0.13917352652047987, lr: [0.0007778213593991466], time: 7.386937141418457\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 26, train Loss: 0.36012605555945776, valid loss: 0.12765345580512752, lr: [0.000770043145805155], time: 7.474247932434082\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 27, train Loss: 0.35979493656300954, valid loss: 0.12959745384013094, lr: [0.0007623427143471034], time: 7.446954727172852\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 28, train Loss: 0.3588623862145586, valid loss: 0.12786342266682585, lr: [0.0007547192872036325], time: 7.382769823074341\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 29, train Loss: 0.3582213041206659, valid loss: 0.1308104858752272, lr: [0.0007471720943315961], time: 7.457942008972168\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 30, train Loss: 0.3587879772377859, valid loss: 0.13382224510683208, lr: [0.0007397003733882801], time: 7.384333372116089\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 31, train Loss: 0.359025229891903, valid loss: 0.12806949469038292, lr: [0.0007323033696543973], time: 7.445256471633911\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 32, train Loss: 0.3585130576651675, valid loss: 0.13094397851578235, lr: [0.0007249803359578533], time: 7.38918924331665\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 33, train Loss: 0.35833776335378753, valid loss: 0.13134214191826563, lr: [0.0007177305325982747], time: 7.448218822479248\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 34, train Loss: 0.358115662960912, valid loss: 0.12806194287233283, lr: [0.000710553227272292], time: 7.3971240520477295\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 35, train Loss: 0.35897462003388836, valid loss: 0.13119622075722626, lr: [0.000703447694999569], time: 7.444073915481567\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 36, train Loss: 0.35870953371571723, valid loss: 0.1372700454437286, lr: [0.0006964132180495733], time: 7.38758111000061\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 37, train Loss: 0.35841304478880026, valid loss: 0.12710168202559285, lr: [0.0006894490858690775], time: 7.460659503936768\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 38, train Loss: 0.3575933457037502, valid loss: 0.1267842257971564, lr: [0.0006825545950103868], time: 7.49469256401062\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 39, train Loss: 0.3576654442008103, valid loss: 0.12699311619102555, lr: [0.000675729049060283], time: 7.407996654510498\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 40, train Loss: 0.3574655639244953, valid loss: 0.12786440708034597, lr: [0.0006689717585696801], time: 7.473610877990723\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 41, train Loss: 0.3585277494188654, valid loss: 0.1268435108678358, lr: [0.0006622820409839833], time: 7.4680399894714355\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 42, train Loss: 0.3585283714509642, valid loss: 0.13741715906224933, lr: [0.0006556592205741434], time: 7.417090892791748\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 43, train Loss: 0.3588811417125871, valid loss: 0.1268257570110014, lr: [0.0006491026283684019], time: 7.4062299728393555\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 44, train Loss: 0.3588126425607906, valid loss: 0.12701358858431744, lr: [0.0006426116020847179], time: 7.394395351409912\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 45, train Loss: 0.35775804201837713, valid loss: 0.1271009367109982, lr: [0.0006361854860638707], time: 7.379055500030518\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 46, train Loss: 0.35761564393180406, valid loss: 0.1309231910630973, lr: [0.000629823631203232], time: 7.392068147659302\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 47, train Loss: 0.358749936964969, valid loss: 0.12767978883561834, lr: [0.0006235253948911997], time: 7.380089044570923\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 48, train Loss: 0.3581078272567045, valid loss: 0.12790975468673899, lr: [0.0006172901409422877], time: 7.376900911331177\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 49, train Loss: 0.3576877317430457, valid loss: 0.1298276710640077, lr: [0.0006111172395328649], time: 7.381236553192139\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 50, train Loss: 0.35752943578567026, valid loss: 0.12780129604540025, lr: [0.0006050060671375363], time: 7.389722108840942\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 51, train Loss: 0.3577909963460837, valid loss: 0.1273332410450116, lr: [0.0005989560064661609], time: 7.38027548789978\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 52, train Loss: 0.3567845165749612, valid loss: 0.13350429424298083, lr: [0.0005929664464014993], time: 7.379366874694824\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 53, train Loss: 0.359264933204103, valid loss: 0.12687023854819868, lr: [0.0005870367819374844], time: 7.3798463344573975\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 54, train Loss: 0.35700213979500045, valid loss: 0.1277686185070352, lr: [0.0005811664141181095], time: 7.390886068344116\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 55, train Loss: 0.35694737381953306, valid loss: 0.13159808455794178, lr: [0.0005753547499769285], time: 7.381473064422607\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 56, train Loss: 0.35701184401299213, valid loss: 0.13498622256701204, lr: [0.0005696012024771592], time: 7.384260654449463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 5, 2 ---  epoch 57, train Loss: 0.35989350721122637, valid loss: 0.13375872520708965, lr: [0.0005639051904523875], time: 7.3755786418914795\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 58, train Loss: 0.3582094839066917, valid loss: 0.1272611020951901, lr: [0.0005582661385478637], time: 7.394150018692017\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 59, train Loss: 0.35758568653963807, valid loss: 0.1272632695291845, lr: [0.000552683477162385], time: 7.3866636753082275\n",
      "Setting (n,k,m):10, 5, 2 ---  epoch 60, train Loss: 0.3570721930098563, valid loss: 0.12781812365005277, lr: [0.0005471566423907612], time: 7.410104990005493\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 1, train Loss: 27.793336346969525, valid loss: 2.2340365277413685, lr: [0.00099], time: 7.5176918506622314\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 2, train Loss: 0.8568816292016199, valid loss: 0.4175363751407632, lr: [0.0009801], time: 7.457718133926392\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 3, train Loss: 0.6534628047425931, valid loss: 0.3424909156944853, lr: [0.000970299], time: 7.460223197937012\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 4, train Loss: 0.5892794110427894, valid loss: 0.33553001773566177, lr: [0.0009605960099999999], time: 7.391095876693726\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 5, train Loss: 0.5852207621593417, valid loss: 0.33423171222621695, lr: [0.0009509900498999999], time: 7.4584925174713135\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 6, train Loss: 0.582965157673315, valid loss: 0.33555662826552357, lr: [0.0009414801494009999], time: 7.442900896072388\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 7, train Loss: 0.5820265159117632, valid loss: 0.33380707488028494, lr: [0.0009320653479069899], time: 7.472317695617676\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 8, train Loss: 0.5816339011196843, valid loss: 0.33415006926465113, lr: [0.00092274469442792], time: 7.377570867538452\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 9, train Loss: 0.5848462844425361, valid loss: 0.3339363603932582, lr: [0.0009135172474836408], time: 7.446049928665161\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 10, train Loss: 0.5816914354942757, valid loss: 0.33443311402142817, lr: [0.0009043820750088043], time: 7.381989002227783\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 11, train Loss: 0.5813625007646653, valid loss: 0.3339069572952291, lr: [0.0008953382542587163], time: 7.392747402191162\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 12, train Loss: 0.5812939299728084, valid loss: 0.33395855833552707, lr: [0.0008863848717161291], time: 7.457455158233643\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 13, train Loss: 0.5826388973477208, valid loss: 0.3342916965693221, lr: [0.0008775210229989678], time: 7.288797616958618\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 14, train Loss: 0.5874226327301487, valid loss: 0.33341153660616285, lr: [0.0008687458127689781], time: 7.4274609088897705\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 15, train Loss: 0.5816999699223768, valid loss: 0.33378935890961825, lr: [0.0008600583546412883], time: 9.41254472732544\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 16, train Loss: 0.5841968619038629, valid loss: 0.3344999153199094, lr: [0.0008514577710948754], time: 7.425363063812256\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 17, train Loss: 0.5848438167152031, valid loss: 0.33603875698474284, lr: [0.0008429431933839266], time: 7.448030471801758\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 18, train Loss: 0.5830490516749867, valid loss: 0.3338353149391804, lr: [0.0008345137614500873], time: 7.383877277374268\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 19, train Loss: 0.5820874918232614, valid loss: 0.33862513269400163, lr: [0.0008261686238355864], time: 7.4566404819488525\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 20, train Loss: 0.5823899697095388, valid loss: 0.3341167988439626, lr: [0.0008179069375972306], time: 7.388823747634888\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 21, train Loss: 0.5835267953925091, valid loss: 0.3415384194140926, lr: [0.0008097278682212583], time: 7.373870611190796\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 22, train Loss: 0.5821880492069283, valid loss: 0.3349619241107206, lr: [0.0008016305895390457], time: 7.450325965881348\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 23, train Loss: 0.5819958421483695, valid loss: 0.33485987447052434, lr: [0.0007936142836436553], time: 7.389124155044556\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 24, train Loss: 0.5820668452499491, valid loss: 0.3335471809589976, lr: [0.0007856781408072188], time: 7.464086532592773\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 25, train Loss: 0.581542685381192, valid loss: 0.3419417172999928, lr: [0.0007778213593991466], time: 7.378203868865967\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 26, train Loss: 0.5830341625690427, valid loss: 0.3339848940309049, lr: [0.000770043145805155], time: 7.384005546569824\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 27, train Loss: 0.6704190607080049, valid loss: 0.3875408272389088, lr: [0.0007623427143471034], time: 7.465439796447754\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 28, train Loss: 0.6079727361630266, valid loss: 0.3531824557081404, lr: [0.0007547192872036325], time: 7.392165660858154\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 29, train Loss: 0.5846054158515772, valid loss: 0.33339378480228304, lr: [0.0007471720943315961], time: 7.482553482055664\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 30, train Loss: 0.5832486632053983, valid loss: 0.3360074225136113, lr: [0.0007397003733882801], time: 7.474565744400024\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 31, train Loss: 0.5832115217416036, valid loss: 0.3342738603195927, lr: [0.0007323033696543973], time: 7.3931968212127686\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 32, train Loss: 0.583761710748076, valid loss: 0.3360262882631096, lr: [0.0007249803359578533], time: 7.390092849731445\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 33, train Loss: 0.5852261175775652, valid loss: 0.33296227634598186, lr: [0.0007177305325982747], time: 7.464274883270264\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 34, train Loss: 0.5892846960302718, valid loss: 0.3448960553427753, lr: [0.000710553227272292], time: 7.389281511306763\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 35, train Loss: 0.5898103520221518, valid loss: 0.33653771335443494, lr: [0.000703447694999569], time: 7.462427377700806\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 36, train Loss: 0.583280811784623, valid loss: 0.3346863988890467, lr: [0.0006964132180495733], time: 7.464471817016602\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 37, train Loss: 0.5818341527879449, valid loss: 0.3380657028793893, lr: [0.0006894490858690775], time: 7.3221025466918945\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 38, train Loss: 0.5837664436959393, valid loss: 0.33454613900247787, lr: [0.0006825545950103868], time: 7.44695520401001\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 39, train Loss: 0.5823102446897043, valid loss: 0.33720119110713564, lr: [0.000675729049060283], time: 7.3883702754974365\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 40, train Loss: 0.5836934635995531, valid loss: 0.33323236027332265, lr: [0.0006689717585696801], time: 7.438036918640137\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 41, train Loss: 0.5828212020978303, valid loss: 0.333069641459178, lr: [0.0006622820409839833], time: 7.458665132522583\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 42, train Loss: 0.5809222576865924, valid loss: 0.33526937472022134, lr: [0.0006556592205741434], time: 7.314985513687134\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 43, train Loss: 0.5822722076036793, valid loss: 0.3333388843714696, lr: [0.0006491026283684019], time: 7.4533820152282715\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 44, train Loss: 0.5809643590922517, valid loss: 0.33944060363245837, lr: [0.0006426116020847179], time: 7.3947227001190186\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 45, train Loss: 0.582985187169806, valid loss: 0.3346121114520397, lr: [0.0006361854860638707], time: 7.450382947921753\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 46, train Loss: 0.5806207510579571, valid loss: 0.332517518078938, lr: [0.000629823631203232], time: 7.479513645172119\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 47, train Loss: 0.5820606293261353, valid loss: 0.3513563173574015, lr: [0.0006235253948911997], time: 7.403561115264893\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 48, train Loss: 0.5816251614558232, valid loss: 0.33387999077583475, lr: [0.0006172901409422877], time: 7.453425407409668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 5, 3 ---  epoch 49, train Loss: 0.582116981674762, valid loss: 0.33228503920950586, lr: [0.0006111172395328649], time: 7.400487422943115\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 50, train Loss: 0.5803444824577464, valid loss: 0.33402960675184634, lr: [0.0006050060671375363], time: 7.38032603263855\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 51, train Loss: 0.5817635654695482, valid loss: 0.33654586916889817, lr: [0.0005989560064661609], time: 7.459162950515747\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 52, train Loss: 0.584513209738994, valid loss: 0.33251241602169795, lr: [0.0005929664464014993], time: 7.393061876296997\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 53, train Loss: 0.5806428899742394, valid loss: 0.334294403773001, lr: [0.0005870367819374844], time: 7.453409194946289\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 54, train Loss: 0.579772147001391, valid loss: 0.33471409940971475, lr: [0.0005811664141181095], time: 7.457677125930786\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 55, train Loss: 0.5821612965575332, valid loss: 0.3339275044989435, lr: [0.0005753547499769285], time: 7.45909571647644\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 56, train Loss: 0.5801842181430149, valid loss: 0.33427448681234306, lr: [0.0005696012024771592], time: 7.406450986862183\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 57, train Loss: 0.5816397483123568, valid loss: 0.33624011695983486, lr: [0.0005639051904523875], time: 7.4535133838653564\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 58, train Loss: 0.5798645195547258, valid loss: 0.33333604263013145, lr: [0.0005582661385478637], time: 7.460077285766602\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 59, train Loss: 0.5796144254506884, valid loss: 0.3336123465355207, lr: [0.000552683477162385], time: 7.317164182662964\n",
      "Setting (n,k,m):10, 5, 3 ---  epoch 60, train Loss: 0.5817663934011337, valid loss: 0.33289713966576073, lr: [0.0005471566423907612], time: 7.458901405334473\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 1, train Loss: 27.13468696027624, valid loss: 5.835035427283752, lr: [0.00099], time: 7.679622411727905\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 2, train Loss: 1.162696191786967, valid loss: 0.9571070093908322, lr: [0.0009801], time: 7.29615330696106\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 3, train Loss: 0.9791988987382741, valid loss: 0.9476450640317726, lr: [0.000970299], time: 7.428210735321045\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 4, train Loss: 0.9747259896326332, valid loss: 0.944533666512239, lr: [0.0009605960099999999], time: 7.427561521530151\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 5, train Loss: 0.9721185571744401, valid loss: 0.9495141622225131, lr: [0.0009509900498999999], time: 7.477047681808472\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 6, train Loss: 0.9717239587101342, valid loss: 0.9447748143735119, lr: [0.0009414801494009999], time: 7.349997520446777\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 7, train Loss: 0.9704434023919848, valid loss: 0.9472455803794011, lr: [0.0009320653479069899], time: 7.41949987411499\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 8, train Loss: 0.97004640886471, valid loss: 0.942717014977956, lr: [0.00092274469442792], time: 7.365672826766968\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 9, train Loss: 0.9710836777502022, valid loss: 0.9416692127753263, lr: [0.0009135172474836408], time: 7.435573577880859\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 10, train Loss: 0.9705724635600425, valid loss: 0.9425727515786958, lr: [0.0009043820750088043], time: 7.361620664596558\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 11, train Loss: 0.970284061626129, valid loss: 0.9417064320765235, lr: [0.0008953382542587163], time: 7.422735929489136\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 12, train Loss: 0.9707448182891693, valid loss: 0.9429237853591622, lr: [0.0008863848717161291], time: 8.640392065048218\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 13, train Loss: 0.9714469264606596, valid loss: 0.9461560075117433, lr: [0.0008775210229989678], time: 7.421103239059448\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 14, train Loss: 0.9701621023089393, valid loss: 0.9481173034447378, lr: [0.0008687458127689781], time: 7.3527021408081055\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 15, train Loss: 0.9704179752500617, valid loss: 0.947592707278978, lr: [0.0008600583546412883], time: 7.423814296722412\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 16, train Loss: 0.9702289435442579, valid loss: 0.9416917541607799, lr: [0.0008514577710948754], time: 7.304486274719238\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 17, train Loss: 0.9732466212711338, valid loss: 0.9438048488946792, lr: [0.0008429431933839266], time: 7.424557685852051\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 18, train Loss: 0.9733513404060693, valid loss: 0.9438459358228716, lr: [0.0008345137614500873], time: 7.3586342334747314\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 19, train Loss: 0.9702448485742275, valid loss: 0.9429060948327835, lr: [0.0008261686238355864], time: 7.2908899784088135\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 20, train Loss: 0.9736890554694418, valid loss: 0.9450310935783875, lr: [0.0008179069375972306], time: 7.432621955871582\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 21, train Loss: 0.9711293154865994, valid loss: 0.9415950486454474, lr: [0.0008097278682212583], time: 7.365630626678467\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 22, train Loss: 0.971269907560242, valid loss: 0.966773810994637, lr: [0.0008016305895390457], time: 7.359817981719971\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 23, train Loss: 0.9733167953862847, valid loss: 0.9623655479882731, lr: [0.0007936142836436553], time: 7.368913650512695\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 24, train Loss: 0.9719679590841704, valid loss: 0.9407520794641341, lr: [0.0007856781408072188], time: 7.432861328125\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 25, train Loss: 0.9712943341387839, valid loss: 0.9408199543019492, lr: [0.0007778213593991466], time: 7.360390663146973\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 26, train Loss: 0.9717316896646724, valid loss: 0.9408234623837565, lr: [0.000770043145805155], time: 7.430325269699097\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 27, train Loss: 0.9728237207389256, valid loss: 0.9448213169813655, lr: [0.0007623427143471034], time: 7.3667824268341064\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 28, train Loss: 0.9718983444539404, valid loss: 0.9485494060434271, lr: [0.0007547192872036325], time: 7.298222064971924\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 29, train Loss: 0.9722811529460845, valid loss: 0.9672947505779087, lr: [0.0007471720943315961], time: 7.433624267578125\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 30, train Loss: 0.9723607117192954, valid loss: 0.941501922228974, lr: [0.0007397003733882801], time: 7.384170293807983\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 31, train Loss: 0.9711510785852373, valid loss: 0.943642214955174, lr: [0.0007323033696543973], time: 7.5566160678863525\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 32, train Loss: 0.9708463057235331, valid loss: 0.9434741800447385, lr: [0.0007249803359578533], time: 7.4381935596466064\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 33, train Loss: 0.9700004797798749, valid loss: 0.9513874199891256, lr: [0.0007177305325982747], time: 7.373250246047974\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 34, train Loss: 0.9715397822339328, valid loss: 0.9432585083379983, lr: [0.000710553227272292], time: 7.4374470710754395\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 35, train Loss: 0.9705541581511222, valid loss: 0.94250789279109, lr: [0.000703447694999569], time: 7.599570989608765\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 36, train Loss: 0.970876704537856, valid loss: 0.9410068803959352, lr: [0.0006964132180495733], time: 7.292912244796753\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 37, train Loss: 0.9709587928828801, valid loss: 0.9424490361165935, lr: [0.0006894490858690775], time: 7.433423280715942\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 38, train Loss: 0.9710604584190167, valid loss: 0.9493971368240196, lr: [0.0006825545950103868], time: 7.396762371063232\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 39, train Loss: 0.9701001442178934, valid loss: 0.9433887369296619, lr: [0.000675729049060283], time: 8.021876811981201\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 40, train Loss: 0.9708601643946705, valid loss: 0.9567937667861507, lr: [0.0006689717585696801], time: 7.805160760879517\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 41, train Loss: 0.9703844108847902, valid loss: 0.9425960183940046, lr: [0.0006622820409839833], time: 8.114818334579468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 5, 4 ---  epoch 42, train Loss: 0.9705581129622055, valid loss: 0.9418671833300806, lr: [0.0006556592205741434], time: 8.683620929718018\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 43, train Loss: 0.9705335112743431, valid loss: 0.9419177844026265, lr: [0.0006491026283684019], time: 7.37202787399292\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 44, train Loss: 0.9729389933907657, valid loss: 0.9498512451247165, lr: [0.0006426116020847179], time: 8.967205286026001\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 45, train Loss: 0.9721454455363632, valid loss: 0.9425126757711955, lr: [0.0006361854860638707], time: 14.69435715675354\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 46, train Loss: 0.9715162793581859, valid loss: 0.9438324556297882, lr: [0.000629823631203232], time: 10.802448511123657\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 47, train Loss: 0.9700297550336953, valid loss: 0.9417665041884109, lr: [0.0006235253948911997], time: 11.328192710876465\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 48, train Loss: 0.9708104909494512, valid loss: 0.9419777381901561, lr: [0.0006172901409422877], time: 8.64802861213684\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 49, train Loss: 0.9697063631612396, valid loss: 0.9449847560527312, lr: [0.0006111172395328649], time: 9.755177736282349\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 50, train Loss: 0.9718886551155242, valid loss: 0.9445147630184231, lr: [0.0006050060671375363], time: 8.69995379447937\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 51, train Loss: 0.9702361046128923, valid loss: 0.9420272375438935, lr: [0.0005989560064661609], time: 9.456664323806763\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 52, train Loss: 0.9701441556165947, valid loss: 0.9405569079418526, lr: [0.0005929664464014993], time: 8.925660133361816\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 53, train Loss: 0.9734948957879567, valid loss: 0.9415804696817339, lr: [0.0005870367819374844], time: 9.969204902648926\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 54, train Loss: 0.970119525641311, valid loss: 0.9458939213788414, lr: [0.0005811664141181095], time: 8.250166654586792\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 55, train Loss: 0.9698140587278771, valid loss: 0.9409549470928904, lr: [0.0005753547499769285], time: 7.9206624031066895\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 56, train Loss: 0.9693499105744668, valid loss: 0.9427807174273031, lr: [0.0005696012024771592], time: 7.973706007003784\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 57, train Loss: 0.9707942610679169, valid loss: 0.9409929311837854, lr: [0.0005639051904523875], time: 8.494478225708008\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 58, train Loss: 0.9694408272559913, valid loss: 0.9413253608050461, lr: [0.0005582661385478637], time: 7.9857118129730225\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 59, train Loss: 0.9706789746856942, valid loss: 0.9489269031110986, lr: [0.000552683477162385], time: 8.545889377593994\n",
      "Setting (n,k,m):10, 5, 4 ---  epoch 60, train Loss: 0.970343053753766, valid loss: 0.9415122860130493, lr: [0.0005471566423907612], time: 10.066714763641357\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 1, train Loss: 31.183022889573557, valid loss: 13.396754574792357, lr: [0.00099], time: 7.710799932479858\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 2, train Loss: 2.1409350019286175, valid loss: 3.601349516541548, lr: [0.0009801], time: 7.367810249328613\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 3, train Loss: 1.9092044371565935, valid loss: 3.574469841932225, lr: [0.000970299], time: 7.430224657058716\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 4, train Loss: 1.8935127131742078, valid loss: 3.552812019250023, lr: [0.0009605960099999999], time: 7.371534585952759\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 5, train Loss: 1.8842408757601796, valid loss: 3.5270283022833326, lr: [0.0009509900498999999], time: 7.430066823959351\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 6, train Loss: 1.871945557180959, valid loss: 3.5342515626469453, lr: [0.0009414801494009999], time: 7.362729787826538\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 7, train Loss: 1.8687561091757467, valid loss: 3.519486589354617, lr: [0.0009320653479069899], time: 7.377324342727661\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 8, train Loss: 1.8680385152761718, valid loss: 3.516034749619872, lr: [0.00092274469442792], time: 7.3916709423065186\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 9, train Loss: 1.8677856544998541, valid loss: 3.5172276057242375, lr: [0.0009135172474836408], time: 7.428171157836914\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 10, train Loss: 1.8672945929582079, valid loss: 3.525411984690004, lr: [0.0009043820750088043], time: 7.430355787277222\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 11, train Loss: 1.866861358063911, valid loss: 3.5082895030507895, lr: [0.0008953382542587163], time: 7.437100648880005\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 12, train Loss: 1.8679315155114393, valid loss: 3.532142588777685, lr: [0.0008863848717161291], time: 7.91338324546814\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 13, train Loss: 1.8856041590054713, valid loss: 3.5178074273360935, lr: [0.0008775210229989678], time: 7.74153208732605\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 14, train Loss: 1.868200430754788, valid loss: 3.5167494482936976, lr: [0.0008687458127689781], time: 8.273351430892944\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 15, train Loss: 1.869158186104114, valid loss: 3.508156660998605, lr: [0.0008600583546412883], time: 8.035772562026978\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 16, train Loss: 1.8677796651868914, valid loss: 3.5083091463628016, lr: [0.0008514577710948754], time: 7.434932231903076\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 17, train Loss: 1.8714226198369495, valid loss: 3.51639398949607, lr: [0.0008429431933839266], time: 8.049967288970947\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 18, train Loss: 1.867506942390718, valid loss: 3.512186450079205, lr: [0.0008345137614500873], time: 9.89233112335205\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 19, train Loss: 1.8674194475230546, valid loss: 3.5084839413993607, lr: [0.0008261686238355864], time: 8.295770406723022\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 20, train Loss: 1.8663833371603973, valid loss: 3.512029059229878, lr: [0.0008179069375972306], time: 7.701676368713379\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 21, train Loss: 2.0630131111508896, valid loss: 5.414479350123818, lr: [0.0008097278682212583], time: 7.428574562072754\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 22, train Loss: 2.280661368209366, valid loss: 3.9936067000994426, lr: [0.0008016305895390457], time: 7.527953863143921\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 23, train Loss: 2.0441073746297205, valid loss: 3.7429172946030165, lr: [0.0007936142836436553], time: 7.439453125\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 24, train Loss: 1.9596934609927967, valid loss: 3.6414872327534815, lr: [0.0007856781408072188], time: 7.367140293121338\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 25, train Loss: 1.9243068036490838, valid loss: 3.5866543492821106, lr: [0.0007778213593991466], time: 9.18324065208435\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 26, train Loss: 1.9047414343874496, valid loss: 3.559646923910451, lr: [0.000770043145805155], time: 9.549926519393921\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 27, train Loss: 1.892789543316599, valid loss: 3.5370860955806536, lr: [0.0007623427143471034], time: 10.807595252990723\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 28, train Loss: 1.8835165734011272, valid loss: 3.5399971988639845, lr: [0.0007547192872036325], time: 7.607386827468872\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 29, train Loss: 1.881729510439782, valid loss: 3.525312280619997, lr: [0.0007471720943315961], time: 8.945337295532227\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 30, train Loss: 1.869374081351916, valid loss: 3.504919430602922, lr: [0.0007397003733882801], time: 8.40970492362976\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 31, train Loss: 1.8686475950091086, valid loss: 3.5190790372436247, lr: [0.0007323033696543973], time: 7.4417243003845215\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 32, train Loss: 1.8743689847833267, valid loss: 3.5210710805365104, lr: [0.0007249803359578533], time: 8.024426460266113\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 33, train Loss: 1.874549051192699, valid loss: 3.5411993644468316, lr: [0.0007177305325982747], time: 7.703267335891724\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 34, train Loss: 1.874532176738467, valid loss: 3.5207535782002095, lr: [0.000710553227272292], time: 7.674360990524292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 5, 5 ---  epoch 35, train Loss: 1.8715229872338883, valid loss: 3.5236638334573995, lr: [0.000703447694999569], time: 7.363303184509277\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 36, train Loss: 1.8677816877250575, valid loss: 3.5413499015777066, lr: [0.0006964132180495733], time: 7.436734914779663\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 37, train Loss: 1.8674561768088052, valid loss: 3.5281722955758172, lr: [0.0006894490858690775], time: 7.630873441696167\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 38, train Loss: 1.868017811604956, valid loss: 3.5110533241554127, lr: [0.0006825545950103868], time: 7.433694124221802\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 39, train Loss: 1.8641332142019165, valid loss: 3.501845805035633, lr: [0.000675729049060283], time: 7.548828125\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 40, train Loss: 1.863471801974528, valid loss: 3.503148532586964, lr: [0.0006689717585696801], time: 7.447361946105957\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 41, train Loss: 1.8657435234024906, valid loss: 3.5044169875034696, lr: [0.0006622820409839833], time: 7.799654006958008\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 42, train Loss: 1.8639646097012663, valid loss: 3.5137435587937977, lr: [0.0006556592205741434], time: 7.442347288131714\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 43, train Loss: 1.8656713413050237, valid loss: 3.5038061082338805, lr: [0.0006491026283684019], time: 7.362653732299805\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 44, train Loss: 1.866011398249629, valid loss: 3.5162430896919408, lr: [0.0006426116020847179], time: 7.538525819778442\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 45, train Loss: 1.8744298414306546, valid loss: 3.5112799007404827, lr: [0.0006361854860638707], time: 7.450835943222046\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 46, train Loss: 1.8702424222887613, valid loss: 3.518567749104941, lr: [0.000629823631203232], time: 7.378362417221069\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 47, train Loss: 1.8701612080531838, valid loss: 3.51366416678541, lr: [0.0006235253948911997], time: 7.546239376068115\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 48, train Loss: 1.868445373063296, valid loss: 3.507635924601617, lr: [0.0006172901409422877], time: 7.698530673980713\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 49, train Loss: 1.8685489245161495, valid loss: 3.511875491094855, lr: [0.0006111172395328649], time: 7.429354667663574\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 50, train Loss: 1.8678566417701132, valid loss: 3.516022944936421, lr: [0.0006050060671375363], time: 7.429873466491699\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 51, train Loss: 1.8679493637389737, valid loss: 3.5110461431143314, lr: [0.0005989560064661609], time: 7.362046957015991\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 52, train Loss: 1.8694057379343672, valid loss: 3.5152761912515653, lr: [0.0005929664464014993], time: 7.428500175476074\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 53, train Loss: 1.870305538867039, valid loss: 3.5096972968360327, lr: [0.0005870367819374844], time: 7.422504901885986\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 54, train Loss: 1.8660540592761878, valid loss: 3.5110430212359507, lr: [0.0005811664141181095], time: 7.366680383682251\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 55, train Loss: 1.8663314549467624, valid loss: 3.521483127780783, lr: [0.0005753547499769285], time: 7.4220733642578125\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 56, train Loss: 1.867444126683836, valid loss: 3.529214075663985, lr: [0.0005696012024771592], time: 7.434192895889282\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 57, train Loss: 1.8673461298058756, valid loss: 3.5154745493114667, lr: [0.0005639051904523875], time: 7.36377215385437\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 58, train Loss: 1.867380598639504, valid loss: 3.5124243083905418, lr: [0.0005582661385478637], time: 7.433362722396851\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 59, train Loss: 1.8647770633104346, valid loss: 3.5015766210040984, lr: [0.000552683477162385], time: 7.740039110183716\n",
      "Setting (n,k,m):10, 5, 5 ---  epoch 60, train Loss: 1.8639641564531408, valid loss: 3.5023916451959214, lr: [0.0005471566423907612], time: 7.376425266265869\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 1, train Loss: 26.706135327162595, valid loss: 2.9421009280669224, lr: [0.00099], time: 10.029887199401855\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 2, train Loss: 0.6570903169923549, valid loss: 0.25263506684571085, lr: [0.0009801], time: 9.900671482086182\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 3, train Loss: 0.5054157560960354, valid loss: 0.2482546460070785, lr: [0.000970299], time: 9.896970510482788\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 4, train Loss: 0.5014991785558177, valid loss: 0.24680626679018167, lr: [0.0009605960099999999], time: 9.830180406570435\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 5, train Loss: 0.500818553968031, valid loss: 0.24645371889663414, lr: [0.0009509900498999999], time: 9.836670875549316\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 6, train Loss: 0.5000348279259608, valid loss: 0.24913245577353788, lr: [0.0009414801494009999], time: 9.895263671875\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 7, train Loss: 0.4997666069833853, valid loss: 0.24573464382474489, lr: [0.0009320653479069899], time: 9.898797273635864\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 8, train Loss: 0.4988516546348926, valid loss: 0.2459754093255039, lr: [0.00092274469442792], time: 9.842537641525269\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 9, train Loss: 0.49913625740817136, valid loss: 0.24711052632508146, lr: [0.0009135172474836408], time: 9.899063110351562\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 10, train Loss: 0.4983818673590721, valid loss: 0.24552046647478562, lr: [0.0009043820750088043], time: 9.843645095825195\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 11, train Loss: 0.4977135108864607, valid loss: 0.24556931979127952, lr: [0.0008953382542587163], time: 9.825533866882324\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 12, train Loss: 0.49802932950713313, valid loss: 0.24473410259362863, lr: [0.0008863848717161291], time: 9.897691249847412\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 13, train Loss: 0.4984743889988174, valid loss: 0.24485535000779074, lr: [0.0008775210229989678], time: 9.825862646102905\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 14, train Loss: 0.498002498107898, valid loss: 0.24834589827726591, lr: [0.0008687458127689781], time: 9.911238670349121\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 15, train Loss: 0.49811368246873505, valid loss: 0.24473470050728455, lr: [0.0008600583546412883], time: 9.823320388793945\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 16, train Loss: 0.49736816941746975, valid loss: 0.24483035851485851, lr: [0.0008514577710948754], time: 9.82605266571045\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 17, train Loss: 0.49940031131455265, valid loss: 0.24462011519282098, lr: [0.0008429431933839266], time: 9.911044359207153\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 18, train Loss: 0.4984689157921838, valid loss: 0.24533967159290881, lr: [0.0008345137614500873], time: 9.884919881820679\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 19, train Loss: 0.49849253709714114, valid loss: 0.248486846992739, lr: [0.0008261686238355864], time: 9.815864562988281\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 20, train Loss: 0.4986857347300237, valid loss: 0.24631318967034022, lr: [0.0008179069375972306], time: 9.83748722076416\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 21, train Loss: 0.49775893560159634, valid loss: 0.24787442639885265, lr: [0.0008097278682212583], time: 9.83535623550415\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 22, train Loss: 0.49777766641604476, valid loss: 0.24812813069534737, lr: [0.0008016305895390457], time: 9.885132074356079\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 23, train Loss: 0.4978424644265957, valid loss: 0.2450301549725995, lr: [0.0007936142836436553], time: 9.832629680633545\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 24, train Loss: 0.49836869506426845, valid loss: 0.24647779166953995, lr: [0.0007856781408072188], time: 9.825190544128418\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 25, train Loss: 0.4983719497497549, valid loss: 0.24646445021319635, lr: [0.0007778213593991466], time: 9.87334132194519\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 26, train Loss: 0.49746068231177853, valid loss: 0.2445299304365895, lr: [0.000770043145805155], time: 9.916876792907715\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 27, train Loss: 0.4989263561643218, valid loss: 0.24522124685589428, lr: [0.0007623427143471034], time: 9.845582246780396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 6, 2 ---  epoch 28, train Loss: 0.4996553001127972, valid loss: 0.24560648680233824, lr: [0.0007547192872036325], time: 9.834606409072876\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 29, train Loss: 0.4979613038611003, valid loss: 0.24957149535261572, lr: [0.0007471720943315961], time: 9.90855360031128\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 30, train Loss: 0.4971456758109141, valid loss: 0.24500046929187747, lr: [0.0007397003733882801], time: 9.845059394836426\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 31, train Loss: 0.49772026824912896, valid loss: 0.24566543805735305, lr: [0.0007323033696543973], time: 9.820590734481812\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 32, train Loss: 0.49766047830778326, valid loss: 0.24564654744007347, lr: [0.0007249803359578533], time: 9.888575792312622\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 33, train Loss: 0.49743975746090935, valid loss: 0.2447019571249468, lr: [0.0007177305325982747], time: 9.897428512573242\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 34, train Loss: 0.4993090968630329, valid loss: 0.24556861930448545, lr: [0.000710553227272292], time: 9.878093957901001\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 35, train Loss: 0.49803859604386513, valid loss: 0.2509577047156003, lr: [0.000703447694999569], time: 9.875607967376709\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 36, train Loss: 0.49844318495404993, valid loss: 0.24571529802079786, lr: [0.0006964132180495733], time: 9.883677005767822\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 37, train Loss: 0.4981279632200098, valid loss: 0.2448668892024442, lr: [0.0006894490858690775], time: 9.880957841873169\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 38, train Loss: 0.49795310883438754, valid loss: 0.24455476712057442, lr: [0.0006825545950103868], time: 9.89187479019165\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 39, train Loss: 0.4969175324251497, valid loss: 0.24458143482731445, lr: [0.000675729049060283], time: 9.883519172668457\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 40, train Loss: 0.4972137611098012, valid loss: 0.24903449070764502, lr: [0.0006689717585696801], time: 11.136801958084106\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 41, train Loss: 0.49692578330629483, valid loss: 0.2446126598032041, lr: [0.0006622820409839833], time: 9.885897397994995\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 42, train Loss: 0.4976439302177328, valid loss: 0.2456359580571973, lr: [0.0006556592205741434], time: 11.553058385848999\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 43, train Loss: 0.49693758245021885, valid loss: 0.2444706152232595, lr: [0.0006491026283684019], time: 10.075033903121948\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 44, train Loss: 0.49700244573953, valid loss: 0.24498821317857386, lr: [0.0006426116020847179], time: 9.898491859436035\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 45, train Loss: 0.4980645913082311, valid loss: 0.24604570692038477, lr: [0.0006361854860638707], time: 9.886844873428345\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 46, train Loss: 0.4978957147515977, valid loss: 0.2474207197237816, lr: [0.000629823631203232], time: 9.894426107406616\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 47, train Loss: 0.4963700437543748, valid loss: 0.24561042923699672, lr: [0.0006235253948911997], time: 9.827408790588379\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 48, train Loss: 0.4989261876924004, valid loss: 0.24525142840683964, lr: [0.0006172901409422877], time: 9.88395071029663\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 49, train Loss: 0.49750529544598715, valid loss: 0.24591990232318828, lr: [0.0006111172395328649], time: 9.882523775100708\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 50, train Loss: 0.49746122383283575, valid loss: 0.2449004578946043, lr: [0.0006050060671375363], time: 9.907667398452759\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 51, train Loss: 0.49750767840256943, valid loss: 0.24496696290639594, lr: [0.0005989560064661609], time: 9.87906527519226\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 52, train Loss: 0.4968767706815344, valid loss: 0.2451729640436018, lr: [0.0005929664464014993], time: 9.881657361984253\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 53, train Loss: 0.49721167555847307, valid loss: 0.24541016356597378, lr: [0.0005870367819374844], time: 9.904839992523193\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 54, train Loss: 0.4970926395472424, valid loss: 0.2503533309462557, lr: [0.0005811664141181095], time: 9.8851478099823\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 55, train Loss: 0.49615213418403414, valid loss: 0.24621780772986832, lr: [0.0005753547499769285], time: 9.884802341461182\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 56, train Loss: 0.49686924117505127, valid loss: 0.2455793098355206, lr: [0.0005696012024771592], time: 9.88976788520813\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 57, train Loss: 0.49688312322235845, valid loss: 0.24444902651957015, lr: [0.0005639051904523875], time: 10.091335773468018\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 58, train Loss: 0.49736811152708044, valid loss: 0.25708846040233707, lr: [0.0005582661385478637], time: 9.885519742965698\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 59, train Loss: 0.49767084359567343, valid loss: 0.24502786682726133, lr: [0.000552683477162385], time: 9.817749977111816\n",
      "Setting (n,k,m):10, 6, 2 ---  epoch 60, train Loss: 0.4965355626140203, valid loss: 0.24446561870895353, lr: [0.0005471566423907612], time: 9.82168459892273\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 1, train Loss: 23.133346199699147, valid loss: 5.373679679042487, lr: [0.00099], time: 9.95119833946228\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 2, train Loss: 1.485962835056163, valid loss: 0.8969383578483148, lr: [0.0009801], time: 9.828713417053223\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 3, train Loss: 0.9587023423664089, valid loss: 0.890122237814533, lr: [0.000970299], time: 9.815869808197021\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 4, train Loss: 0.9490637805005109, valid loss: 0.8812357940742329, lr: [0.0009605960099999999], time: 9.883666038513184\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 5, train Loss: 0.9463334511656838, valid loss: 0.8847421886350546, lr: [0.0009509900498999999], time: 9.868897914886475\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 6, train Loss: 0.9462028766593059, valid loss: 0.8802257829960821, lr: [0.0009414801494009999], time: 9.869941711425781\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 7, train Loss: 0.9451689306598858, valid loss: 0.8866801134208638, lr: [0.0009320653479069899], time: 9.87759256362915\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 8, train Loss: 1.0642759556833215, valid loss: 0.8810970270963225, lr: [0.00092274469442792], time: 9.813612222671509\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 9, train Loss: 0.9462097909061449, valid loss: 0.8799690020071423, lr: [0.0009135172474836408], time: 9.818701028823853\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 10, train Loss: 0.948199142245662, valid loss: 0.8838090400019379, lr: [0.0009043820750088043], time: 9.87785291671753\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 11, train Loss: 0.9528515712848449, valid loss: 0.8995833075202443, lr: [0.0008953382542587163], time: 9.877680540084839\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 12, train Loss: 0.9501183579729244, valid loss: 0.8820489027465978, lr: [0.0008863848717161291], time: 9.808412075042725\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 13, train Loss: 0.9472999865537298, valid loss: 0.8790752872026943, lr: [0.0008775210229989678], time: 9.887845516204834\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 14, train Loss: 0.9452849728588326, valid loss: 0.8798293971997257, lr: [0.0008687458127689781], time: 9.879971981048584\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 15, train Loss: 0.9452678850107454, valid loss: 0.8893757112087047, lr: [0.0008600583546412883], time: 9.816380739212036\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 16, train Loss: 0.9472050069421851, valid loss: 0.8865265983183687, lr: [0.0008514577710948754], time: 9.808903455734253\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 17, train Loss: 0.9464731219577213, valid loss: 0.8827084840480596, lr: [0.0008429431933839266], time: 9.885326862335205\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 18, train Loss: 0.9454327471360844, valid loss: 0.889745358000097, lr: [0.0008345137614500873], time: 9.883650064468384\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 19, train Loss: 0.9464804672103385, valid loss: 0.8792101330342672, lr: [0.0008261686238355864], time: 9.884829998016357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 6, 3 ---  epoch 20, train Loss: 0.9526273801251945, valid loss: 0.8781576006623893, lr: [0.0008179069375972306], time: 9.914859771728516\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 21, train Loss: 0.9468316929584931, valid loss: 0.8780239821017564, lr: [0.0008097278682212583], time: 9.91333293914795\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 22, train Loss: 0.9442988210491614, valid loss: 0.8782199809545685, lr: [0.0008016305895390457], time: 9.873384714126587\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 23, train Loss: 0.9450151305825106, valid loss: 0.8783658554049051, lr: [0.0007936142836436553], time: 9.813631772994995\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 24, train Loss: 0.9492650276704572, valid loss: 0.8790873380143044, lr: [0.0007856781408072188], time: 9.819508075714111\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 25, train Loss: 0.949086860836166, valid loss: 0.8791222846510458, lr: [0.0007778213593991466], time: 9.875900030136108\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 26, train Loss: 0.9450780767091005, valid loss: 0.8783075313846597, lr: [0.000770043145805155], time: 9.87512993812561\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 27, train Loss: 0.9711624796678839, valid loss: 0.9051716670307415, lr: [0.0007623427143471034], time: 9.882539749145508\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 28, train Loss: 0.9525007856107962, valid loss: 0.8783494554440148, lr: [0.0007547192872036325], time: 9.808964014053345\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 29, train Loss: 0.9458016133896423, valid loss: 0.877526174692972, lr: [0.0007471720943315961], time: 9.876698017120361\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 30, train Loss: 0.9444900773709486, valid loss: 0.8979673038624842, lr: [0.0007397003733882801], time: 9.878655433654785\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 31, train Loss: 0.9476804575112743, valid loss: 0.8860317187668411, lr: [0.0007323033696543973], time: 9.809974193572998\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 32, train Loss: 0.9449590762717813, valid loss: 0.8784865977192966, lr: [0.0007249803359578533], time: 9.961811542510986\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 33, train Loss: 0.9468146453190001, valid loss: 0.878055418133074, lr: [0.0007177305325982747], time: 9.820815563201904\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 34, train Loss: 0.945548460478964, valid loss: 0.8798208060885274, lr: [0.000710553227272292], time: 9.803546905517578\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 35, train Loss: 0.9469830064789593, valid loss: 0.8806740062202263, lr: [0.000703447694999569], time: 9.89228892326355\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 36, train Loss: 0.9442590993904286, valid loss: 0.8847598214952128, lr: [0.0006964132180495733], time: 9.872825860977173\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 37, train Loss: 0.9450171111824043, valid loss: 0.8832945479973232, lr: [0.0006894490858690775], time: 9.863294124603271\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 38, train Loss: 0.9467162056097107, valid loss: 0.8850424955578823, lr: [0.0006825545950103868], time: 9.822211027145386\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 39, train Loss: 0.9460871492624977, valid loss: 0.8786569773681868, lr: [0.000675729049060283], time: 9.871477842330933\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 40, train Loss: 0.9449928664806237, valid loss: 0.8795897050318753, lr: [0.0006689717585696801], time: 9.815476655960083\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 41, train Loss: 0.9455557714279504, valid loss: 0.8818386395042773, lr: [0.0006622820409839833], time: 9.893312454223633\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 42, train Loss: 0.9458370638504179, valid loss: 0.8812211295860012, lr: [0.0006556592205741434], time: 9.817861318588257\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 43, train Loss: 0.944421049229805, valid loss: 0.8810506606516388, lr: [0.0006491026283684019], time: 9.819679021835327\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 44, train Loss: 0.942708850052496, valid loss: 0.8799075321603815, lr: [0.0006426116020847179], time: 9.899645566940308\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 45, train Loss: 0.9446257767847377, valid loss: 0.8796349106853392, lr: [0.0006361854860638707], time: 9.878785133361816\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 46, train Loss: 0.9439590668367276, valid loss: 0.8822089125505386, lr: [0.000629823631203232], time: 9.883234024047852\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 47, train Loss: 0.9458094288849703, valid loss: 0.8835237092629759, lr: [0.0006235253948911997], time: 9.834946632385254\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 48, train Loss: 0.9438802146900355, valid loss: 0.8770467707824857, lr: [0.0006172901409422877], time: 9.891200065612793\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 49, train Loss: 0.9436154070326415, valid loss: 0.8833287226726285, lr: [0.0006111172395328649], time: 9.87629508972168\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 50, train Loss: 0.9439883325047721, valid loss: 0.8772599557137633, lr: [0.0006050060671375363], time: 9.827812433242798\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 51, train Loss: 0.9422529512914999, valid loss: 0.87852264485714, lr: [0.0005989560064661609], time: 9.892928838729858\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 52, train Loss: 0.9427979085518031, valid loss: 0.8829287759343148, lr: [0.0005929664464014993], time: 9.818028926849365\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 53, train Loss: 0.9419906842820357, valid loss: 0.8778291242270034, lr: [0.0005870367819374844], time: 9.827779531478882\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 54, train Loss: 0.9439054949666413, valid loss: 0.8802752832423614, lr: [0.0005811664141181095], time: 9.8989577293396\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 55, train Loss: 0.9423493344315061, valid loss: 0.8867675840310868, lr: [0.0005753547499769285], time: 9.883670330047607\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 56, train Loss: 0.9440535736791462, valid loss: 0.8775616501710729, lr: [0.0005696012024771592], time: 9.901482820510864\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 57, train Loss: 0.9437695216561786, valid loss: 0.8773789392921746, lr: [0.0005639051904523875], time: 9.838823080062866\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 58, train Loss: 0.9423584366284795, valid loss: 0.8829408712569305, lr: [0.0005582661385478637], time: 9.884475231170654\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 59, train Loss: 0.9424847297350558, valid loss: 0.8792412335887327, lr: [0.000552683477162385], time: 9.842234373092651\n",
      "Setting (n,k,m):10, 6, 3 ---  epoch 60, train Loss: 0.943953686223871, valid loss: 0.8790710261672701, lr: [0.0005471566423907612], time: 9.904560089111328\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 1, train Loss: 26.731388846572393, valid loss: 14.254682844287185, lr: [0.00099], time: 9.927674770355225\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 2, train Loss: 2.0666834986662024, valid loss: 3.566140832973323, lr: [0.0009801], time: 9.857772588729858\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 3, train Loss: 1.8993462800326213, valid loss: 3.5557980851154167, lr: [0.000970299], time: 10.030263662338257\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 4, train Loss: 1.896141623423472, valid loss: 3.5449980121880014, lr: [0.0009605960099999999], time: 9.807672500610352\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 5, train Loss: 1.8898179608165415, valid loss: 3.5301986931886566, lr: [0.0009509900498999999], time: 9.886084794998169\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 6, train Loss: 1.884497210077083, valid loss: 3.5251715735992093, lr: [0.0009414801494009999], time: 9.855661153793335\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 7, train Loss: 1.8797471456068413, valid loss: 3.5153519964467566, lr: [0.0009320653479069899], time: 9.789266586303711\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 8, train Loss: 1.8665574378423782, valid loss: 3.4889038284191973, lr: [0.00092274469442792], time: 9.857028245925903\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 9, train Loss: 1.8660055447759283, valid loss: 3.4860835787924236, lr: [0.0009135172474836408], time: 9.975952625274658\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 10, train Loss: 1.8668169202288445, valid loss: 3.5299764044480324, lr: [0.0009043820750088043], time: 22.98224925994873\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 11, train Loss: 1.8655574573993727, valid loss: 3.4933267872383533, lr: [0.0008953382542587163], time: 10.760332584381104\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 12, train Loss: 1.8642992296498968, valid loss: 3.4943460004244438, lr: [0.0008863848717161291], time: 9.774784326553345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting (n,k,m):10, 6, 4 ---  epoch 13, train Loss: 1.8636603142713846, valid loss: 3.4838505869489316, lr: [0.0008775210229989678], time: 10.670151233673096\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 14, train Loss: 1.8615500076313827, valid loss: 3.5096730867026045, lr: [0.0008687458127689781], time: 9.79117202758789\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 15, train Loss: 1.8660178693434426, valid loss: 3.4883303004918944, lr: [0.0008600583546412883], time: 9.845614433288574\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 16, train Loss: 1.8630769100335027, valid loss: 3.526570938851089, lr: [0.0008514577710948754], time: 9.78415298461914\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 17, train Loss: 1.8642379399102467, valid loss: 3.4909572838596086, lr: [0.0008429431933839266], time: 9.789223909378052\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 18, train Loss: 1.8628403074322968, valid loss: 3.480255451131161, lr: [0.0008345137614500873], time: 9.86118745803833\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 19, train Loss: 1.8629789156889642, valid loss: 3.4808715005405677, lr: [0.0008261686238355864], time: 9.783322811126709\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 20, train Loss: 1.8625616408978722, valid loss: 3.4825438223530116, lr: [0.0008179069375972306], time: 9.854579210281372\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 21, train Loss: 1.863863225463016, valid loss: 3.4910614609662103, lr: [0.0008097278682212583], time: 9.773974895477295\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 22, train Loss: 1.8609614148738507, valid loss: 3.501128948975794, lr: [0.0008016305895390457], time: 9.777804851531982\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 23, train Loss: 1.8624432882670252, valid loss: 3.500434872611396, lr: [0.0007936142836436553], time: 9.787493228912354\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 24, train Loss: 1.863893259343075, valid loss: 3.4785756790982187, lr: [0.0007856781408072188], time: 10.200546979904175\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 25, train Loss: 1.8612555064880665, valid loss: 3.4907087866913082, lr: [0.0007778213593991466], time: 9.786585807800293\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 26, train Loss: 1.86319967814263, valid loss: 3.4849804235073, lr: [0.000770043145805155], time: 9.86243462562561\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 27, train Loss: 1.8616037786792945, valid loss: 3.486930040364541, lr: [0.0007623427143471034], time: 9.798439979553223\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 28, train Loss: 1.8596050719662105, valid loss: 3.4864012168952017, lr: [0.0007547192872036325], time: 9.923339366912842\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 29, train Loss: 1.8628855546757344, valid loss: 3.47980772416143, lr: [0.0007471720943315961], time: 9.88430118560791\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 30, train Loss: 1.858879038606637, valid loss: 3.4794246344053286, lr: [0.0007397003733882801], time: 9.79548168182373\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 31, train Loss: 1.8597764008993478, valid loss: 3.478403502948857, lr: [0.0007323033696543973], time: 10.552365779876709\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 32, train Loss: 1.8607589398408868, valid loss: 3.482487341019507, lr: [0.0007249803359578533], time: 9.881980419158936\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 33, train Loss: 1.8585313481872074, valid loss: 3.4775583642617085, lr: [0.0007177305325982747], time: 10.090690612792969\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 34, train Loss: 1.858662782228046, valid loss: 3.482391102934766, lr: [0.000710553227272292], time: 9.952509880065918\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 35, train Loss: 1.8598456047288532, valid loss: 3.4766798812115955, lr: [0.000703447694999569], time: 10.76403260231018\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 36, train Loss: 1.8607019896656116, valid loss: 3.477690729883995, lr: [0.0006964132180495733], time: 9.874693870544434\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 37, train Loss: 1.8596843178575408, valid loss: 3.477662775033255, lr: [0.0006894490858690775], time: 9.79856014251709\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 38, train Loss: 1.859597898900459, valid loss: 3.479987402890672, lr: [0.0006825545950103868], time: 9.817347764968872\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 39, train Loss: 1.8594256615052445, valid loss: 3.498817489266022, lr: [0.000675729049060283], time: 10.206783056259155\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 40, train Loss: 1.8587263645333847, valid loss: 3.4811773893787024, lr: [0.0006689717585696801], time: 9.80047869682312\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 41, train Loss: 1.8600343560402905, valid loss: 3.482360811560475, lr: [0.0006622820409839833], time: 9.797098636627197\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 42, train Loss: 1.8588085528158662, valid loss: 3.4775010898883285, lr: [0.0006556592205741434], time: 10.770190000534058\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 43, train Loss: 1.8600768075903371, valid loss: 3.4863464124723262, lr: [0.0006491026283684019], time: 9.791770219802856\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 44, train Loss: 1.8586550781252957, valid loss: 3.481300432024577, lr: [0.0006426116020847179], time: 10.301348209381104\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 45, train Loss: 1.8589529422812574, valid loss: 3.4810598871418543, lr: [0.0006361854860638707], time: 9.790099859237671\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 46, train Loss: 1.8578812906893705, valid loss: 3.476295974654261, lr: [0.000629823631203232], time: 9.797823905944824\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 47, train Loss: 1.8584804482848698, valid loss: 3.4851214876124605, lr: [0.0006235253948911997], time: 9.796038150787354\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 48, train Loss: 1.8593385744696527, valid loss: 3.4796715809213903, lr: [0.0006172901409422877], time: 9.874416828155518\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 49, train Loss: 1.8630583844463393, valid loss: 3.4756881330849865, lr: [0.0006111172395328649], time: 10.066012620925903\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 50, train Loss: 1.8599758859675104, valid loss: 3.4784839909638103, lr: [0.0006050060671375363], time: 10.66707706451416\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 51, train Loss: 1.8583091428943028, valid loss: 3.484462313408549, lr: [0.0005989560064661609], time: 13.172281980514526\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 52, train Loss: 1.8573895094428532, valid loss: 3.475427218640518, lr: [0.0005929664464014993], time: 10.426739931106567\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 53, train Loss: 1.8582001453712438, valid loss: 3.4827898692211634, lr: [0.0005870367819374844], time: 9.80626392364502\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 54, train Loss: 1.8579194320466386, valid loss: 3.4798508183423964, lr: [0.0005811664141181095], time: 9.862780809402466\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 55, train Loss: 1.8564925651635353, valid loss: 3.475044751201887, lr: [0.0005753547499769285], time: 9.808715343475342\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 56, train Loss: 1.8580824995966119, valid loss: 3.4838334419887502, lr: [0.0005696012024771592], time: 9.867072343826294\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 57, train Loss: 1.857950722911368, valid loss: 3.4781552654726293, lr: [0.0005639051904523875], time: 9.861000299453735\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 58, train Loss: 1.8582409985296058, valid loss: 3.4779689363436703, lr: [0.0005582661385478637], time: 9.797559261322021\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 59, train Loss: 1.8571351548681934, valid loss: 3.4900253292054293, lr: [0.000552683477162385], time: 9.802146911621094\n",
      "Setting (n,k,m):10, 6, 4 ---  epoch 60, train Loss: 1.8574810425725945, valid loss: 3.475525563025562, lr: [0.0005471566423907612], time: 11.566187858581543\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "input_dim = 30#20#13\n",
    "latent_dim = 4#2#4#5\n",
    "output_dim = 1\n",
    "num_flows = 10#10#4\n",
    "num_epochs = 60#20 * 2\n",
    "flow_type = 'planar'  # Choose 'planar' or 'radial'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "for item in nkm:\n",
    "\n",
    "    n,k,m = item[0], item[1], item[2]\n",
    "\n",
    "    data, target = modif_data[[(modif_data[:,-1]==item[2]) & (modif_data[:,-2]==item[1]) & (modif_data[:,-3]==item[0])]], target_vals[[(modif_data[:,-1]==item[2]) & (modif_data[:,-2]==item[1]) & (modif_data[:,-3]==item[0])]]\n",
    "#     data, target = modif_data_s[[(modif_data_s[:,-1]==item[2]) & (modif_data_s[:,-2]==item[1]) & (modif_data_s[:,-3]==item[0])]], target_vals_s[[(modif_data_s[:,-1]==item[2]) & (modif_data_s[:,-2]==item[1]) & (modif_data_s[:,-3]==item[0])]]\n",
    "\n",
    "\n",
    "    train_data, val_data, train_targets, val_targets = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "    val_data, test_data, val_targets, test_targets = train_test_split(val_data, val_targets, test_size=0.5, random_state=42)\n",
    "\n",
    "    inds = np.where(train_targets < 0.5*1e9) # removing very large values ---> can be outliers\n",
    "    train_data, train_targets = train_data[inds], train_targets[inds]\n",
    "\n",
    "\n",
    "    train_dataset = CustomDataset(torch.tensor(train_data), (torch.log2(torch.tensor(train_targets))))\n",
    "    valid_dataset = CustomDataset(torch.tensor(val_data), torch.tensor(val_targets))\n",
    "    test_dataset = CustomDataset(torch.tensor(test_data), torch.tensor(test_targets))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = 1024 * 2 #256 * 4\n",
    "    num_workers = 4\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)#, num_workers = num_workers)  # Shuffle only for training data\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)#, num_workers = num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)#, num_workers = num_workers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model = VAEWithFlow(input_dim, latent_dim, output_dim, num_flows, flow_type).to(torch.float64).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    bperf = torch.inf\n",
    "    p_var = torch.tensor(0.2)\n",
    "    model.train()\n",
    "    eps = 1e-5\n",
    "    scale_kl = 1e-3 \n",
    "    var_scale = 1e-1 * 1* 0\n",
    "    tr_loss_track, val_loss_track = [],[]\n",
    "    tr_loss_all, val_loss_all, tr_kl_all, mse_all= [],[],[],[]\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        t_start = time.time()\n",
    "        \n",
    "        step = 0\n",
    "        for x,y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x,y = x.to(device), y.to(device) # ---> log2 y (in train loader we have log2 y)\n",
    "            y_hat, z_k, mu, log_var, log_det = model(x) # ---> log2 y hat (model is forced to predict log2 y)\n",
    "\n",
    "\n",
    "            d_var = torch.std(y)\n",
    "            \n",
    "            loss = torch.abs(( (y+eps)-(y_hat.squeeze(-1)+eps) )) ** 2  * (1/d_var) #2 \n",
    "            loss = loss.mean()\n",
    "            \n",
    "            ### this mse is just for checking the model's performance --> it was not used in updating the model\n",
    "            mse = F.mse_loss( y, y_hat.squeeze(-1))\n",
    "\n",
    "            ### scaling the loss for putting more weight on the accuracy of the model \n",
    "            new_loss = 1 * loss \n",
    "\n",
    "            ### kl divergence loss for Normalizing Flow part which is optimized by elbo (similar loss as VAE)\n",
    "            ### kl divergence is scaled which is a common practice, and helps the model to converge faster\n",
    "            kl_div = ((mu-0)**2 + p_var**2)/(2*torch.exp(log_var)) + log_var - math.log(p_var) - 0.5\n",
    "            elbo_loss = new_loss + scale_kl * kl_div.mean() - var_scale * torch.std(y_hat) #+ mse\n",
    "            elbo_loss.backward()\n",
    "                    \n",
    "\n",
    "            optimizer.step()        \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            ### keeping track of different losses\n",
    "            tr_kl_all.append(kl_div.mean().item())\n",
    "            tr_loss_all.append(loss.item())\n",
    "            mse_all.append(mse.item())\n",
    "            \n",
    "            step += 1\n",
    "            print(f'step {step}/{len(train_loader)}, loss: {loss.item()}, kl_div: {tr_kl_all[-1]}', end = '\\r')\n",
    "\n",
    "        tr_loss_track.append(total_loss/len(train_loader))\n",
    "        # print(f'Epoch: {epoch}, Train error: {tr_loss_track[-1]}')\n",
    "\n",
    "        ### Validating the model \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in valid_loader:\n",
    "                x_val,y_val = x_val.to(device), y_val.to(device)\n",
    "                y_hat, z_k, mu, log_var, log_det = model(x_val) # ---> log y hat\n",
    "\n",
    "                # y_hat = torch.round(y_hat)\n",
    "                \n",
    "                # y_val = torch.log10(torch.log2(y_val)) # ---> since the model predicts the log2 y , and the validation y is not on log \n",
    "                y_val = torch.log2(y_val) # ---> since the model predicts the log2 y , and the validation y is not on log \n",
    "                \n",
    "                ### again, for consistency, similar loss as in training is utilized \n",
    "                val_loss = F.mse_loss( (((y_val + eps))), ((y_hat.squeeze(-1) + eps)))\n",
    "                total_val_loss += val_loss.item()\n",
    "                val_loss_all.append(val_loss.item())\n",
    "            \n",
    "        \n",
    "        total_val_loss /= len(valid_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        val_loss_track.append(total_val_loss)\n",
    "        \n",
    "        ### saving the loss track\n",
    "        np.save(f'more_feats_kl{n}{k}{m}.npy', np.array(tr_kl_all))\n",
    "        np.save(f'more_feats_vloss{n}{k}{m}.npy', np.array(val_loss_track))\n",
    "        np.save(f'more_feats_tloss{n}{k}{m}.npy', np.array(tr_loss_track))\n",
    "        \n",
    "        if total_val_loss < bperf: \n",
    "            bperf = total_val_loss\n",
    "            torch.save(model.state_dict(), f'more_feats_best_model{n}{k}{m}.pth')\n",
    "        e_time = time.time()\n",
    "        if epoch%1 == 0:\n",
    "                print(f\"Setting (n,k,m):{n}, {k}, {m} ---  epoch {epoch+1}, train Loss: {total_loss / len(train_loader)}, valid loss: {total_val_loss}, lr: {scheduler.get_last_lr()}, time: {e_time-t_start}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23cf184",
   "metadata": {},
   "source": [
    "# Loading and Plotting the performance on training/validation losses for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad222133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 21 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "for i,item in enumerate(nkm):\n",
    "    n,k,m = item[0], item[1], item[2]\n",
    "    kl_track = np.load(f'more_feats_kl{n}{k}{m}.npy')\n",
    "    tr_track = np.load(f'more_feats_vloss{n}{k}{m}.npy')\n",
    "    val_track = np.load(f'more_feats_tloss{n}{k}{m}.npy')\n",
    "    \n",
    "    plt.subplot(3,7,i+1)\n",
    "    \n",
    "\n",
    "    plt.plot(range(1,len(val_track)+1),val_track, label = 'valid loss', color = 'red')\n",
    "    plt.plot(range(1,len(tr_track)+1),tr_track[:], label = 'train loss', color = 'blue')\n",
    "    plt.title(f'nkm:{n}-{k}-{m}')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a4e7d",
   "metadata": {},
   "source": [
    "# loading and Testing each model on its specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f81e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amira\\AppData\\Local\\Temp\\ipykernel_15828\\1878603544.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = CustomDataset(torch.tensor(train_data), (torch.log2(torch.tensor(train_targets))))\n",
      "C:\\Users\\amira\\AppData\\Local\\Temp\\ipykernel_15828\\1878603544.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  valid_dataset = CustomDataset(torch.tensor(val_data), torch.tensor(val_targets))\n",
      "C:\\Users\\amira\\AppData\\Local\\Temp\\ipykernel_15828\\1878603544.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_dataset = CustomDataset(torch.tensor(test_data), torch.tensor(test_targets))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error for n=9, k=4, m=2: 0.19914556193499\n",
      "test error for n=9, k=4, m=3: 0.24564016313906387\n",
      "test error for n=9, k=4, m=4: 0.8989567486764962\n",
      "test error for n=9, k=4, m=5: 3.4113714396044905\n",
      "test error for n=9, k=5, m=2: 0.23067422136479226\n",
      "test error for n=9, k=5, m=3: 0.8673638902748029\n",
      "test error for n=9, k=5, m=4: 3.45105979785408\n",
      "test error for n=9, k=6, m=2: 0.6560678592137988\n",
      "test error for n=9, k=6, m=3: 3.365795997428826\n",
      "test error for n=10, k=4, m=2: 0.8554715536671678\n",
      "test error for n=10, k=4, m=3: 0.10951986871024193\n",
      "test error for n=10, k=4, m=4: 0.29692996083784207\n",
      "test error for n=10, k=4, m=5: 0.9454339040431081\n",
      "test error for n=10, k=4, m=6: 3.426969735844606\n",
      "test error for n=10, k=5, m=2: 0.1252524893880134\n",
      "test error for n=10, k=5, m=3: 0.3297015241737947\n",
      "test error for n=10, k=5, m=4: 0.936814005571577\n",
      "test error for n=10, k=5, m=5: 3.375906578806332\n",
      "test error for n=10, k=6, m=2: 0.24741990564222566\n",
      "test error for n=10, k=6, m=3: 0.8933539268384674\n",
      "test error for n=10, k=6, m=4: 3.4498733022715435\n",
      "\n",
      " <<<<<<<<<<<================================>>>>>>>>>>>> \n",
      "\n",
      "The average test loss among all setting is: 1.3485105921564888\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "input_dim = 30#20#13\n",
    "latent_dim = 4#2#4#5\n",
    "output_dim = 1\n",
    "num_flows = 10#10#4\n",
    "num_epochs = 50#20 * 2\n",
    "flow_type = 'planar'  # Choose 'planar' or 'radial'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "all_test_losses = 0\n",
    "for item in nkm:\n",
    "\n",
    "    n,k,m = item[0], item[1], item[2]\n",
    "\n",
    "#     data, target = modif_data_s[[(modif_data_s[:,-1]==item[2]) & (modif_data_s[:,-2]==item[1]) & (modif_data_s[:,-3]==item[0])]], target_vals_s[[(modif_data_s[:,-1]==item[2]) & (modif_data_s[:,-2]==item[1]) & (modif_data_s[:,-3]==item[0])]]\n",
    "    data, target = modif_data[[(modif_data[:,-1]==item[2]) & (modif_data[:,-2]==item[1]) & (modif_data[:,-3]==item[0])]], target_vals[[(modif_data[:,-1]==item[2]) & (modif_data[:,-2]==item[1]) & (modif_data[:,-3]==item[0])]]\n",
    "\n",
    "\n",
    "    train_data, val_data, train_targets, val_targets = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "    val_data, test_data, val_targets, test_targets = train_test_split(val_data, val_targets, test_size=0.5, random_state=42)\n",
    "\n",
    "    \n",
    "    inds = np.where(train_targets < 0.5*1e9) # removing very large values ---> can be outliers\n",
    "    train_data, train_targets = train_data[inds], train_targets[inds]\n",
    "\n",
    "\n",
    "    train_dataset = CustomDataset(torch.tensor(train_data), (torch.log2(torch.tensor(train_targets))))\n",
    "    valid_dataset = CustomDataset(torch.tensor(val_data), torch.tensor(val_targets))\n",
    "    test_dataset = CustomDataset(torch.tensor(test_data), torch.tensor(test_targets))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = 1024 * 2 #256 * 4\n",
    "    num_workers = 4\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)#, num_workers = num_workers)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)#, num_workers = num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)#, num_workers = num_workers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model = VAEWithFlow(input_dim, latent_dim, output_dim, num_flows, flow_type).to(torch.float64).to(device)\n",
    "\n",
    "    checkpoint = torch.load(f'more_feats_best_model{n}{k}{m}.pth', map_location=torch.device(device)) \n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "\n",
    "    # model.eval()\n",
    "    # total_val_loss = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for x_val, y_val in valid_loader:\n",
    "    #         x_val,y_val = x_val.to(device), y_val.to(device)\n",
    "    #         y_hat, z_k, mu, log_var, log_det = model(x_val) # ---> log y hat\n",
    "\n",
    "    #         # y_hat = torch.round(y_hat)\n",
    "            \n",
    "    #         # y_val = torch.log10(torch.log2(y_val)) # ---> since the model predicts the log2 y , and the validation y is not on log \n",
    "    #         y_val = torch.log2(y_val) # ---> since the model predicts the log2 y , and the validation y is not on log \n",
    "            \n",
    "    #         ### again, for consistency, similar loss as in training is utilized \n",
    "    #         val_loss = F.mse_loss( (((y_val + eps))), ((y_hat.squeeze(-1) + eps)))\n",
    "    #         total_val_loss += val_loss.item()\n",
    "    #         val_loss_all.append(val_loss.item())\n",
    "        \n",
    "    \n",
    "    # total_val_loss /= len(valid_loader)\n",
    "\n",
    "    \n",
    "\n",
    "    eps = 1e-5\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_test,y_test = x_test.to(device), y_test.to(device)\n",
    "            y_hat, _,_,_,_ = model(x_test) # ---> log2 y\n",
    "            \n",
    "            test_loss = F.mse_loss( torch.log2((y_test + eps)), (y_hat.squeeze(-1)))\n",
    "            \n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "        total_test_loss /= len(test_loader)\n",
    "\n",
    "    all_test_losses += total_test_loss/21\n",
    "    print(f'test error for n={n}, k={k}, m={m}: {total_test_loss}')\n",
    "\n",
    "print(f'\\n <<<<<<<<<<<================================>>>>>>>>>>>> \\n\\nThe average test loss among all setting is: {all_test_losses}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ead96",
   "metadata": {},
   "source": [
    "# creating the function for the template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tamu_csce_636_project1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca401f1",
   "metadata": {},
   "source": [
    "# Please use the following function for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ready_to_use_func(n,k,m,p_list, path = 'models/'): ## please save the model where ever you are using the function\n",
    "    \n",
    "    # checkpoint = torch.load(f'vaenf_best_perf_model_check{n}{k}{m}.pth', map_location=torch.device(device)) \n",
    "\n",
    "    \n",
    "    #### loading the model \n",
    "    model_path = path + f'more_feats_best_model{n}{k}{m}.pth'\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = VAEWithFlow(input_dim=30, latent_dim=4, output_dim=1, num_flows=10, flow_type='planar').to(torch.float64).to(device)\n",
    "    model_check_point = torch.load(model_path, map_location=torch.device(device))\n",
    "    model.load_state_dict(model_check_point)    \n",
    "\n",
    "    # nmks = torch.tensor([[n,m,k] for item in p_list])\n",
    "    nkms = torch.tensor([[n,k,m] for item in p_list])\n",
    "\n",
    "    p_list_tensor = torch.tensor(p_list) ### following the instrunctions in the template\n",
    "    p_list_tensor = p_list_tensor.reshape(-1, p_list_tensor.shape[1] * p_list_tensor.shape[2])\n",
    "    modif_matrices = pre_process_data(p_list_tensor, nfeat=17)\n",
    "    \n",
    "    modif_features = torch.cat([modif_matrices, nkms], axis = 1)\n",
    "    modif_features = modif_features.to(device).to(torch.float64)\n",
    "\n",
    "    output = model(modif_features)\n",
    "    output = 2 ** output ### my model predicts the log2\n",
    "    return output.flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683567d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
